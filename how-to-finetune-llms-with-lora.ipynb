{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9c0be9e-8fc2-4fa4-911c-8f73b537e14c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Low-Rank Adaption (LoRA)\n",
    "This Notebook introduces how to apply low-rank adaptation (LoRA) to your model of choice using [Parameter-Efficient Fine-Tuning (PEFT) library developed by Hugging Face](https://huggingface.co/docs/peft/index). \n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "1. Apply LoRA to a model\n",
    "1. Fine-tune on your provided dataset\n",
    "1. Save your model\n",
    "1. Conduct inference using the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: ipykernel\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip show ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "302ace68-2999-45a2-bee0-97d0b01ba737",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-10T11:02:29.115407Z",
     "iopub.status.busy": "2024-11-10T11:02:29.114749Z",
     "iopub.status.idle": "2024-11-10T11:03:01.074197Z",
     "shell.execute_reply": "2024-11-10T11:03:01.072453Z",
     "shell.execute_reply.started": "2024-11-10T11:02:29.115343Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/kookmin/chaewon/LLM-Document_Summarizer/.venv/bin/pip: /home/kookmin/chaewon/LLM_document_summary/.venv/bin/python3: bad interpreter: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install peft==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T12:57:02.748074Z",
     "iopub.status.busy": "2024-11-10T12:57:02.747519Z",
     "iopub.status.idle": "2024-11-10T12:57:33.268116Z",
     "shell.execute_reply": "2024-11-10T12:57:33.266636Z",
     "shell.execute_reply.started": "2024-11-10T12:57:02.748014Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/kookmin/chaewon/LLM-Document_Summarizer/.venv/bin/pip: /home/kookmin/chaewon/LLM_document_summary/.venv/bin/python3: bad interpreter: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T11:03:01.080214Z",
     "iopub.status.busy": "2024-11-10T11:03:01.079664Z",
     "iopub.status.idle": "2024-11-10T11:03:01.213543Z",
     "shell.execute_reply": "2024-11-10T11:03:01.211621Z",
     "shell.execute_reply.started": "2024-11-10T11:03:01.080160Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘cache’: File exists\n"
     ]
    }
   ],
   "source": [
    "mkdir cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0de62379-46a6-4ed7-bd31-2cc14414d4df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will re-use the same dataset and model from the demo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T13:14:01.393491Z",
     "iopub.status.busy": "2024-11-10T13:14:01.393162Z",
     "iopub.status.idle": "2024-11-10T13:14:02.345318Z",
     "shell.execute_reply": "2024-11-10T13:14:02.344450Z",
     "shell.execute_reply.started": "2024-11-10T13:14:01.393465Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘offload’: File exists\n"
     ]
    }
   ],
   "source": [
    "mkdir offload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T00:07:38.130737Z",
     "iopub.status.busy": "2024-11-11T00:07:38.130017Z",
     "iopub.status.idle": "2024-11-11T00:07:38.768248Z",
     "shell.execute_reply": "2024-11-11T00:07:38.766124Z",
     "shell.execute_reply.started": "2024-11-11T00:07:38.130673Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘working’: File exists\n"
     ]
    }
   ],
   "source": [
    "mkdir working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/kookmin/chaewon/LLM-Document_Summarizer/.venv/bin/pip: /home/kookmin/chaewon/LLM_document_summary/.venv/bin/python3: bad interpreter: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b7986fa-e0be-4a3c-a08f-61d3825b8c21",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T03:42:34.094632Z",
     "iopub.status.busy": "2024-11-11T03:42:34.093950Z",
     "iopub.status.idle": "2024-11-11T03:44:19.952596Z",
     "shell.execute_reply": "2024-11-11T03:44:19.951465Z",
     "shell.execute_reply.started": "2024-11-11T03:42:34.094567Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBllossom/llama-3.2-Korean-Bllossom-3B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m foundation_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTOKEN_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, token\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOKEN_1\u001b[39m\u001b[38;5;124m'\u001b[39m], device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbirate/english_quotes\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./working/cache\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:3577\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3573\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3574\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3575\u001b[0m         )\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 3577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   3578\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3579\u001b[0m         )\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[0;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(model_name, token=os.environ['TOKEN_1'], device_map='cuda', torch_dtype=torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=os.environ['TOKEN_1'], device_map='auto', torch_dtype=torch.float32)\n",
    "\n",
    "data = load_dataset(\"Abirate/english_quotes\", cache_dir=\"./working/cache\"+\"/datasets\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "train_sample = data[\"train\"].select(range(50))\n",
    "display(train_sample) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8813b451-e821-4948-aa8c-52074fe438f9",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T03:44:19.958566Z",
     "iopub.status.busy": "2024-11-11T03:44:19.956604Z",
     "iopub.status.idle": "2024-11-11T03:44:20.079874Z",
     "shell.execute_reply": "2024-11-11T03:44:20.078637Z",
     "shell.execute_reply.started": "2024-11-11T03:44:19.958527Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=1,  # dimension of the updated matrices\n",
    "    lora_alpha=4,  # parameter for scaling\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        # \"up_proj\",\n",
    "        # \"o_proj\",\n",
    "        # \"k_proj\",\n",
    "        # \"down_proj\",\n",
    "        # \"gate_proj\",\n",
    "        # \"v_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.1,  # dropout probability for layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bbd4b67-7ed2-4a85-a164-ff03bd425a7c",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T03:44:20.086576Z",
     "iopub.status.busy": "2024-11-11T03:44:20.084284Z",
     "iopub.status.idle": "2024-11-11T03:45:01.361667Z",
     "shell.execute_reply": "2024-11-11T03:45:01.360545Z",
     "shell.execute_reply.started": "2024-11-11T03:44:20.086535Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 172,032 || all params: 3,212,921,856 || trainable%: 0.00535437859089966\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2f6ac13-5d20-4b5f-bf84-efc5464796e0",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T03:45:08.608223Z",
     "iopub.status.busy": "2024-11-11T03:45:08.607981Z",
     "iopub.status.idle": "2024-11-11T03:46:37.964317Z",
     "shell.execute_reply": "2024-11-11T03:46:37.963000Z",
     "shell.execute_reply.started": "2024-11-11T03:45:08.608199Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m      8\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./cache/working\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_lab_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     10\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# no_cuda=True\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "output_directory = os.path.join(\"./cache/working\", \"peft_lab_outputs\")\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    output_dir=output_directory,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=5,\n",
    "    # no_cuda=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d619737-ae7a-4ed6-8c46-31cccfb94738",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T03:49:01.467576Z",
     "iopub.status.busy": "2024-11-11T03:49:01.465603Z",
     "iopub.status.idle": "2024-11-11T03:49:01.573718Z",
     "shell.execute_reply": "2024-11-11T03:49:01.572844Z",
     "shell.execute_reply.started": "2024-11-11T03:49:01.467498Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_directory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m time_now \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m peft_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43moutput_directory\u001b[49m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_now\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(peft_model_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_directory' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "time_now = time.time()\n",
    "\n",
    "peft_model_path = os.path.join(output_directory, f\"peft_model_{time_now}\")\n",
    "\n",
    "trainer.model.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig \n",
    "config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            # bnb_4bit_quant_type=\"nf4\",\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a4fa5a6-049a-4b05-877b-60c161582d3c",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T04:07:56.498662Z",
     "iopub.status.busy": "2024-11-11T04:07:56.498076Z",
     "iopub.status.idle": "2024-11-11T04:09:11.613415Z",
     "shell.execute_reply": "2024-11-11T04:09:11.612656Z",
     "shell.execute_reply.started": "2024-11-11T04:07:56.498602Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [02:32<00:00, 76.33s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "import os\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        device_map=\"cuda\",  # 두 번째 GPU로 할당\n",
    "        quantization_config=config,\n",
    "        token=\"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\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, device_map=\"cuda\", token=\"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\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})  # pad_token 설정\n",
    "loaded_model = PeftModel.from_pretrained(model, \"/workspace/LLM-Document_Summarizer/results/checkpoint-27534\", \n",
    "                                        is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 128009], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<|eot_id|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# 사용자 정의 StoppingCriteria\n",
    "class StopOnKeyword(StoppingCriteria):\n",
    "    def __init__(self, stop_words, tokenizer, max_words=500):\n",
    "        self.stop_words = stop_words\n",
    "        self.tokenizer = tokenizer\n",
    "        self.words = 0\n",
    "        self.max = max_words\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        self.words += 1\n",
    "        if self.words > self.max:\n",
    "            return any(word in generated_text[-1] for word in self.stop_words)\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('dataR.json', 'r') as F:\n",
    "    d = json.load(F)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cd10848-531d-483c-81f2-3aa66af87f37",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T04:09:49.068972Z",
     "iopub.status.busy": "2024-11-11T04:09:49.068286Z",
     "iopub.status.idle": "2024-11-11T04:09:52.141513Z",
     "shell.execute_reply": "2024-11-11T04:09:52.139784Z",
     "shell.execute_reply.started": "2024-11-11T04:09:49.068912Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>우아한형제들 대표 김 대표는 DH의 투자자 지분 13%를 DH 본사 지분으로 인수하기로 했다. 김 대표는 DH 경영인 가운데 개인 최대 주주로 된다. 김 대표의 고민은 민족 브랜드가 외국 자본에 넘어가는 것에 닿아 있었다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import StoppingCriteriaList\n",
    "\n",
    "# CUDA 환경 설정 초기화\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# 모델 장치 출력\n",
    "print(f\"Model device: {loaded_model.device}\")\n",
    "\n",
    "# 원본 텍스트\n",
    "inputs_raw = \"\"\"40억 달러 ‘딜’ 주인공 김봉진 우아한형제들 대표태풍 뒤의 고요함이랄까.\n",
    "40억 달러짜리 ‘딜’(거래)을 마친 뒤의 사무실은 조용했다.\n",
    "음식 배달 앱 ‘배달의민족’ 운영사 ㈜우아한형제들의 서울 송파구 방이동 사옥은 비어 있었다.\n",
    "전 직원들에게 연말 특별 휴가를 선물한 김봉진(44) 대표는 혼자 출근해 남은 일을 처리하고 있었다.\n",
    "축하한다는 말을 건네자 “이제 시작인 걸요”라는 답이 돌아왔다.\n",
    "그에게 기업 매각은 단순한 ‘엑시트’(창업 후 지분 매각으로 이익을 실현하는 일)가 아니었다.\n",
    "성공 스토리 뒤에는 환희의 무게만큼 고민이 자리 잡고 있었다.\n",
    "독일계 음식 배달서비스업체 DH(딜리버리 히어로)가 평가한 우아한형제들의 기업가치는 약 4조8000억원.\n",
    "국내 스타트업 M&A 사상 최대 규모다.\n",
    "앱 하나로 평가받은 기업가치가 GS나 현대건설의 시가총액과 맞먹는다.\n",
    "‘매각’이라는 표현을 썼지만, 창업자인 김 대표가 회사를 떠나는 것은 아니다.\n",
    "오히려 역할이 커졌다.\n",
    "두 회사가 절반씩 출자해 싱가포르에 세우는 합작법인 ‘우아DH아시아’의 책임자로서 아시아 11개국 사업을 총괄하게 된다.\n",
    "DH는 우아한형제들의 투자자 지분 87%를 인수하고, 김 대표 등 경영진이 가진 지분 13%는 DH 본사 지분으로 전환하기로 했다.\n",
    "그렇게 되면 김 대표는 DH 경영진 가운데 개인 최대 주주가 된다.\n",
    "“더 큰 꿈 위해 글로벌 자본 선택” 김 대표의 고민은 ‘민족’이라는 단어에 닿아 있었다.\n",
    "회사가 외국 자본에 넘어가면서 민족 브랜드가 어울리지 않게 됐다는 시선 때문이다.\n",
    "소비자 반응이 긍정적이지만은 않다.\n",
    "“겸허하게 받아들인다.\n",
    "DH와는 경쟁 관계이지만 창업 초기부터 지속해서 교류해왔다.\n",
    "그 과정에서 그들이 지닌 ‘글로벌 DNA’에 놀랐다.\n",
    "DH는 홈그라운드 격인 독일 사업마저 네덜란드 기업에 넘기고 글로벌 마케팅을 강화해왔다.\n",
    "그들과 계속 싸울지, 합쳐서 글로벌 무대로 나갈지 마지막까지 고민했다.\n",
    "더 큰 도전을 위한 선택이라고 이해해줬으면 한다.\n",
    "국내 상장도 생각해볼 수 있었을 텐데.\n",
    "국내 상장이 이뤄지지 않아 아쉽긴 하다.\n",
    "난들 여의도 거래소에서 멋있게 상장 축하 종을 쳐보고 싶지 않았겠나.\"\"\"\n",
    "\n",
    "summary_len = len(inputs_raw) // 100 * 10\n",
    "prompt = f\"\"\"\n",
    "MAKE SURE THAT YOU SUMMARIZE THE FOLLOWING TEXT TO A MAXIMUM OF {summary_len} OF ITS LENGTH. THE SUMMARY CAN BE SHORTER if all essential information is included, ensuring the following rules:\n",
    "\n",
    "1. **Summary Quality:**\n",
    "   - The summarized text should have no spelling errors or typos.\n",
    "   - Avoid repeating similar content. If multiple sentences convey similar ideas, output only one concise sentence to represent them.\n",
    "   - The text should be logically structured and divided into appropriate paragraphs to maintain readability.\n",
    "\n",
    "2. **Prevent Duplication:**\n",
    "   - Do not generate sentences that repeat or convey the same idea as other sentences within the summarized text.\n",
    "   - If a sentence shares a similar meaning with another, only include the most concise and representative one. The rest must be omitted.\n",
    "   - The summary does not need to reach a specific target length, as long as all essential information is included without duplication.\n",
    "\n",
    "Ensure the final summarized text adheres to these rules and retains its readability and logical structure.\n",
    "\"\"\"\n",
    "\n",
    "inputs_raw = f\"\"\"<|begin_of_text|><|start_header_id|>user: <|end_header_id|>{prompt}\n",
    "{inputs_raw}<|eot_id|><|start_header_id|>assistant: <|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# 종료 조건 설정\n",
    "stop_words = [\".\"]  # 종료를 트리거하는 키워드\n",
    "stopping_criteria = StoppingCriteriaList([StopOnKeyword(stop_words, tokenizer, summary_len)])\n",
    "\n",
    "# 입력 토큰화\n",
    "inputs = tokenizer(inputs_raw, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# 출력 생성\n",
    "outputs = loaded_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"], \n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_new_tokens=len(inputs_raw) // 100 * 15,\n",
    "    eos_token_id=128009,\n",
    "    temperature=0.4,\n",
    "    no_repeat_ngram_size=7,  # 반복을 방지\n",
    "    # repetition_penalty = 1.2,\n",
    "    stopping_criteria=stopping_criteria,  # 사용자 정의 종료 조건\n",
    "    # early_stopping=True,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# 불필요한 특수 문자 제거 및 포맷팅\n",
    "decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
    "filtered_output = decoded_output.replace(inputs_raw, '').strip()\n",
    "\n",
    "# 한자 및 일본어 제거 함수\n",
    "def remove_non_korean(text):\n",
    "    \"\"\"\n",
    "    한자(중국어) 및 일본어를 제거하는 함수.\n",
    "    유니코드 범위:\n",
    "    - 한자: \\u4E00-\\u9FFF\n",
    "    - 일본어(히라가나): \\u3040-\\u309F\n",
    "    - 일본어(가타카나): \\u30A0-\\u30FF\n",
    "    \"\"\"\n",
    "    return re.sub(r'[\\u4E00-\\u9FFF\\u3040-\\u309F\\u30A0-\\u30FF]+', '', text)\n",
    "\n",
    "# 불필요한 문자 제거 및 최종 출력\n",
    "final_output = remove_non_korean(filtered_output)\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "2223\n",
      "141\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "print(summary_len)\n",
    "print(len(inputs_raw))\n",
    "print(len(final_output))\n",
    "print(len(inputs_raw) // 100 * 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T03:56:41.907425Z",
     "iopub.status.busy": "2024-11-11T03:56:41.906729Z",
     "iopub.status.idle": "2024-11-11T03:56:41.923157Z",
     "shell.execute_reply": "2024-11-11T03:56:41.919042Z",
     "shell.execute_reply.started": "2024-11-11T03:56:41.907361Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%` not found.\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM 02L - LoRA with PEFT",
   "widgets": {}
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6201832,
     "sourceId": 53482,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 6169864,
     "sourceId": 54662,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30527,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
