{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_data(file_path):\n",
    "    # 데이터셋 로드\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    # 데이터를 Trainer가 사용할 수 있도록 Dataset 형식으로 변환\n",
    "    def generate_prompts(example):\n",
    "        prompt_list = []\n",
    "        for key, example in dataset.items():\n",
    "            prompt_list.append(\n",
    "                f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>다음 글을 요약해주세요:\n",
    "{example['content']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "{example['answer']}<|eot_id|>\"\"\"\n",
    "            )\n",
    "        return prompt_list\n",
    "\n",
    "    # 프롬프트 리스트 생성\n",
    "    prompts = generate_prompts(dataset)\n",
    "\n",
    "    # Dataset으로 변환\n",
    "    prompts_dataset = Dataset.from_list([{\"input_text\": prompt} for prompt in prompts])\n",
    "    return prompts_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def train_model(tokenized_prompts, model, tokenizer):\n",
    "    # DataCollator 정의 (MLM을 사용하지 않도록 설정)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # 학습 설정\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        per_device_train_batch_size=1,  # 배치 크기 조정\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=10,\n",
    "        fp16=True,  # 16-bit 연산 활성화\n",
    "        save_strategy=\"no\",  # 모델 저장 전략 설정\n",
    "    )\n",
    "\n",
    "    # Trainer 설정\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_prompts,\n",
    "        data_collator=data_collator  # DataCollatorForLanguageModeling 사용\n",
    "    )\n",
    "\n",
    "    # 학습 시작\n",
    "    print(f\"Model is on device: {model.device}\")  # 모델이 할당된 GPU 디바이스 확인\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "def setup_model():\n",
    "    BASE_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    \n",
    "    # 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})  # pad_token 설정\n",
    "\n",
    "    # 4bit 양자화 설정 - QLoRA로 해야 함\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    # 모델 로드 (양자화)\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 1}  # 두 번째 GPU로 할당\n",
    "    )\n",
    "\n",
    "    # PEFT 설정: LoRA 어댑터 추가\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    # LoRA 어댑터를 모델에 적용\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# 데이터 로드\n",
    "prompts_dataset = load_data(\"dataFromAIHUB.json\")\n",
    "\n",
    "# 모델 설정 (여기서 device_map을 통해 모델을 정확한 GPU에 로드)\n",
    "model, tokenizer = setup_model()\n",
    "\n",
    "# 데이터셋 토큰화\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"]  # labels 필드를 input_ids와 동일하게 설정\n",
    "    return tokenized\n",
    "\n",
    "# 데이터셋 토큰화\n",
    "tokenized_prompts = prompts_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 데이터셋 텐서 형식으로 설정\n",
    "tokenized_prompts.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# DataCollator 정의 (MLM을 사용하지 않도록 설정)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,  # 배치 크기 조정\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    fp16=True,  # 16-bit 연산 활성화\n",
    "    save_strategy=\"no\",  # 모델 저장 전략 설정\n",
    ")\n",
    "\n",
    "# 모델이 두 번째 GPU에 할당되었는지 확인\n",
    "print(f\"Model is on device: {model.device}\")\n",
    "\n",
    "# 메모리 비우기 (필요시 학습 전에)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_prompts,\n",
    "    data_collator=data_collator  # DataCollatorForLanguageModeling 사용\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# 학습 전에 데이터를 model.device로 이동 (모델이 위치한 GPU로 이동)\n",
    "tokenized_prompts = tokenized_prompts.to(model.device)  # 데이터가 모델과 동일한 GPU로 이동\n",
    "\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,  # 배치 크기 조정\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    fp16=True,  # 16-bit 연산 활성화\n",
    "    save_strategy=\"no\",  # 모델 저장 전략 설정\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_prompts,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)  # MLM을 사용하지 않음\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 완료된 모델을 사용하여 예측을 수행\n",
    "text = \"예시 텍스트\"  # 예시 텍스트를 입력합니다.\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs['input_ids'], max_length=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     22\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     23\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     26\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 모델 로드 (양자화)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 두 번째 GPU로 할당\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(BASE_MODEL)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# tokenizer.pad_token = tokenizer.eos_token\u001b[39;00m\n",
      "File \u001b[0;32m~/chaewon/LLM-Document_Summarizer/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/chaewon/LLM-Document_Summarizer/.venv/lib/python3.8/site-packages/transformers/modeling_utils.py:3657\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3654\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3657\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3660\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3661\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/chaewon/LLM-Document_Summarizer/.venv/lib/python3.8/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:74\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "with open(\"dataFromAIHUB.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "    \n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# LoRA 설정 : 양자화된 모델에서 Adaptor를 붙여서 학습할 파라미터만 따로 구성함\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0.05,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 4bit 양자화 설정 - QLoRA로 해야 함\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 모델 로드 (양자화)\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map={\"\": 1}  # 두 번째 GPU로 할당\n",
    "                                            )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# LoRA 적용\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "def generate_prompts(example):\n",
    "    prompt_list = []\n",
    "    for key, example in dataset.items():\n",
    "        prompt_list.append(\n",
    "f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>다음 글을 요약해주세요:\n",
    "{example['content']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "{example['answer']}<|eot_id|>\"\"\"\n",
    "        )\n",
    "    return prompt_list\n",
    "\n",
    "# 프롬프트 리스트 생성\n",
    "prompts = generate_prompts(dataset)\n",
    "\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.device)  # 모델이 할당된 장치 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments, default_data_collator\n",
    "\n",
    "# prompts 리스트를 Dataset 형식으로 변환\n",
    "prompts_dataset = Dataset.from_list([{\"input_text\": prompt} for prompt in prompts])\n",
    "# prompts_dataset = Dataset.from_list(prompts)\n",
    "\n",
    "# 토큰화 함수 정의\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # 복사본을 labels로 추가\n",
    "    return tokenized\n",
    "\n",
    "# 토큰화 적용\n",
    "tokenized_prompts = prompts_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 모델을 GPU에 명시적으로 할당\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 토큰화된 데이터셋을 GPU에 할당하는 함수 추가\n",
    "def move_to_device(batch):\n",
    "    return {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "# 데이터셋의 모든 항목을 GPU로 이동\n",
    "tokenized_prompts = tokenized_prompts.map(move_to_device, batched=True)\n",
    "\n",
    "# 기존 Data Collator 정의를 default_data_collator로 대체\n",
    "data_collator = default_data_collator\n",
    "\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    fp16=True,  # 16-bit로 계산하여 메모리 절약\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_prompts,  # 토큰화된 데이터셋 사용\n",
    "    # tokenizer=tokenizer,\n",
    "    data_collator=data_collator  # 추가된 부분\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,  # 배치 크기 조정\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    fp16=True,  # 16-bit 연산 활성화\n",
    "    save_strategy=\"no\",  # 필요에 따라 변경 가능\n",
    "    # CUDA 디바이스를 명시적으로 설정할 필요가 없지만,\n",
    "    # torch.cuda.is_available()이 True일 경우 자동으로 GPU 사용\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,  # 모델이 두 번째 GPU에 이미 로드되어 있음\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_prompts,\n",
    "    data_collator=data_collator  # DataCollatorForLanguageModeling 사용\n",
    ")\n",
    "\n",
    "# 모델이 두 번째 GPU에 잘 할당되었는지 확인\n",
    "print(f\"Model is on device: {model.device}\")\n",
    "\n",
    "# 학습 시작\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DataLoader 생성 (collate_fn으로 데이터 구조를 맞춤)\n",
    "data_loader = DataLoader(tokenized_prompts, batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "# 첫 번째 배치 가져오기\n",
    "first_batch = next(iter(data_loader))\n",
    "\n",
    "# 각 텐서의 shape 출력\n",
    "print(\"input_ids shape:\", first_batch['input_ids'].shape)\n",
    "print(\"attention_mask shape:\", first_batch['attention_mask'].shape)\n",
    "print(\"labels shape:\", first_batch['labels'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 설정 및 학습 시작\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_prompts,  # 토큰화된 데이터셋 사용\n",
    "    data_collator=data_collator  # data_collator를 사용해 배치 구성\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels 필드가 제대로 추가되었는지 확인\n",
    "print(tokenized_prompts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./trained_model\")\n",
    "tokenizer.save_pretrained(\"./trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_summary(text):\n",
    "    # 입력 텍스트 토큰화\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # 요약 생성\n",
    "    summary_ids = model.generate(**inputs, max_length=150, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# 요약할 새로운 데이터 예시\n",
    "new_data = [\n",
    "    \"여기에 새로운 본문 텍스트를 넣습니다.\",\n",
    "    \"다른 요약이 필요한 본문 텍스트를 여기에 넣습니다.\"\n",
    "]\n",
    "\n",
    "# 요약 생성 및 출력\n",
    "for text in new_data:\n",
    "    summary = generate_summary(text)\n",
    "    print(\"원문:\", text)\n",
    "    print(\"요약:\", summary)\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
