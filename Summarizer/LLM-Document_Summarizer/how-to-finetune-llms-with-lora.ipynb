{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9c0be9e-8fc2-4fa4-911c-8f73b537e14c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Low-Rank Adaption (LoRA)\n",
    "This Notebook introduces how to apply low-rank adaptation (LoRA) to your model of choice using [Parameter-Efficient Fine-Tuning (PEFT) library developed by Hugging Face](https://huggingface.co/docs/peft/index). \n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "1. Apply LoRA to a model\n",
    "1. Fine-tune on your provided dataset\n",
    "1. Save your model\n",
    "1. Conduct inference using the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: ipykernel\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip show ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "302ace68-2999-45a2-bee0-97d0b01ba737",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-10T11:02:29.115407Z",
     "iopub.status.busy": "2024-11-10T11:02:29.114749Z",
     "iopub.status.idle": "2024-11-10T11:03:01.074197Z",
     "shell.execute_reply": "2024-11-10T11:03:01.072453Z",
     "shell.execute_reply.started": "2024-11-10T11:02:29.115343Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/kookmin/chaewon/LLM-Document_Summarizer/.venv/bin/pip: /home/kookmin/chaewon/LLM_document_summary/.venv/bin/python3: bad interpreter: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install peft==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T12:57:02.748074Z",
     "iopub.status.busy": "2024-11-10T12:57:02.747519Z",
     "iopub.status.idle": "2024-11-10T12:57:33.268116Z",
     "shell.execute_reply": "2024-11-10T12:57:33.266636Z",
     "shell.execute_reply.started": "2024-11-10T12:57:02.748014Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/kookmin/chaewon/LLM-Document_Summarizer/.venv/bin/pip: /home/kookmin/chaewon/LLM_document_summary/.venv/bin/python3: bad interpreter: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T11:03:01.080214Z",
     "iopub.status.busy": "2024-11-10T11:03:01.079664Z",
     "iopub.status.idle": "2024-11-10T11:03:01.213543Z",
     "shell.execute_reply": "2024-11-10T11:03:01.211621Z",
     "shell.execute_reply.started": "2024-11-10T11:03:01.080160Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘cache’: File exists\n"
     ]
    }
   ],
   "source": [
    "mkdir cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0de62379-46a6-4ed7-bd31-2cc14414d4df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will re-use the same dataset and model from the demo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T13:14:01.393491Z",
     "iopub.status.busy": "2024-11-10T13:14:01.393162Z",
     "iopub.status.idle": "2024-11-10T13:14:02.345318Z",
     "shell.execute_reply": "2024-11-10T13:14:02.344450Z",
     "shell.execute_reply.started": "2024-11-10T13:14:01.393465Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘offload’: File exists\n"
     ]
    }
   ],
   "source": [
    "mkdir offload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T00:07:38.130737Z",
     "iopub.status.busy": "2024-11-11T00:07:38.130017Z",
     "iopub.status.idle": "2024-11-11T00:07:38.768248Z",
     "shell.execute_reply": "2024-11-11T00:07:38.766124Z",
     "shell.execute_reply.started": "2024-11-11T00:07:38.130673Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘working’: File exists\n"
     ]
    }
   ],
   "source": [
    "mkdir working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/kookmin/chaewon/LLM-Document_Summarizer/.venv/bin/pip: /home/kookmin/chaewon/LLM_document_summary/.venv/bin/python3: bad interpreter: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b7986fa-e0be-4a3c-a08f-61d3825b8c21",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T03:42:34.094632Z",
     "iopub.status.busy": "2024-11-11T03:42:34.093950Z",
     "iopub.status.idle": "2024-11-11T03:44:19.952596Z",
     "shell.execute_reply": "2024-11-11T03:44:19.951465Z",
     "shell.execute_reply.started": "2024-11-11T03:42:34.094567Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBllossom/llama-3.2-Korean-Bllossom-3B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m foundation_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTOKEN_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, token\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOKEN_1\u001b[39m\u001b[38;5;124m'\u001b[39m], device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbirate/english_quotes\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./working/cache\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:3577\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3573\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3574\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3575\u001b[0m         )\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 3577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   3578\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3579\u001b[0m         )\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[0;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(model_name, token=os.environ['TOKEN_1'], device_map='cuda', torch_dtype=torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=os.environ['TOKEN_1'], device_map='auto', torch_dtype=torch.float32)\n",
    "\n",
    "data = load_dataset(\"Abirate/english_quotes\", cache_dir=\"./working/cache\"+\"/datasets\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "train_sample = data[\"train\"].select(range(50))\n",
    "display(train_sample) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8813b451-e821-4948-aa8c-52074fe438f9",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T03:44:19.958566Z",
     "iopub.status.busy": "2024-11-11T03:44:19.956604Z",
     "iopub.status.idle": "2024-11-11T03:44:20.079874Z",
     "shell.execute_reply": "2024-11-11T03:44:20.078637Z",
     "shell.execute_reply.started": "2024-11-11T03:44:19.958527Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=1,  # dimension of the updated matrices\n",
    "    lora_alpha=4,  # parameter for scaling\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        # \"up_proj\",\n",
    "        # \"o_proj\",\n",
    "        # \"k_proj\",\n",
    "        # \"down_proj\",\n",
    "        # \"gate_proj\",\n",
    "        # \"v_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.1,  # dropout probability for layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bbd4b67-7ed2-4a85-a164-ff03bd425a7c",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T03:44:20.086576Z",
     "iopub.status.busy": "2024-11-11T03:44:20.084284Z",
     "iopub.status.idle": "2024-11-11T03:45:01.361667Z",
     "shell.execute_reply": "2024-11-11T03:45:01.360545Z",
     "shell.execute_reply.started": "2024-11-11T03:44:20.086535Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 172,032 || all params: 3,212,921,856 || trainable%: 0.00535437859089966\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2f6ac13-5d20-4b5f-bf84-efc5464796e0",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T03:45:08.608223Z",
     "iopub.status.busy": "2024-11-11T03:45:08.607981Z",
     "iopub.status.idle": "2024-11-11T03:46:37.964317Z",
     "shell.execute_reply": "2024-11-11T03:46:37.963000Z",
     "shell.execute_reply.started": "2024-11-11T03:45:08.608199Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m      8\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./cache/working\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_lab_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     10\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# no_cuda=True\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "output_directory = os.path.join(\"./cache/working\", \"peft_lab_outputs\")\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    output_dir=output_directory,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=5,\n",
    "    # no_cuda=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d619737-ae7a-4ed6-8c46-31cccfb94738",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T03:49:01.467576Z",
     "iopub.status.busy": "2024-11-11T03:49:01.465603Z",
     "iopub.status.idle": "2024-11-11T03:49:01.573718Z",
     "shell.execute_reply": "2024-11-11T03:49:01.572844Z",
     "shell.execute_reply.started": "2024-11-11T03:49:01.467498Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_directory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m time_now \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m peft_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43moutput_directory\u001b[49m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_now\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(peft_model_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_directory' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "time_now = time.time()\n",
    "\n",
    "peft_model_path = os.path.join(output_directory, f\"peft_model_{time_now}\")\n",
    "\n",
    "trainer.model.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig \n",
    "config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            # bnb_4bit_quant_type=\"nf4\",\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a4fa5a6-049a-4b05-877b-60c161582d3c",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T04:07:56.498662Z",
     "iopub.status.busy": "2024-11-11T04:07:56.498076Z",
     "iopub.status.idle": "2024-11-11T04:09:11.613415Z",
     "shell.execute_reply": "2024-11-11T04:09:11.612656Z",
     "shell.execute_reply.started": "2024-11-11T04:07:56.498602Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "import os\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        device_map=\"cuda\",  # 두 번째 GPU로 할당\n",
    "        quantization_config=config,\n",
    "        token=\"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\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, device_map=\"cuda\", token=\"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\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})  # pad_token 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = PeftModel.from_pretrained(model, \"/workspace/1129backup/LLM-Document_Summarizer/results/checkpoint-27534\", \n",
    "                                        is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 128009], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<|eot_id|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# 사용자 정의 StoppingCriteria\n",
    "class StopOnKeyword(StoppingCriteria):\n",
    "    def __init__(self, stop_words, tokenizer, max_words=500):\n",
    "        self.stop_words = stop_words\n",
    "        self.tokenizer = tokenizer\n",
    "        self.words = 0\n",
    "        self.max = max_words\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        self.words += 1\n",
    "        if self.words > self.max:\n",
    "            return any(word in generated_text[-1] for word in self.stop_words)\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cd10848-531d-483c-81f2-3aa66af87f37",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T04:09:49.068972Z",
     "iopub.status.busy": "2024-11-11T04:09:49.068286Z",
     "iopub.status.idle": "2024-11-11T04:09:52.141513Z",
     "shell.execute_reply": "2024-11-11T04:09:52.139784Z",
     "shell.execute_reply.started": "2024-11-11T04:09:49.068912Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>국회예산정책처의 발제는 공공기관 재무건강 악화에 대한 정확한 진단과 관리체계의 문제점이 지적되었다. 2012년 부채상위 10개 공기업 부채 규모가 425조원으로 전체 공공기업의 86% 수준이며 304개 전체 공공기업 전반에 걸친 문제라고 파악하는 것은 논란이 있다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import StoppingCriteriaList\n",
    "\n",
    "# CUDA 환경 설정 초기화\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# 모델 장치 출력\n",
    "print(f\"Model device: {loaded_model.device}\")\n",
    "\n",
    "# 원본 텍스트\n",
    "inputs_raw = \"\"\"둘째, 중장기재무관리계획 작성, 정상화 대책 이행계획 제출 등 공공기관 스스로 작성하고 정부가 이를 평가한다고 했지만, MB정부 당시 가장 부채가 많은 한국토지주택공사(LH)를 상으로 추진했던 사업조정의 실패, 구분회계 도입의 실효성을 보건데, 대책이라고 보기에는 매우 부족함. 마지막으로 공운위 산하에 민관합동으로 ‘정상화협의회’를 구성한다고 했는데, 이 또한 기존의 폐쇄적인 공운위 운영방식에 다름 아니며, 현재 정부의 공공기관 정상화 진행 방식인 ‘불통과 배제’ 방식과 다름 아닌 획일적인 지침에 따른 수직적인 통제방식임. 잘못된 공공기관 정상화 억지 대책과 졸속개혁은 더 큰 부실과 국민피해만 남길 것임. 국회예산정책처 발제에 한 토론 다음으로 국회예산정책처 조영철 사업평가국장의 발제는 그동안 제기되었던 공공기관 재무건전성 악화에 한 정확한 진단, 지배구조 등 관리체계의 문제점에 해 정확히 지적하였음. 한편 전체 공공기관 부채 현황에 해 밝혔지만, 2012년 부채 상위 10개 공기업의 부채 규모가 424조원으로 전체 공공기관의 86% 수준임을 감안하면 304개 전체 공공기관 전반에 걸친 문제로 파악하는 것은 논란의 여지가 있기 때문에 한정할 필요가 있음.\"\"\"\n",
    "\n",
    "summary_len = len(inputs_raw) // 100 * 10\n",
    "prompt = f\"\"\"\n",
    "    MAKE SURE THAT YOU SUMMARIZE THE FOLLOWING TEXT TO A MAXIMUM OF {summary_len} TOKENS. THE SUMMARY CAN BE SHORTER if all essential information is included, ensuring the following rules:\n",
    "\n",
    "    1. **Summary Quality:**\n",
    "    - The summarized text should have no spelling errors or typos.\n",
    "    - Avoid repeating similar content. If multiple sentences convey similar ideas, output only one concise sentence to represent them.\n",
    "    - The text should be logically structured and divided into appropriate paragraphs to maintain readability.\n",
    "\n",
    "    2. **Key Information:**\n",
    "    - Ensure that the summary includes key points such as the causes of debt, government policies, and the need for improved debt management systems.\n",
    "\n",
    "    3. **Prevent Duplication:**\n",
    "    - Do not generate sentences that repeat or convey the same idea as other sentences within the summarized text.\n",
    "    - If a sentence shares a similar meaning with another, only include the most concise and representative one. The rest must be omitted.\n",
    "    - The summary does not need to reach a specific target length, as long as all essential information is included without duplication.\n",
    "\n",
    "    4. **Example of a Good Summary:**\n",
    "    - \"The analysis highlights the need to distinguish between debt caused by price regulations and other factors, emphasizing government policy impacts and the necessity for better debt management.\"\n",
    "\n",
    "    5. **Avoid This Type of Summary:**\n",
    "    - \"Debt is caused by many things. Government policies are involved. Management is needed.\" (Too vague and lacks detail)\n",
    "\n",
    "    Ensure the final summarized text adheres to these rules and retains its readability and logical structure.\n",
    "    \"\"\"\n",
    "\n",
    "inputs_raw = f\"\"\"<|begin_of_text|><|start_header_id|>user: <|end_header_id|>{prompt}\n",
    "{inputs_raw}<|eot_id|><|start_header_id|>assistant: <|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# 종료 조건 설정\n",
    "stop_words = [\".\"]  # 종료를 트리거하는 키워드\n",
    "stopping_criteria = StoppingCriteriaList([StopOnKeyword(stop_words, tokenizer, summary_len)])\n",
    "\n",
    "# 입력 토큰화\n",
    "inputs = tokenizer(inputs_raw, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# 출력 생성\n",
    "outputs = loaded_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"], \n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_new_tokens=len(inputs_raw) // 100 * 15,\n",
    "    eos_token_id=128009,\n",
    "    temperature=0.4,\n",
    "    no_repeat_ngram_size=7,  # 반복을 방지\n",
    "    # repetition_penalty = 1.2,\n",
    "    stopping_criteria=stopping_criteria,  # 사용자 정의 종료 조건\n",
    "    # early_stopping=True,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# 불필요한 특수 문자 제거 및 포맷팅\n",
    "decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
    "filtered_output = decoded_output.replace(inputs_raw, '').strip()\n",
    "\n",
    "# 한자 및 일본어 제거 함수\n",
    "def remove_non_korean(text):\n",
    "    \"\"\"\n",
    "    한자(중국어) 및 일본어를 제거하는 함수.\n",
    "    유니코드 범위:\n",
    "    - 한자: \\u4E00-\\u9FFF\n",
    "    - 일본어(히라가나): \\u3040-\\u309F\n",
    "    - 일본어(가타카나): \\u30A0-\\u30FF\n",
    "    \"\"\"\n",
    "    return re.sub(r'[\\u4E00-\\u9FFF\\u3040-\\u309F\\u30A0-\\u30FF]+', '', text)\n",
    "\n",
    "# 불필요한 문자 제거 및 최종 출력\n",
    "final_output = remove_non_korean(filtered_output)\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "2359\n",
      "165\n",
      "230\n"
     ]
    }
   ],
   "source": [
    "print(summary_len)\n",
    "print(len(inputs_raw))\n",
    "print(len(final_output))\n",
    "print(len(inputs_raw) // 100 * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T03:56:41.907425Z",
     "iopub.status.busy": "2024-11-11T03:56:41.906729Z",
     "iopub.status.idle": "2024-11-11T03:56:41.923157Z",
     "shell.execute_reply": "2024-11-11T03:56:41.919042Z",
     "shell.execute_reply.started": "2024-11-11T03:56:41.907361Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%` not found.\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM 02L - LoRA with PEFT",
   "widgets": {}
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6201832,
     "sourceId": 53482,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 6169864,
     "sourceId": 54662,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30527,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
