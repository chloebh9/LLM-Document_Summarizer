{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9998093448271166,
  "eval_steps": 500,
  "global_step": 27534,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0010894581307622122,
      "grad_norm": 0.49337515234947205,
      "learning_rate": 4.998184063339871e-05,
      "loss": 3.5663,
      "step": 10
    },
    {
      "epoch": 0.0021789162615244244,
      "grad_norm": 0.35918092727661133,
      "learning_rate": 4.996368126679742e-05,
      "loss": 3.385,
      "step": 20
    },
    {
      "epoch": 0.0032683743922866364,
      "grad_norm": 0.4015496075153351,
      "learning_rate": 4.9945521900196125e-05,
      "loss": 3.3618,
      "step": 30
    },
    {
      "epoch": 0.004357832523048849,
      "grad_norm": 0.38382190465927124,
      "learning_rate": 4.992736253359483e-05,
      "loss": 3.281,
      "step": 40
    },
    {
      "epoch": 0.0054472906538110604,
      "grad_norm": 0.4459000527858734,
      "learning_rate": 4.990920316699354e-05,
      "loss": 3.2815,
      "step": 50
    },
    {
      "epoch": 0.006536748784573273,
      "grad_norm": 0.49700120091438293,
      "learning_rate": 4.989104380039225e-05,
      "loss": 3.2897,
      "step": 60
    },
    {
      "epoch": 0.007626206915335485,
      "grad_norm": 0.5270726680755615,
      "learning_rate": 4.9872884433790954e-05,
      "loss": 3.1112,
      "step": 70
    },
    {
      "epoch": 0.008715665046097698,
      "grad_norm": 0.5489051342010498,
      "learning_rate": 4.985472506718966e-05,
      "loss": 3.1853,
      "step": 80
    },
    {
      "epoch": 0.00980512317685991,
      "grad_norm": 0.5532507300376892,
      "learning_rate": 4.9836565700588365e-05,
      "loss": 3.1359,
      "step": 90
    },
    {
      "epoch": 0.010894581307622121,
      "grad_norm": 0.5555531978607178,
      "learning_rate": 4.981840633398707e-05,
      "loss": 3.1038,
      "step": 100
    },
    {
      "epoch": 0.011984039438384334,
      "grad_norm": 0.6042996048927307,
      "learning_rate": 4.9800246967385776e-05,
      "loss": 3.171,
      "step": 110
    },
    {
      "epoch": 0.013073497569146546,
      "grad_norm": 0.6337570548057556,
      "learning_rate": 4.978208760078449e-05,
      "loss": 3.0885,
      "step": 120
    },
    {
      "epoch": 0.014162955699908757,
      "grad_norm": 0.6329254508018494,
      "learning_rate": 4.976392823418319e-05,
      "loss": 3.0995,
      "step": 130
    },
    {
      "epoch": 0.01525241383067097,
      "grad_norm": 0.689115583896637,
      "learning_rate": 4.97457688675819e-05,
      "loss": 3.0945,
      "step": 140
    },
    {
      "epoch": 0.01634187196143318,
      "grad_norm": 0.7062636017799377,
      "learning_rate": 4.972760950098061e-05,
      "loss": 3.1171,
      "step": 150
    },
    {
      "epoch": 0.017431330092195396,
      "grad_norm": 0.7176716923713684,
      "learning_rate": 4.9709450134379316e-05,
      "loss": 2.9584,
      "step": 160
    },
    {
      "epoch": 0.018520788222957607,
      "grad_norm": 0.7453056573867798,
      "learning_rate": 4.969129076777802e-05,
      "loss": 3.015,
      "step": 170
    },
    {
      "epoch": 0.01961024635371982,
      "grad_norm": 0.7642362713813782,
      "learning_rate": 4.9673131401176734e-05,
      "loss": 3.0595,
      "step": 180
    },
    {
      "epoch": 0.02069970448448203,
      "grad_norm": 0.7869282364845276,
      "learning_rate": 4.965497203457544e-05,
      "loss": 2.9904,
      "step": 190
    },
    {
      "epoch": 0.021789162615244242,
      "grad_norm": 0.8123071789741516,
      "learning_rate": 4.9636812667974145e-05,
      "loss": 3.0011,
      "step": 200
    },
    {
      "epoch": 0.022878620746006453,
      "grad_norm": 0.8409108519554138,
      "learning_rate": 4.961865330137285e-05,
      "loss": 2.986,
      "step": 210
    },
    {
      "epoch": 0.02396807887676867,
      "grad_norm": 0.8482626676559448,
      "learning_rate": 4.9600493934771555e-05,
      "loss": 2.98,
      "step": 220
    },
    {
      "epoch": 0.02505753700753088,
      "grad_norm": 0.8406897187232971,
      "learning_rate": 4.958233456817026e-05,
      "loss": 3.087,
      "step": 230
    },
    {
      "epoch": 0.02614699513829309,
      "grad_norm": 0.8685596585273743,
      "learning_rate": 4.956417520156897e-05,
      "loss": 2.8582,
      "step": 240
    },
    {
      "epoch": 0.027236453269055303,
      "grad_norm": 0.843748927116394,
      "learning_rate": 4.954601583496768e-05,
      "loss": 2.9814,
      "step": 250
    },
    {
      "epoch": 0.028325911399817515,
      "grad_norm": 0.9049525856971741,
      "learning_rate": 4.9527856468366384e-05,
      "loss": 3.0558,
      "step": 260
    },
    {
      "epoch": 0.029415369530579726,
      "grad_norm": 0.897358775138855,
      "learning_rate": 4.950969710176509e-05,
      "loss": 2.9953,
      "step": 270
    },
    {
      "epoch": 0.03050482766134194,
      "grad_norm": 0.8825892806053162,
      "learning_rate": 4.94915377351638e-05,
      "loss": 2.8912,
      "step": 280
    },
    {
      "epoch": 0.03159428579210415,
      "grad_norm": 0.9581798911094666,
      "learning_rate": 4.947337836856251e-05,
      "loss": 2.8829,
      "step": 290
    },
    {
      "epoch": 0.03268374392286636,
      "grad_norm": 1.0171507596969604,
      "learning_rate": 4.945521900196121e-05,
      "loss": 2.9003,
      "step": 300
    },
    {
      "epoch": 0.033773202053628576,
      "grad_norm": 0.9813293814659119,
      "learning_rate": 4.9437059635359925e-05,
      "loss": 2.9397,
      "step": 310
    },
    {
      "epoch": 0.03486266018439079,
      "grad_norm": 1.001660943031311,
      "learning_rate": 4.941890026875863e-05,
      "loss": 2.9033,
      "step": 320
    },
    {
      "epoch": 0.035952118315153,
      "grad_norm": 1.0520695447921753,
      "learning_rate": 4.9400740902157335e-05,
      "loss": 3.0068,
      "step": 330
    },
    {
      "epoch": 0.037041576445915214,
      "grad_norm": 0.9871662259101868,
      "learning_rate": 4.938258153555604e-05,
      "loss": 2.9673,
      "step": 340
    },
    {
      "epoch": 0.03813103457667742,
      "grad_norm": 1.1198434829711914,
      "learning_rate": 4.9364422168954746e-05,
      "loss": 2.8901,
      "step": 350
    },
    {
      "epoch": 0.03922049270743964,
      "grad_norm": 1.0224937200546265,
      "learning_rate": 4.934626280235345e-05,
      "loss": 2.9762,
      "step": 360
    },
    {
      "epoch": 0.04030995083820185,
      "grad_norm": 1.007568359375,
      "learning_rate": 4.9328103435752164e-05,
      "loss": 2.9451,
      "step": 370
    },
    {
      "epoch": 0.04139940896896406,
      "grad_norm": 1.046326756477356,
      "learning_rate": 4.930994406915087e-05,
      "loss": 2.9329,
      "step": 380
    },
    {
      "epoch": 0.042488867099726275,
      "grad_norm": 1.0188579559326172,
      "learning_rate": 4.9291784702549575e-05,
      "loss": 2.8844,
      "step": 390
    },
    {
      "epoch": 0.043578325230488484,
      "grad_norm": 1.0457251071929932,
      "learning_rate": 4.927362533594829e-05,
      "loss": 2.8517,
      "step": 400
    },
    {
      "epoch": 0.0446677833612507,
      "grad_norm": 1.0373866558074951,
      "learning_rate": 4.925546596934699e-05,
      "loss": 2.9619,
      "step": 410
    },
    {
      "epoch": 0.04575724149201291,
      "grad_norm": 1.0077989101409912,
      "learning_rate": 4.92373066027457e-05,
      "loss": 2.7998,
      "step": 420
    },
    {
      "epoch": 0.04684669962277512,
      "grad_norm": 1.0368223190307617,
      "learning_rate": 4.921914723614441e-05,
      "loss": 2.8232,
      "step": 430
    },
    {
      "epoch": 0.04793615775353734,
      "grad_norm": 1.0901669263839722,
      "learning_rate": 4.9200987869543115e-05,
      "loss": 2.7808,
      "step": 440
    },
    {
      "epoch": 0.049025615884299545,
      "grad_norm": 1.0386250019073486,
      "learning_rate": 4.918282850294182e-05,
      "loss": 2.886,
      "step": 450
    },
    {
      "epoch": 0.05011507401506176,
      "grad_norm": 1.018742561340332,
      "learning_rate": 4.9164669136340526e-05,
      "loss": 2.8024,
      "step": 460
    },
    {
      "epoch": 0.05120453214582397,
      "grad_norm": 1.1162716150283813,
      "learning_rate": 4.914650976973924e-05,
      "loss": 2.8891,
      "step": 470
    },
    {
      "epoch": 0.05229399027658618,
      "grad_norm": 1.147857904434204,
      "learning_rate": 4.9128350403137944e-05,
      "loss": 2.9324,
      "step": 480
    },
    {
      "epoch": 0.0533834484073484,
      "grad_norm": 1.1329033374786377,
      "learning_rate": 4.911019103653664e-05,
      "loss": 2.8199,
      "step": 490
    },
    {
      "epoch": 0.054472906538110606,
      "grad_norm": 1.116186499595642,
      "learning_rate": 4.9092031669935355e-05,
      "loss": 2.8458,
      "step": 500
    },
    {
      "epoch": 0.05556236466887282,
      "grad_norm": 1.0459486246109009,
      "learning_rate": 4.907387230333406e-05,
      "loss": 2.865,
      "step": 510
    },
    {
      "epoch": 0.05665182279963503,
      "grad_norm": 1.0720957517623901,
      "learning_rate": 4.9055712936732765e-05,
      "loss": 2.7765,
      "step": 520
    },
    {
      "epoch": 0.057741280930397244,
      "grad_norm": 1.1766952276229858,
      "learning_rate": 4.903755357013148e-05,
      "loss": 2.7995,
      "step": 530
    },
    {
      "epoch": 0.05883073906115945,
      "grad_norm": 1.1198848485946655,
      "learning_rate": 4.901939420353018e-05,
      "loss": 2.8466,
      "step": 540
    },
    {
      "epoch": 0.05992019719192167,
      "grad_norm": 1.0979403257369995,
      "learning_rate": 4.900123483692889e-05,
      "loss": 2.8525,
      "step": 550
    },
    {
      "epoch": 0.06100965532268388,
      "grad_norm": 1.0722994804382324,
      "learning_rate": 4.89830754703276e-05,
      "loss": 2.8633,
      "step": 560
    },
    {
      "epoch": 0.06209911345344609,
      "grad_norm": 1.0546061992645264,
      "learning_rate": 4.8964916103726306e-05,
      "loss": 2.8764,
      "step": 570
    },
    {
      "epoch": 0.0631885715842083,
      "grad_norm": 1.113311529159546,
      "learning_rate": 4.894675673712501e-05,
      "loss": 2.7831,
      "step": 580
    },
    {
      "epoch": 0.06427802971497051,
      "grad_norm": 1.1238245964050293,
      "learning_rate": 4.8928597370523724e-05,
      "loss": 2.795,
      "step": 590
    },
    {
      "epoch": 0.06536748784573272,
      "grad_norm": 1.1983128786087036,
      "learning_rate": 4.891043800392243e-05,
      "loss": 2.8839,
      "step": 600
    },
    {
      "epoch": 0.06645694597649494,
      "grad_norm": 1.1140832901000977,
      "learning_rate": 4.8892278637321135e-05,
      "loss": 2.8843,
      "step": 610
    },
    {
      "epoch": 0.06754640410725715,
      "grad_norm": 1.1732449531555176,
      "learning_rate": 4.887411927071984e-05,
      "loss": 2.8592,
      "step": 620
    },
    {
      "epoch": 0.06863586223801936,
      "grad_norm": 1.1526854038238525,
      "learning_rate": 4.8855959904118545e-05,
      "loss": 2.8531,
      "step": 630
    },
    {
      "epoch": 0.06972532036878158,
      "grad_norm": 1.129225254058838,
      "learning_rate": 4.883780053751725e-05,
      "loss": 2.7262,
      "step": 640
    },
    {
      "epoch": 0.07081477849954379,
      "grad_norm": 1.1282987594604492,
      "learning_rate": 4.8819641170915956e-05,
      "loss": 2.8116,
      "step": 650
    },
    {
      "epoch": 0.071904236630306,
      "grad_norm": 1.1514261960983276,
      "learning_rate": 4.880148180431467e-05,
      "loss": 2.8308,
      "step": 660
    },
    {
      "epoch": 0.07299369476106822,
      "grad_norm": 1.1951009035110474,
      "learning_rate": 4.8783322437713374e-05,
      "loss": 2.8572,
      "step": 670
    },
    {
      "epoch": 0.07408315289183043,
      "grad_norm": 1.182863473892212,
      "learning_rate": 4.876516307111208e-05,
      "loss": 2.805,
      "step": 680
    },
    {
      "epoch": 0.07517261102259264,
      "grad_norm": 1.144089698791504,
      "learning_rate": 4.874700370451079e-05,
      "loss": 2.8165,
      "step": 690
    },
    {
      "epoch": 0.07626206915335484,
      "grad_norm": 1.2241885662078857,
      "learning_rate": 4.87288443379095e-05,
      "loss": 2.8172,
      "step": 700
    },
    {
      "epoch": 0.07735152728411707,
      "grad_norm": 1.1296168565750122,
      "learning_rate": 4.87106849713082e-05,
      "loss": 2.7937,
      "step": 710
    },
    {
      "epoch": 0.07844098541487927,
      "grad_norm": 1.208116054534912,
      "learning_rate": 4.8692525604706914e-05,
      "loss": 2.814,
      "step": 720
    },
    {
      "epoch": 0.07953044354564148,
      "grad_norm": 1.1010546684265137,
      "learning_rate": 4.867436623810562e-05,
      "loss": 2.7366,
      "step": 730
    },
    {
      "epoch": 0.0806199016764037,
      "grad_norm": 1.2124369144439697,
      "learning_rate": 4.8656206871504325e-05,
      "loss": 2.8321,
      "step": 740
    },
    {
      "epoch": 0.08170935980716591,
      "grad_norm": 1.1456384658813477,
      "learning_rate": 4.863804750490303e-05,
      "loss": 2.8328,
      "step": 750
    },
    {
      "epoch": 0.08279881793792812,
      "grad_norm": 1.2125318050384521,
      "learning_rate": 4.8619888138301736e-05,
      "loss": 2.8877,
      "step": 760
    },
    {
      "epoch": 0.08388827606869033,
      "grad_norm": 1.195249319076538,
      "learning_rate": 4.860172877170044e-05,
      "loss": 2.8082,
      "step": 770
    },
    {
      "epoch": 0.08497773419945255,
      "grad_norm": 1.1922619342803955,
      "learning_rate": 4.8583569405099154e-05,
      "loss": 2.8011,
      "step": 780
    },
    {
      "epoch": 0.08606719233021476,
      "grad_norm": 1.3151921033859253,
      "learning_rate": 4.856541003849786e-05,
      "loss": 2.829,
      "step": 790
    },
    {
      "epoch": 0.08715665046097697,
      "grad_norm": 1.1977815628051758,
      "learning_rate": 4.8547250671896565e-05,
      "loss": 2.7463,
      "step": 800
    },
    {
      "epoch": 0.08824610859173919,
      "grad_norm": 1.1851061582565308,
      "learning_rate": 4.852909130529527e-05,
      "loss": 2.733,
      "step": 810
    },
    {
      "epoch": 0.0893355667225014,
      "grad_norm": 1.1875284910202026,
      "learning_rate": 4.851093193869398e-05,
      "loss": 2.7506,
      "step": 820
    },
    {
      "epoch": 0.0904250248532636,
      "grad_norm": 1.200934648513794,
      "learning_rate": 4.849277257209269e-05,
      "loss": 2.8217,
      "step": 830
    },
    {
      "epoch": 0.09151448298402581,
      "grad_norm": 1.2399680614471436,
      "learning_rate": 4.847461320549139e-05,
      "loss": 2.7663,
      "step": 840
    },
    {
      "epoch": 0.09260394111478804,
      "grad_norm": 1.1945531368255615,
      "learning_rate": 4.8456453838890105e-05,
      "loss": 2.7804,
      "step": 850
    },
    {
      "epoch": 0.09369339924555024,
      "grad_norm": 1.1823039054870605,
      "learning_rate": 4.843829447228881e-05,
      "loss": 2.7587,
      "step": 860
    },
    {
      "epoch": 0.09478285737631245,
      "grad_norm": 1.220780611038208,
      "learning_rate": 4.8420135105687516e-05,
      "loss": 2.8214,
      "step": 870
    },
    {
      "epoch": 0.09587231550707467,
      "grad_norm": 1.19318687915802,
      "learning_rate": 4.840197573908623e-05,
      "loss": 2.8342,
      "step": 880
    },
    {
      "epoch": 0.09696177363783688,
      "grad_norm": 1.2889076471328735,
      "learning_rate": 4.8383816372484934e-05,
      "loss": 2.839,
      "step": 890
    },
    {
      "epoch": 0.09805123176859909,
      "grad_norm": 1.283867597579956,
      "learning_rate": 4.836565700588363e-05,
      "loss": 2.7817,
      "step": 900
    },
    {
      "epoch": 0.09914068989936131,
      "grad_norm": 1.2201305627822876,
      "learning_rate": 4.8347497639282344e-05,
      "loss": 2.8099,
      "step": 910
    },
    {
      "epoch": 0.10023014803012352,
      "grad_norm": 1.2301584482192993,
      "learning_rate": 4.832933827268105e-05,
      "loss": 2.7972,
      "step": 920
    },
    {
      "epoch": 0.10131960616088573,
      "grad_norm": 1.1875859498977661,
      "learning_rate": 4.8311178906079755e-05,
      "loss": 2.7562,
      "step": 930
    },
    {
      "epoch": 0.10240906429164794,
      "grad_norm": 1.1737761497497559,
      "learning_rate": 4.829301953947847e-05,
      "loss": 2.7746,
      "step": 940
    },
    {
      "epoch": 0.10349852242241016,
      "grad_norm": 1.1570301055908203,
      "learning_rate": 4.827486017287717e-05,
      "loss": 2.7392,
      "step": 950
    },
    {
      "epoch": 0.10458798055317237,
      "grad_norm": 1.2130146026611328,
      "learning_rate": 4.825670080627588e-05,
      "loss": 2.7618,
      "step": 960
    },
    {
      "epoch": 0.10567743868393457,
      "grad_norm": 1.236517310142517,
      "learning_rate": 4.8238541439674584e-05,
      "loss": 2.8402,
      "step": 970
    },
    {
      "epoch": 0.1067668968146968,
      "grad_norm": 1.1492834091186523,
      "learning_rate": 4.8220382073073296e-05,
      "loss": 2.7762,
      "step": 980
    },
    {
      "epoch": 0.107856354945459,
      "grad_norm": 1.249451994895935,
      "learning_rate": 4.8202222706472e-05,
      "loss": 2.821,
      "step": 990
    },
    {
      "epoch": 0.10894581307622121,
      "grad_norm": 1.2664724588394165,
      "learning_rate": 4.818406333987071e-05,
      "loss": 2.8131,
      "step": 1000
    },
    {
      "epoch": 0.11003527120698342,
      "grad_norm": 1.1343072652816772,
      "learning_rate": 4.816590397326942e-05,
      "loss": 2.6541,
      "step": 1010
    },
    {
      "epoch": 0.11112472933774564,
      "grad_norm": 1.2026466131210327,
      "learning_rate": 4.8147744606668124e-05,
      "loss": 2.6434,
      "step": 1020
    },
    {
      "epoch": 0.11221418746850785,
      "grad_norm": 1.2138466835021973,
      "learning_rate": 4.812958524006683e-05,
      "loss": 2.746,
      "step": 1030
    },
    {
      "epoch": 0.11330364559927006,
      "grad_norm": 1.30343496799469,
      "learning_rate": 4.8111425873465535e-05,
      "loss": 2.7857,
      "step": 1040
    },
    {
      "epoch": 0.11439310373003228,
      "grad_norm": 1.2468417882919312,
      "learning_rate": 4.809326650686424e-05,
      "loss": 2.6905,
      "step": 1050
    },
    {
      "epoch": 0.11548256186079449,
      "grad_norm": 1.326042890548706,
      "learning_rate": 4.8075107140262946e-05,
      "loss": 2.8474,
      "step": 1060
    },
    {
      "epoch": 0.1165720199915567,
      "grad_norm": 1.1947084665298462,
      "learning_rate": 4.805694777366166e-05,
      "loss": 2.8117,
      "step": 1070
    },
    {
      "epoch": 0.1176614781223189,
      "grad_norm": 1.1621543169021606,
      "learning_rate": 4.8038788407060364e-05,
      "loss": 2.6848,
      "step": 1080
    },
    {
      "epoch": 0.11875093625308113,
      "grad_norm": 1.1810951232910156,
      "learning_rate": 4.802062904045907e-05,
      "loss": 2.8024,
      "step": 1090
    },
    {
      "epoch": 0.11984039438384333,
      "grad_norm": 1.2513697147369385,
      "learning_rate": 4.800246967385778e-05,
      "loss": 2.7657,
      "step": 1100
    },
    {
      "epoch": 0.12092985251460554,
      "grad_norm": 1.1854441165924072,
      "learning_rate": 4.798431030725649e-05,
      "loss": 2.7226,
      "step": 1110
    },
    {
      "epoch": 0.12201931064536777,
      "grad_norm": 1.1704556941986084,
      "learning_rate": 4.796615094065519e-05,
      "loss": 2.7107,
      "step": 1120
    },
    {
      "epoch": 0.12310876877612997,
      "grad_norm": 1.2327921390533447,
      "learning_rate": 4.79479915740539e-05,
      "loss": 2.8107,
      "step": 1130
    },
    {
      "epoch": 0.12419822690689218,
      "grad_norm": 1.2018492221832275,
      "learning_rate": 4.792983220745261e-05,
      "loss": 2.682,
      "step": 1140
    },
    {
      "epoch": 0.1252876850376544,
      "grad_norm": 1.3062317371368408,
      "learning_rate": 4.7911672840851315e-05,
      "loss": 2.8374,
      "step": 1150
    },
    {
      "epoch": 0.1263771431684166,
      "grad_norm": 1.245087742805481,
      "learning_rate": 4.789351347425002e-05,
      "loss": 2.6968,
      "step": 1160
    },
    {
      "epoch": 0.12746660129917883,
      "grad_norm": 1.197292685508728,
      "learning_rate": 4.7875354107648726e-05,
      "loss": 2.6993,
      "step": 1170
    },
    {
      "epoch": 0.12855605942994103,
      "grad_norm": 1.3440479040145874,
      "learning_rate": 4.785719474104743e-05,
      "loss": 2.7997,
      "step": 1180
    },
    {
      "epoch": 0.12964551756070325,
      "grad_norm": 1.1750173568725586,
      "learning_rate": 4.783903537444614e-05,
      "loss": 2.7195,
      "step": 1190
    },
    {
      "epoch": 0.13073497569146544,
      "grad_norm": 1.2725480794906616,
      "learning_rate": 4.782087600784485e-05,
      "loss": 2.7433,
      "step": 1200
    },
    {
      "epoch": 0.13182443382222767,
      "grad_norm": 1.2401162385940552,
      "learning_rate": 4.7802716641243554e-05,
      "loss": 2.7067,
      "step": 1210
    },
    {
      "epoch": 0.1329138919529899,
      "grad_norm": 1.2157944440841675,
      "learning_rate": 4.778455727464226e-05,
      "loss": 2.7102,
      "step": 1220
    },
    {
      "epoch": 0.13400335008375208,
      "grad_norm": 1.2676326036453247,
      "learning_rate": 4.776639790804097e-05,
      "loss": 2.7767,
      "step": 1230
    },
    {
      "epoch": 0.1350928082145143,
      "grad_norm": 1.2151172161102295,
      "learning_rate": 4.774823854143968e-05,
      "loss": 2.7011,
      "step": 1240
    },
    {
      "epoch": 0.13618226634527653,
      "grad_norm": 1.3062111139297485,
      "learning_rate": 4.773007917483838e-05,
      "loss": 2.8015,
      "step": 1250
    },
    {
      "epoch": 0.13727172447603872,
      "grad_norm": 1.3215579986572266,
      "learning_rate": 4.7711919808237095e-05,
      "loss": 2.7261,
      "step": 1260
    },
    {
      "epoch": 0.13836118260680094,
      "grad_norm": 1.317230224609375,
      "learning_rate": 4.76937604416358e-05,
      "loss": 2.7576,
      "step": 1270
    },
    {
      "epoch": 0.13945064073756316,
      "grad_norm": 1.256903052330017,
      "learning_rate": 4.7675601075034506e-05,
      "loss": 2.7734,
      "step": 1280
    },
    {
      "epoch": 0.14054009886832536,
      "grad_norm": 1.17270028591156,
      "learning_rate": 4.765744170843322e-05,
      "loss": 2.7585,
      "step": 1290
    },
    {
      "epoch": 0.14162955699908758,
      "grad_norm": 1.213025689125061,
      "learning_rate": 4.763928234183192e-05,
      "loss": 2.7413,
      "step": 1300
    },
    {
      "epoch": 0.1427190151298498,
      "grad_norm": 1.1714422702789307,
      "learning_rate": 4.762112297523062e-05,
      "loss": 2.7128,
      "step": 1310
    },
    {
      "epoch": 0.143808473260612,
      "grad_norm": 1.3681360483169556,
      "learning_rate": 4.760296360862933e-05,
      "loss": 2.7945,
      "step": 1320
    },
    {
      "epoch": 0.14489793139137422,
      "grad_norm": 1.22855806350708,
      "learning_rate": 4.758480424202804e-05,
      "loss": 2.8219,
      "step": 1330
    },
    {
      "epoch": 0.14598738952213644,
      "grad_norm": 1.2463974952697754,
      "learning_rate": 4.7566644875426745e-05,
      "loss": 2.8655,
      "step": 1340
    },
    {
      "epoch": 0.14707684765289863,
      "grad_norm": 1.266064167022705,
      "learning_rate": 4.754848550882545e-05,
      "loss": 2.7535,
      "step": 1350
    },
    {
      "epoch": 0.14816630578366086,
      "grad_norm": 1.2456073760986328,
      "learning_rate": 4.753032614222416e-05,
      "loss": 2.8689,
      "step": 1360
    },
    {
      "epoch": 0.14925576391442305,
      "grad_norm": 1.2722535133361816,
      "learning_rate": 4.751216677562287e-05,
      "loss": 2.6704,
      "step": 1370
    },
    {
      "epoch": 0.15034522204518527,
      "grad_norm": 1.2632468938827515,
      "learning_rate": 4.7494007409021574e-05,
      "loss": 2.8157,
      "step": 1380
    },
    {
      "epoch": 0.1514346801759475,
      "grad_norm": 1.327122449874878,
      "learning_rate": 4.7475848042420286e-05,
      "loss": 2.6848,
      "step": 1390
    },
    {
      "epoch": 0.1525241383067097,
      "grad_norm": 1.154883861541748,
      "learning_rate": 4.745768867581899e-05,
      "loss": 2.8629,
      "step": 1400
    },
    {
      "epoch": 0.1536135964374719,
      "grad_norm": 1.220727562904358,
      "learning_rate": 4.74395293092177e-05,
      "loss": 2.7189,
      "step": 1410
    },
    {
      "epoch": 0.15470305456823413,
      "grad_norm": 1.2765330076217651,
      "learning_rate": 4.742136994261641e-05,
      "loss": 2.797,
      "step": 1420
    },
    {
      "epoch": 0.15579251269899633,
      "grad_norm": 1.2429543733596802,
      "learning_rate": 4.7403210576015114e-05,
      "loss": 2.8404,
      "step": 1430
    },
    {
      "epoch": 0.15688197082975855,
      "grad_norm": 1.2594168186187744,
      "learning_rate": 4.738505120941382e-05,
      "loss": 2.719,
      "step": 1440
    },
    {
      "epoch": 0.15797142896052077,
      "grad_norm": 1.3317071199417114,
      "learning_rate": 4.7366891842812525e-05,
      "loss": 2.7435,
      "step": 1450
    },
    {
      "epoch": 0.15906088709128297,
      "grad_norm": 1.3132165670394897,
      "learning_rate": 4.734873247621123e-05,
      "loss": 2.6367,
      "step": 1460
    },
    {
      "epoch": 0.1601503452220452,
      "grad_norm": 1.1927005052566528,
      "learning_rate": 4.7330573109609936e-05,
      "loss": 2.7073,
      "step": 1470
    },
    {
      "epoch": 0.1612398033528074,
      "grad_norm": 1.2846953868865967,
      "learning_rate": 4.731241374300864e-05,
      "loss": 2.7825,
      "step": 1480
    },
    {
      "epoch": 0.1623292614835696,
      "grad_norm": 1.2512742280960083,
      "learning_rate": 4.7294254376407354e-05,
      "loss": 2.6929,
      "step": 1490
    },
    {
      "epoch": 0.16341871961433183,
      "grad_norm": 1.1661051511764526,
      "learning_rate": 4.727609500980606e-05,
      "loss": 2.6878,
      "step": 1500
    },
    {
      "epoch": 0.16450817774509405,
      "grad_norm": 1.2492479085922241,
      "learning_rate": 4.7257935643204764e-05,
      "loss": 2.7048,
      "step": 1510
    },
    {
      "epoch": 0.16559763587585624,
      "grad_norm": 1.2207304239273071,
      "learning_rate": 4.7239776276603477e-05,
      "loss": 2.8055,
      "step": 1520
    },
    {
      "epoch": 0.16668709400661846,
      "grad_norm": 1.40451979637146,
      "learning_rate": 4.722161691000218e-05,
      "loss": 2.7651,
      "step": 1530
    },
    {
      "epoch": 0.16777655213738066,
      "grad_norm": 1.2768832445144653,
      "learning_rate": 4.720345754340089e-05,
      "loss": 2.6859,
      "step": 1540
    },
    {
      "epoch": 0.16886601026814288,
      "grad_norm": 1.2269477844238281,
      "learning_rate": 4.71852981767996e-05,
      "loss": 2.775,
      "step": 1550
    },
    {
      "epoch": 0.1699554683989051,
      "grad_norm": 1.2533618211746216,
      "learning_rate": 4.7167138810198305e-05,
      "loss": 2.717,
      "step": 1560
    },
    {
      "epoch": 0.1710449265296673,
      "grad_norm": 1.1985969543457031,
      "learning_rate": 4.714897944359701e-05,
      "loss": 2.8034,
      "step": 1570
    },
    {
      "epoch": 0.17213438466042952,
      "grad_norm": 1.2504732608795166,
      "learning_rate": 4.7130820076995716e-05,
      "loss": 2.7163,
      "step": 1580
    },
    {
      "epoch": 0.17322384279119174,
      "grad_norm": 1.253906488418579,
      "learning_rate": 4.711266071039442e-05,
      "loss": 2.6735,
      "step": 1590
    },
    {
      "epoch": 0.17431330092195393,
      "grad_norm": 1.2603689432144165,
      "learning_rate": 4.709450134379313e-05,
      "loss": 2.7475,
      "step": 1600
    },
    {
      "epoch": 0.17540275905271616,
      "grad_norm": 1.3533943891525269,
      "learning_rate": 4.707634197719184e-05,
      "loss": 2.7655,
      "step": 1610
    },
    {
      "epoch": 0.17649221718347838,
      "grad_norm": 1.1376516819000244,
      "learning_rate": 4.7058182610590544e-05,
      "loss": 2.6992,
      "step": 1620
    },
    {
      "epoch": 0.17758167531424057,
      "grad_norm": 1.2986791133880615,
      "learning_rate": 4.704002324398925e-05,
      "loss": 2.6802,
      "step": 1630
    },
    {
      "epoch": 0.1786711334450028,
      "grad_norm": 1.3078720569610596,
      "learning_rate": 4.702186387738796e-05,
      "loss": 2.7305,
      "step": 1640
    },
    {
      "epoch": 0.17976059157576502,
      "grad_norm": 1.2935441732406616,
      "learning_rate": 4.700370451078667e-05,
      "loss": 2.7136,
      "step": 1650
    },
    {
      "epoch": 0.1808500497065272,
      "grad_norm": 1.1997007131576538,
      "learning_rate": 4.698554514418537e-05,
      "loss": 2.6312,
      "step": 1660
    },
    {
      "epoch": 0.18193950783728943,
      "grad_norm": 1.215439796447754,
      "learning_rate": 4.696738577758408e-05,
      "loss": 2.6487,
      "step": 1670
    },
    {
      "epoch": 0.18302896596805163,
      "grad_norm": 1.2185757160186768,
      "learning_rate": 4.694922641098279e-05,
      "loss": 2.7225,
      "step": 1680
    },
    {
      "epoch": 0.18411842409881385,
      "grad_norm": 1.2053978443145752,
      "learning_rate": 4.6931067044381496e-05,
      "loss": 2.7024,
      "step": 1690
    },
    {
      "epoch": 0.18520788222957607,
      "grad_norm": 1.2750911712646484,
      "learning_rate": 4.69129076777802e-05,
      "loss": 2.67,
      "step": 1700
    },
    {
      "epoch": 0.18629734036033826,
      "grad_norm": 1.253408432006836,
      "learning_rate": 4.689474831117891e-05,
      "loss": 2.7185,
      "step": 1710
    },
    {
      "epoch": 0.1873867984911005,
      "grad_norm": 1.2258821725845337,
      "learning_rate": 4.687658894457761e-05,
      "loss": 2.7865,
      "step": 1720
    },
    {
      "epoch": 0.1884762566218627,
      "grad_norm": 1.2240804433822632,
      "learning_rate": 4.685842957797632e-05,
      "loss": 2.6872,
      "step": 1730
    },
    {
      "epoch": 0.1895657147526249,
      "grad_norm": 1.1812340021133423,
      "learning_rate": 4.684027021137503e-05,
      "loss": 2.7528,
      "step": 1740
    },
    {
      "epoch": 0.19065517288338713,
      "grad_norm": 1.3277987241744995,
      "learning_rate": 4.6822110844773735e-05,
      "loss": 2.7153,
      "step": 1750
    },
    {
      "epoch": 0.19174463101414935,
      "grad_norm": 1.2630590200424194,
      "learning_rate": 4.680395147817244e-05,
      "loss": 2.7483,
      "step": 1760
    },
    {
      "epoch": 0.19283408914491154,
      "grad_norm": 1.2514805793762207,
      "learning_rate": 4.678579211157115e-05,
      "loss": 2.7654,
      "step": 1770
    },
    {
      "epoch": 0.19392354727567376,
      "grad_norm": 1.138802170753479,
      "learning_rate": 4.676763274496986e-05,
      "loss": 2.7875,
      "step": 1780
    },
    {
      "epoch": 0.19501300540643599,
      "grad_norm": 1.3267946243286133,
      "learning_rate": 4.6749473378368564e-05,
      "loss": 2.5879,
      "step": 1790
    },
    {
      "epoch": 0.19610246353719818,
      "grad_norm": 1.244777798652649,
      "learning_rate": 4.6731314011767276e-05,
      "loss": 2.7928,
      "step": 1800
    },
    {
      "epoch": 0.1971919216679604,
      "grad_norm": 1.1906964778900146,
      "learning_rate": 4.671315464516598e-05,
      "loss": 2.7234,
      "step": 1810
    },
    {
      "epoch": 0.19828137979872262,
      "grad_norm": 1.3042926788330078,
      "learning_rate": 4.6694995278564687e-05,
      "loss": 2.6849,
      "step": 1820
    },
    {
      "epoch": 0.19937083792948482,
      "grad_norm": 1.2714248895645142,
      "learning_rate": 4.667683591196339e-05,
      "loss": 2.7142,
      "step": 1830
    },
    {
      "epoch": 0.20046029606024704,
      "grad_norm": 1.2367244958877563,
      "learning_rate": 4.6658676545362104e-05,
      "loss": 2.8301,
      "step": 1840
    },
    {
      "epoch": 0.20154975419100923,
      "grad_norm": 1.2052509784698486,
      "learning_rate": 4.664051717876081e-05,
      "loss": 2.6363,
      "step": 1850
    },
    {
      "epoch": 0.20263921232177146,
      "grad_norm": 1.2075047492980957,
      "learning_rate": 4.6622357812159515e-05,
      "loss": 2.7002,
      "step": 1860
    },
    {
      "epoch": 0.20372867045253368,
      "grad_norm": 1.278774380683899,
      "learning_rate": 4.660419844555822e-05,
      "loss": 2.692,
      "step": 1870
    },
    {
      "epoch": 0.20481812858329587,
      "grad_norm": 1.2673934698104858,
      "learning_rate": 4.6586039078956926e-05,
      "loss": 2.7011,
      "step": 1880
    },
    {
      "epoch": 0.2059075867140581,
      "grad_norm": 1.197874665260315,
      "learning_rate": 4.656787971235563e-05,
      "loss": 2.683,
      "step": 1890
    },
    {
      "epoch": 0.20699704484482032,
      "grad_norm": 1.2713991403579712,
      "learning_rate": 4.6549720345754343e-05,
      "loss": 2.7382,
      "step": 1900
    },
    {
      "epoch": 0.2080865029755825,
      "grad_norm": 1.2523462772369385,
      "learning_rate": 4.653156097915305e-05,
      "loss": 2.7814,
      "step": 1910
    },
    {
      "epoch": 0.20917596110634473,
      "grad_norm": 1.2490849494934082,
      "learning_rate": 4.6513401612551754e-05,
      "loss": 2.6726,
      "step": 1920
    },
    {
      "epoch": 0.21026541923710695,
      "grad_norm": 1.2228749990463257,
      "learning_rate": 4.6495242245950466e-05,
      "loss": 2.6842,
      "step": 1930
    },
    {
      "epoch": 0.21135487736786915,
      "grad_norm": 1.279780626296997,
      "learning_rate": 4.647708287934917e-05,
      "loss": 2.6745,
      "step": 1940
    },
    {
      "epoch": 0.21244433549863137,
      "grad_norm": 1.2123068571090698,
      "learning_rate": 4.645892351274788e-05,
      "loss": 2.7341,
      "step": 1950
    },
    {
      "epoch": 0.2135337936293936,
      "grad_norm": 1.303430438041687,
      "learning_rate": 4.644076414614659e-05,
      "loss": 2.6915,
      "step": 1960
    },
    {
      "epoch": 0.2146232517601558,
      "grad_norm": 1.2355924844741821,
      "learning_rate": 4.6422604779545295e-05,
      "loss": 2.7712,
      "step": 1970
    },
    {
      "epoch": 0.215712709890918,
      "grad_norm": 1.1985111236572266,
      "learning_rate": 4.6404445412944e-05,
      "loss": 2.82,
      "step": 1980
    },
    {
      "epoch": 0.21680216802168023,
      "grad_norm": 1.2027961015701294,
      "learning_rate": 4.6386286046342706e-05,
      "loss": 2.7545,
      "step": 1990
    },
    {
      "epoch": 0.21789162615244242,
      "grad_norm": 1.2369203567504883,
      "learning_rate": 4.636812667974141e-05,
      "loss": 2.7131,
      "step": 2000
    },
    {
      "epoch": 0.21898108428320465,
      "grad_norm": 1.3108192682266235,
      "learning_rate": 4.6349967313140117e-05,
      "loss": 2.7691,
      "step": 2010
    },
    {
      "epoch": 0.22007054241396684,
      "grad_norm": 1.2137151956558228,
      "learning_rate": 4.633180794653882e-05,
      "loss": 2.7163,
      "step": 2020
    },
    {
      "epoch": 0.22116000054472906,
      "grad_norm": 1.3322504758834839,
      "learning_rate": 4.6313648579937534e-05,
      "loss": 2.7001,
      "step": 2030
    },
    {
      "epoch": 0.22224945867549128,
      "grad_norm": 1.2671995162963867,
      "learning_rate": 4.629548921333624e-05,
      "loss": 2.7854,
      "step": 2040
    },
    {
      "epoch": 0.22333891680625348,
      "grad_norm": 1.2403817176818848,
      "learning_rate": 4.6277329846734945e-05,
      "loss": 2.7756,
      "step": 2050
    },
    {
      "epoch": 0.2244283749370157,
      "grad_norm": 1.2428702116012573,
      "learning_rate": 4.625917048013366e-05,
      "loss": 2.7253,
      "step": 2060
    },
    {
      "epoch": 0.22551783306777792,
      "grad_norm": 1.2223135232925415,
      "learning_rate": 4.624101111353236e-05,
      "loss": 2.6291,
      "step": 2070
    },
    {
      "epoch": 0.22660729119854012,
      "grad_norm": 1.2553657293319702,
      "learning_rate": 4.622285174693107e-05,
      "loss": 2.706,
      "step": 2080
    },
    {
      "epoch": 0.22769674932930234,
      "grad_norm": 1.182691216468811,
      "learning_rate": 4.620469238032978e-05,
      "loss": 2.7114,
      "step": 2090
    },
    {
      "epoch": 0.22878620746006456,
      "grad_norm": 1.2516015768051147,
      "learning_rate": 4.6186533013728486e-05,
      "loss": 2.6866,
      "step": 2100
    },
    {
      "epoch": 0.22987566559082676,
      "grad_norm": 1.2408056259155273,
      "learning_rate": 4.616837364712719e-05,
      "loss": 2.7275,
      "step": 2110
    },
    {
      "epoch": 0.23096512372158898,
      "grad_norm": 1.256229043006897,
      "learning_rate": 4.6150214280525897e-05,
      "loss": 2.587,
      "step": 2120
    },
    {
      "epoch": 0.2320545818523512,
      "grad_norm": 1.2243475914001465,
      "learning_rate": 4.61320549139246e-05,
      "loss": 2.6501,
      "step": 2130
    },
    {
      "epoch": 0.2331440399831134,
      "grad_norm": 1.2978851795196533,
      "learning_rate": 4.611389554732331e-05,
      "loss": 2.6834,
      "step": 2140
    },
    {
      "epoch": 0.23423349811387562,
      "grad_norm": 1.2961868047714233,
      "learning_rate": 4.609573618072202e-05,
      "loss": 2.7455,
      "step": 2150
    },
    {
      "epoch": 0.2353229562446378,
      "grad_norm": 1.2204844951629639,
      "learning_rate": 4.6077576814120725e-05,
      "loss": 2.7066,
      "step": 2160
    },
    {
      "epoch": 0.23641241437540003,
      "grad_norm": 1.2122445106506348,
      "learning_rate": 4.605941744751943e-05,
      "loss": 2.6094,
      "step": 2170
    },
    {
      "epoch": 0.23750187250616225,
      "grad_norm": 1.3436051607131958,
      "learning_rate": 4.6041258080918136e-05,
      "loss": 2.7512,
      "step": 2180
    },
    {
      "epoch": 0.23859133063692445,
      "grad_norm": 1.2830290794372559,
      "learning_rate": 4.602309871431685e-05,
      "loss": 2.6198,
      "step": 2190
    },
    {
      "epoch": 0.23968078876768667,
      "grad_norm": 1.2590018510818481,
      "learning_rate": 4.6004939347715553e-05,
      "loss": 2.7152,
      "step": 2200
    },
    {
      "epoch": 0.2407702468984489,
      "grad_norm": 1.2579264640808105,
      "learning_rate": 4.598677998111426e-05,
      "loss": 2.7007,
      "step": 2210
    },
    {
      "epoch": 0.2418597050292111,
      "grad_norm": 1.2514464855194092,
      "learning_rate": 4.596862061451297e-05,
      "loss": 2.6595,
      "step": 2220
    },
    {
      "epoch": 0.2429491631599733,
      "grad_norm": 1.3714871406555176,
      "learning_rate": 4.5950461247911676e-05,
      "loss": 2.664,
      "step": 2230
    },
    {
      "epoch": 0.24403862129073553,
      "grad_norm": 1.2419304847717285,
      "learning_rate": 4.593230188131038e-05,
      "loss": 2.6362,
      "step": 2240
    },
    {
      "epoch": 0.24512807942149772,
      "grad_norm": 1.3406773805618286,
      "learning_rate": 4.5914142514709094e-05,
      "loss": 2.633,
      "step": 2250
    },
    {
      "epoch": 0.24621753755225995,
      "grad_norm": 1.278499722480774,
      "learning_rate": 4.58959831481078e-05,
      "loss": 2.6503,
      "step": 2260
    },
    {
      "epoch": 0.24730699568302217,
      "grad_norm": 1.3156598806381226,
      "learning_rate": 4.5877823781506505e-05,
      "loss": 2.6575,
      "step": 2270
    },
    {
      "epoch": 0.24839645381378436,
      "grad_norm": 1.2917734384536743,
      "learning_rate": 4.585966441490521e-05,
      "loss": 2.5951,
      "step": 2280
    },
    {
      "epoch": 0.24948591194454658,
      "grad_norm": 1.259186029434204,
      "learning_rate": 4.5841505048303916e-05,
      "loss": 2.7784,
      "step": 2290
    },
    {
      "epoch": 0.2505753700753088,
      "grad_norm": 1.3379919528961182,
      "learning_rate": 4.582334568170262e-05,
      "loss": 2.6788,
      "step": 2300
    },
    {
      "epoch": 0.251664828206071,
      "grad_norm": 1.198850154876709,
      "learning_rate": 4.580518631510133e-05,
      "loss": 2.6094,
      "step": 2310
    },
    {
      "epoch": 0.2527542863368332,
      "grad_norm": 1.328439712524414,
      "learning_rate": 4.578702694850004e-05,
      "loss": 2.6238,
      "step": 2320
    },
    {
      "epoch": 0.25384374446759544,
      "grad_norm": 1.325881838798523,
      "learning_rate": 4.5768867581898744e-05,
      "loss": 2.7464,
      "step": 2330
    },
    {
      "epoch": 0.25493320259835767,
      "grad_norm": 1.1985350847244263,
      "learning_rate": 4.5750708215297456e-05,
      "loss": 2.6257,
      "step": 2340
    },
    {
      "epoch": 0.25602266072911983,
      "grad_norm": 1.281360387802124,
      "learning_rate": 4.573254884869616e-05,
      "loss": 2.7114,
      "step": 2350
    },
    {
      "epoch": 0.25711211885988206,
      "grad_norm": 1.2402056455612183,
      "learning_rate": 4.571438948209487e-05,
      "loss": 2.7127,
      "step": 2360
    },
    {
      "epoch": 0.2582015769906443,
      "grad_norm": 1.2405805587768555,
      "learning_rate": 4.569623011549357e-05,
      "loss": 2.6561,
      "step": 2370
    },
    {
      "epoch": 0.2592910351214065,
      "grad_norm": 1.2268275022506714,
      "learning_rate": 4.5678070748892285e-05,
      "loss": 2.6916,
      "step": 2380
    },
    {
      "epoch": 0.2603804932521687,
      "grad_norm": 1.2405816316604614,
      "learning_rate": 4.565991138229099e-05,
      "loss": 2.7225,
      "step": 2390
    },
    {
      "epoch": 0.2614699513829309,
      "grad_norm": 1.2935271263122559,
      "learning_rate": 4.5641752015689696e-05,
      "loss": 2.6467,
      "step": 2400
    },
    {
      "epoch": 0.2625594095136931,
      "grad_norm": 1.1825740337371826,
      "learning_rate": 4.56235926490884e-05,
      "loss": 2.6174,
      "step": 2410
    },
    {
      "epoch": 0.26364886764445533,
      "grad_norm": 1.2576593160629272,
      "learning_rate": 4.5605433282487106e-05,
      "loss": 2.5798,
      "step": 2420
    },
    {
      "epoch": 0.26473832577521755,
      "grad_norm": 1.2430899143218994,
      "learning_rate": 4.558727391588581e-05,
      "loss": 2.6384,
      "step": 2430
    },
    {
      "epoch": 0.2658277839059798,
      "grad_norm": 1.2916926145553589,
      "learning_rate": 4.5569114549284524e-05,
      "loss": 2.6703,
      "step": 2440
    },
    {
      "epoch": 0.266917242036742,
      "grad_norm": 1.2351479530334473,
      "learning_rate": 4.555095518268323e-05,
      "loss": 2.6419,
      "step": 2450
    },
    {
      "epoch": 0.26800670016750416,
      "grad_norm": 1.2537572383880615,
      "learning_rate": 4.5532795816081935e-05,
      "loss": 2.6857,
      "step": 2460
    },
    {
      "epoch": 0.2690961582982664,
      "grad_norm": 1.2177408933639526,
      "learning_rate": 4.551463644948065e-05,
      "loss": 2.634,
      "step": 2470
    },
    {
      "epoch": 0.2701856164290286,
      "grad_norm": 1.2077382802963257,
      "learning_rate": 4.549647708287935e-05,
      "loss": 2.6929,
      "step": 2480
    },
    {
      "epoch": 0.27127507455979083,
      "grad_norm": 1.2712072134017944,
      "learning_rate": 4.547831771627806e-05,
      "loss": 2.7758,
      "step": 2490
    },
    {
      "epoch": 0.27236453269055305,
      "grad_norm": 1.2725000381469727,
      "learning_rate": 4.546015834967677e-05,
      "loss": 2.6772,
      "step": 2500
    },
    {
      "epoch": 0.2734539908213153,
      "grad_norm": 1.1999081373214722,
      "learning_rate": 4.5441998983075476e-05,
      "loss": 2.7382,
      "step": 2510
    },
    {
      "epoch": 0.27454344895207744,
      "grad_norm": 1.289676308631897,
      "learning_rate": 4.542383961647418e-05,
      "loss": 2.6835,
      "step": 2520
    },
    {
      "epoch": 0.27563290708283966,
      "grad_norm": 1.2569538354873657,
      "learning_rate": 4.5405680249872886e-05,
      "loss": 2.5886,
      "step": 2530
    },
    {
      "epoch": 0.2767223652136019,
      "grad_norm": 1.2610656023025513,
      "learning_rate": 4.538752088327159e-05,
      "loss": 2.694,
      "step": 2540
    },
    {
      "epoch": 0.2778118233443641,
      "grad_norm": 1.2598177194595337,
      "learning_rate": 4.53693615166703e-05,
      "loss": 2.6433,
      "step": 2550
    },
    {
      "epoch": 0.27890128147512633,
      "grad_norm": 1.246591567993164,
      "learning_rate": 4.5351202150069e-05,
      "loss": 2.7172,
      "step": 2560
    },
    {
      "epoch": 0.2799907396058885,
      "grad_norm": 1.2149975299835205,
      "learning_rate": 4.5333042783467715e-05,
      "loss": 2.7489,
      "step": 2570
    },
    {
      "epoch": 0.2810801977366507,
      "grad_norm": 1.2646733522415161,
      "learning_rate": 4.531488341686642e-05,
      "loss": 2.6578,
      "step": 2580
    },
    {
      "epoch": 0.28216965586741294,
      "grad_norm": 1.211165189743042,
      "learning_rate": 4.5296724050265126e-05,
      "loss": 2.632,
      "step": 2590
    },
    {
      "epoch": 0.28325911399817516,
      "grad_norm": 1.3025778532028198,
      "learning_rate": 4.527856468366384e-05,
      "loss": 2.7013,
      "step": 2600
    },
    {
      "epoch": 0.2843485721289374,
      "grad_norm": 1.2693102359771729,
      "learning_rate": 4.526040531706254e-05,
      "loss": 2.7382,
      "step": 2610
    },
    {
      "epoch": 0.2854380302596996,
      "grad_norm": 1.2807949781417847,
      "learning_rate": 4.524224595046125e-05,
      "loss": 2.6464,
      "step": 2620
    },
    {
      "epoch": 0.28652748839046177,
      "grad_norm": 1.2342336177825928,
      "learning_rate": 4.522408658385996e-05,
      "loss": 2.6557,
      "step": 2630
    },
    {
      "epoch": 0.287616946521224,
      "grad_norm": 1.3110735416412354,
      "learning_rate": 4.5205927217258666e-05,
      "loss": 2.5738,
      "step": 2640
    },
    {
      "epoch": 0.2887064046519862,
      "grad_norm": 1.2163528203964233,
      "learning_rate": 4.518776785065737e-05,
      "loss": 2.6433,
      "step": 2650
    },
    {
      "epoch": 0.28979586278274844,
      "grad_norm": 1.243733286857605,
      "learning_rate": 4.5169608484056084e-05,
      "loss": 2.7797,
      "step": 2660
    },
    {
      "epoch": 0.29088532091351066,
      "grad_norm": 1.2771118879318237,
      "learning_rate": 4.515144911745479e-05,
      "loss": 2.7078,
      "step": 2670
    },
    {
      "epoch": 0.2919747790442729,
      "grad_norm": 1.1625722646713257,
      "learning_rate": 4.513328975085349e-05,
      "loss": 2.7179,
      "step": 2680
    },
    {
      "epoch": 0.29306423717503505,
      "grad_norm": 1.268930196762085,
      "learning_rate": 4.51151303842522e-05,
      "loss": 2.6577,
      "step": 2690
    },
    {
      "epoch": 0.29415369530579727,
      "grad_norm": 1.2212724685668945,
      "learning_rate": 4.5096971017650906e-05,
      "loss": 2.685,
      "step": 2700
    },
    {
      "epoch": 0.2952431534365595,
      "grad_norm": 1.2396812438964844,
      "learning_rate": 4.507881165104961e-05,
      "loss": 2.5998,
      "step": 2710
    },
    {
      "epoch": 0.2963326115673217,
      "grad_norm": 1.149867057800293,
      "learning_rate": 4.5060652284448316e-05,
      "loss": 2.6,
      "step": 2720
    },
    {
      "epoch": 0.29742206969808394,
      "grad_norm": 1.3050988912582397,
      "learning_rate": 4.504249291784703e-05,
      "loss": 2.6994,
      "step": 2730
    },
    {
      "epoch": 0.2985115278288461,
      "grad_norm": 1.2835978269577026,
      "learning_rate": 4.5024333551245734e-05,
      "loss": 2.6531,
      "step": 2740
    },
    {
      "epoch": 0.2996009859596083,
      "grad_norm": 1.275015115737915,
      "learning_rate": 4.500617418464444e-05,
      "loss": 2.6224,
      "step": 2750
    },
    {
      "epoch": 0.30069044409037055,
      "grad_norm": 1.2116402387619019,
      "learning_rate": 4.498801481804315e-05,
      "loss": 2.6301,
      "step": 2760
    },
    {
      "epoch": 0.30177990222113277,
      "grad_norm": 1.2178940773010254,
      "learning_rate": 4.496985545144186e-05,
      "loss": 2.6817,
      "step": 2770
    },
    {
      "epoch": 0.302869360351895,
      "grad_norm": 1.3577384948730469,
      "learning_rate": 4.495169608484056e-05,
      "loss": 2.7424,
      "step": 2780
    },
    {
      "epoch": 0.3039588184826572,
      "grad_norm": 1.1682336330413818,
      "learning_rate": 4.4933536718239275e-05,
      "loss": 2.6769,
      "step": 2790
    },
    {
      "epoch": 0.3050482766134194,
      "grad_norm": 1.188798189163208,
      "learning_rate": 4.491537735163798e-05,
      "loss": 2.6488,
      "step": 2800
    },
    {
      "epoch": 0.3061377347441816,
      "grad_norm": 1.35262131690979,
      "learning_rate": 4.4897217985036686e-05,
      "loss": 2.696,
      "step": 2810
    },
    {
      "epoch": 0.3072271928749438,
      "grad_norm": 1.2407499551773071,
      "learning_rate": 4.487905861843539e-05,
      "loss": 2.6775,
      "step": 2820
    },
    {
      "epoch": 0.30831665100570604,
      "grad_norm": 1.2541714906692505,
      "learning_rate": 4.4860899251834096e-05,
      "loss": 2.6589,
      "step": 2830
    },
    {
      "epoch": 0.30940610913646827,
      "grad_norm": 1.2910040616989136,
      "learning_rate": 4.48427398852328e-05,
      "loss": 2.6557,
      "step": 2840
    },
    {
      "epoch": 0.3104955672672305,
      "grad_norm": 1.2088545560836792,
      "learning_rate": 4.4824580518631514e-05,
      "loss": 2.6431,
      "step": 2850
    },
    {
      "epoch": 0.31158502539799265,
      "grad_norm": 1.2919079065322876,
      "learning_rate": 4.480642115203022e-05,
      "loss": 2.7609,
      "step": 2860
    },
    {
      "epoch": 0.3126744835287549,
      "grad_norm": 1.318915605545044,
      "learning_rate": 4.4788261785428925e-05,
      "loss": 2.6804,
      "step": 2870
    },
    {
      "epoch": 0.3137639416595171,
      "grad_norm": 1.3835039138793945,
      "learning_rate": 4.477010241882763e-05,
      "loss": 2.57,
      "step": 2880
    },
    {
      "epoch": 0.3148533997902793,
      "grad_norm": 1.2304590940475464,
      "learning_rate": 4.475194305222634e-05,
      "loss": 2.6628,
      "step": 2890
    },
    {
      "epoch": 0.31594285792104154,
      "grad_norm": 1.351001501083374,
      "learning_rate": 4.473378368562505e-05,
      "loss": 2.6519,
      "step": 2900
    },
    {
      "epoch": 0.3170323160518037,
      "grad_norm": 1.192814588546753,
      "learning_rate": 4.471562431902375e-05,
      "loss": 2.7014,
      "step": 2910
    },
    {
      "epoch": 0.31812177418256593,
      "grad_norm": 1.267122507095337,
      "learning_rate": 4.4697464952422465e-05,
      "loss": 2.7038,
      "step": 2920
    },
    {
      "epoch": 0.31921123231332815,
      "grad_norm": 1.2096608877182007,
      "learning_rate": 4.467930558582117e-05,
      "loss": 2.629,
      "step": 2930
    },
    {
      "epoch": 0.3203006904440904,
      "grad_norm": 1.278716802597046,
      "learning_rate": 4.4661146219219876e-05,
      "loss": 2.7139,
      "step": 2940
    },
    {
      "epoch": 0.3213901485748526,
      "grad_norm": 1.2036023139953613,
      "learning_rate": 4.464298685261858e-05,
      "loss": 2.7459,
      "step": 2950
    },
    {
      "epoch": 0.3224796067056148,
      "grad_norm": 1.2775304317474365,
      "learning_rate": 4.462482748601729e-05,
      "loss": 2.5947,
      "step": 2960
    },
    {
      "epoch": 0.323569064836377,
      "grad_norm": 1.297446846961975,
      "learning_rate": 4.460666811941599e-05,
      "loss": 2.6455,
      "step": 2970
    },
    {
      "epoch": 0.3246585229671392,
      "grad_norm": 1.3363306522369385,
      "learning_rate": 4.4588508752814705e-05,
      "loss": 2.6316,
      "step": 2980
    },
    {
      "epoch": 0.32574798109790143,
      "grad_norm": 1.2583024501800537,
      "learning_rate": 4.457034938621341e-05,
      "loss": 2.6392,
      "step": 2990
    },
    {
      "epoch": 0.32683743922866365,
      "grad_norm": 1.239001989364624,
      "learning_rate": 4.4552190019612116e-05,
      "loss": 2.6633,
      "step": 3000
    },
    {
      "epoch": 0.3279268973594259,
      "grad_norm": 1.2984602451324463,
      "learning_rate": 4.453403065301083e-05,
      "loss": 2.6347,
      "step": 3010
    },
    {
      "epoch": 0.3290163554901881,
      "grad_norm": 1.1846487522125244,
      "learning_rate": 4.451587128640953e-05,
      "loss": 2.616,
      "step": 3020
    },
    {
      "epoch": 0.33010581362095026,
      "grad_norm": 1.2087657451629639,
      "learning_rate": 4.449771191980824e-05,
      "loss": 2.737,
      "step": 3030
    },
    {
      "epoch": 0.3311952717517125,
      "grad_norm": 1.2434996366500854,
      "learning_rate": 4.447955255320695e-05,
      "loss": 2.6674,
      "step": 3040
    },
    {
      "epoch": 0.3322847298824747,
      "grad_norm": 1.1910481452941895,
      "learning_rate": 4.4461393186605656e-05,
      "loss": 2.6662,
      "step": 3050
    },
    {
      "epoch": 0.3333741880132369,
      "grad_norm": 1.3371504545211792,
      "learning_rate": 4.444323382000436e-05,
      "loss": 2.7227,
      "step": 3060
    },
    {
      "epoch": 0.33446364614399915,
      "grad_norm": 1.2255827188491821,
      "learning_rate": 4.442507445340307e-05,
      "loss": 2.6027,
      "step": 3070
    },
    {
      "epoch": 0.3355531042747613,
      "grad_norm": 1.3459200859069824,
      "learning_rate": 4.440691508680177e-05,
      "loss": 2.6395,
      "step": 3080
    },
    {
      "epoch": 0.33664256240552354,
      "grad_norm": 1.2889378070831299,
      "learning_rate": 4.438875572020048e-05,
      "loss": 2.662,
      "step": 3090
    },
    {
      "epoch": 0.33773202053628576,
      "grad_norm": 1.2654476165771484,
      "learning_rate": 4.437059635359918e-05,
      "loss": 2.6812,
      "step": 3100
    },
    {
      "epoch": 0.338821478667048,
      "grad_norm": 1.1908528804779053,
      "learning_rate": 4.4352436986997895e-05,
      "loss": 2.6659,
      "step": 3110
    },
    {
      "epoch": 0.3399109367978102,
      "grad_norm": 1.314536452293396,
      "learning_rate": 4.43342776203966e-05,
      "loss": 2.7307,
      "step": 3120
    },
    {
      "epoch": 0.3410003949285724,
      "grad_norm": 1.2795751094818115,
      "learning_rate": 4.4316118253795306e-05,
      "loss": 2.5412,
      "step": 3130
    },
    {
      "epoch": 0.3420898530593346,
      "grad_norm": 1.2811375856399536,
      "learning_rate": 4.429795888719402e-05,
      "loss": 2.5958,
      "step": 3140
    },
    {
      "epoch": 0.3431793111900968,
      "grad_norm": 1.2811884880065918,
      "learning_rate": 4.4279799520592724e-05,
      "loss": 2.6056,
      "step": 3150
    },
    {
      "epoch": 0.34426876932085904,
      "grad_norm": 1.3053174018859863,
      "learning_rate": 4.426164015399143e-05,
      "loss": 2.6734,
      "step": 3160
    },
    {
      "epoch": 0.34535822745162126,
      "grad_norm": 1.2430603504180908,
      "learning_rate": 4.424348078739014e-05,
      "loss": 2.6505,
      "step": 3170
    },
    {
      "epoch": 0.3464476855823835,
      "grad_norm": 1.2431902885437012,
      "learning_rate": 4.422532142078885e-05,
      "loss": 2.7081,
      "step": 3180
    },
    {
      "epoch": 0.34753714371314565,
      "grad_norm": 1.2452242374420166,
      "learning_rate": 4.420716205418755e-05,
      "loss": 2.5967,
      "step": 3190
    },
    {
      "epoch": 0.34862660184390787,
      "grad_norm": 1.3046250343322754,
      "learning_rate": 4.4189002687586265e-05,
      "loss": 2.6119,
      "step": 3200
    },
    {
      "epoch": 0.3497160599746701,
      "grad_norm": 1.3384052515029907,
      "learning_rate": 4.417084332098497e-05,
      "loss": 2.6655,
      "step": 3210
    },
    {
      "epoch": 0.3508055181054323,
      "grad_norm": 1.2669533491134644,
      "learning_rate": 4.4152683954383675e-05,
      "loss": 2.6566,
      "step": 3220
    },
    {
      "epoch": 0.35189497623619453,
      "grad_norm": 1.3275797367095947,
      "learning_rate": 4.413452458778238e-05,
      "loss": 2.6502,
      "step": 3230
    },
    {
      "epoch": 0.35298443436695676,
      "grad_norm": 1.189117670059204,
      "learning_rate": 4.4116365221181086e-05,
      "loss": 2.6498,
      "step": 3240
    },
    {
      "epoch": 0.3540738924977189,
      "grad_norm": 1.3009212017059326,
      "learning_rate": 4.409820585457979e-05,
      "loss": 2.6548,
      "step": 3250
    },
    {
      "epoch": 0.35516335062848114,
      "grad_norm": 1.3203328847885132,
      "learning_rate": 4.40800464879785e-05,
      "loss": 2.6579,
      "step": 3260
    },
    {
      "epoch": 0.35625280875924337,
      "grad_norm": 1.24478280544281,
      "learning_rate": 4.406188712137721e-05,
      "loss": 2.6525,
      "step": 3270
    },
    {
      "epoch": 0.3573422668900056,
      "grad_norm": 1.2664333581924438,
      "learning_rate": 4.4043727754775915e-05,
      "loss": 2.6441,
      "step": 3280
    },
    {
      "epoch": 0.3584317250207678,
      "grad_norm": 1.2582565546035767,
      "learning_rate": 4.402556838817462e-05,
      "loss": 2.6676,
      "step": 3290
    },
    {
      "epoch": 0.35952118315153003,
      "grad_norm": 1.2942782640457153,
      "learning_rate": 4.400740902157333e-05,
      "loss": 2.639,
      "step": 3300
    },
    {
      "epoch": 0.3606106412822922,
      "grad_norm": 1.3173909187316895,
      "learning_rate": 4.398924965497204e-05,
      "loss": 2.6247,
      "step": 3310
    },
    {
      "epoch": 0.3617000994130544,
      "grad_norm": 1.278244972229004,
      "learning_rate": 4.397109028837074e-05,
      "loss": 2.5534,
      "step": 3320
    },
    {
      "epoch": 0.36278955754381664,
      "grad_norm": 1.3098080158233643,
      "learning_rate": 4.3952930921769455e-05,
      "loss": 2.7597,
      "step": 3330
    },
    {
      "epoch": 0.36387901567457887,
      "grad_norm": 1.3229342699050903,
      "learning_rate": 4.393477155516816e-05,
      "loss": 2.5876,
      "step": 3340
    },
    {
      "epoch": 0.3649684738053411,
      "grad_norm": 1.2645937204360962,
      "learning_rate": 4.3916612188566866e-05,
      "loss": 2.5902,
      "step": 3350
    },
    {
      "epoch": 0.36605793193610325,
      "grad_norm": 1.2065207958221436,
      "learning_rate": 4.389845282196557e-05,
      "loss": 2.7101,
      "step": 3360
    },
    {
      "epoch": 0.3671473900668655,
      "grad_norm": 1.2668176889419556,
      "learning_rate": 4.388029345536428e-05,
      "loss": 2.5975,
      "step": 3370
    },
    {
      "epoch": 0.3682368481976277,
      "grad_norm": 1.285125970840454,
      "learning_rate": 4.386213408876298e-05,
      "loss": 2.645,
      "step": 3380
    },
    {
      "epoch": 0.3693263063283899,
      "grad_norm": 1.2130969762802124,
      "learning_rate": 4.384397472216169e-05,
      "loss": 2.6868,
      "step": 3390
    },
    {
      "epoch": 0.37041576445915214,
      "grad_norm": 1.2057654857635498,
      "learning_rate": 4.38258153555604e-05,
      "loss": 2.6602,
      "step": 3400
    },
    {
      "epoch": 0.37150522258991436,
      "grad_norm": 1.2154042720794678,
      "learning_rate": 4.3807655988959105e-05,
      "loss": 2.6256,
      "step": 3410
    },
    {
      "epoch": 0.37259468072067653,
      "grad_norm": 1.2572565078735352,
      "learning_rate": 4.378949662235781e-05,
      "loss": 2.6492,
      "step": 3420
    },
    {
      "epoch": 0.37368413885143875,
      "grad_norm": 1.262217402458191,
      "learning_rate": 4.377133725575652e-05,
      "loss": 2.7243,
      "step": 3430
    },
    {
      "epoch": 0.374773596982201,
      "grad_norm": 1.2814462184906006,
      "learning_rate": 4.375317788915523e-05,
      "loss": 2.6277,
      "step": 3440
    },
    {
      "epoch": 0.3758630551129632,
      "grad_norm": 1.1726469993591309,
      "learning_rate": 4.3735018522553934e-05,
      "loss": 2.6119,
      "step": 3450
    },
    {
      "epoch": 0.3769525132437254,
      "grad_norm": 1.276180386543274,
      "learning_rate": 4.3716859155952646e-05,
      "loss": 2.6235,
      "step": 3460
    },
    {
      "epoch": 0.37804197137448764,
      "grad_norm": 1.2971899509429932,
      "learning_rate": 4.369869978935135e-05,
      "loss": 2.6186,
      "step": 3470
    },
    {
      "epoch": 0.3791314295052498,
      "grad_norm": 1.289410948753357,
      "learning_rate": 4.368054042275006e-05,
      "loss": 2.632,
      "step": 3480
    },
    {
      "epoch": 0.38022088763601203,
      "grad_norm": 1.315480351448059,
      "learning_rate": 4.366238105614876e-05,
      "loss": 2.6657,
      "step": 3490
    },
    {
      "epoch": 0.38131034576677425,
      "grad_norm": 1.2516206502914429,
      "learning_rate": 4.364422168954747e-05,
      "loss": 2.6373,
      "step": 3500
    },
    {
      "epoch": 0.38239980389753647,
      "grad_norm": 1.2896277904510498,
      "learning_rate": 4.362606232294617e-05,
      "loss": 2.6496,
      "step": 3510
    },
    {
      "epoch": 0.3834892620282987,
      "grad_norm": 1.2188702821731567,
      "learning_rate": 4.3607902956344885e-05,
      "loss": 2.6277,
      "step": 3520
    },
    {
      "epoch": 0.38457872015906086,
      "grad_norm": 1.200334906578064,
      "learning_rate": 4.358974358974359e-05,
      "loss": 2.6928,
      "step": 3530
    },
    {
      "epoch": 0.3856681782898231,
      "grad_norm": 1.1749043464660645,
      "learning_rate": 4.3571584223142296e-05,
      "loss": 2.5536,
      "step": 3540
    },
    {
      "epoch": 0.3867576364205853,
      "grad_norm": 1.3454939126968384,
      "learning_rate": 4.355342485654101e-05,
      "loss": 2.6212,
      "step": 3550
    },
    {
      "epoch": 0.3878470945513475,
      "grad_norm": 1.2722573280334473,
      "learning_rate": 4.3535265489939714e-05,
      "loss": 2.7421,
      "step": 3560
    },
    {
      "epoch": 0.38893655268210975,
      "grad_norm": 1.3136518001556396,
      "learning_rate": 4.351710612333842e-05,
      "loss": 2.6519,
      "step": 3570
    },
    {
      "epoch": 0.39002601081287197,
      "grad_norm": 1.3090379238128662,
      "learning_rate": 4.3498946756737125e-05,
      "loss": 2.6705,
      "step": 3580
    },
    {
      "epoch": 0.39111546894363414,
      "grad_norm": 1.229537844657898,
      "learning_rate": 4.348078739013584e-05,
      "loss": 2.6169,
      "step": 3590
    },
    {
      "epoch": 0.39220492707439636,
      "grad_norm": 1.2603988647460938,
      "learning_rate": 4.346262802353454e-05,
      "loss": 2.6493,
      "step": 3600
    },
    {
      "epoch": 0.3932943852051586,
      "grad_norm": 1.32622492313385,
      "learning_rate": 4.344446865693325e-05,
      "loss": 2.5763,
      "step": 3610
    },
    {
      "epoch": 0.3943838433359208,
      "grad_norm": 1.2541896104812622,
      "learning_rate": 4.342630929033196e-05,
      "loss": 2.5903,
      "step": 3620
    },
    {
      "epoch": 0.395473301466683,
      "grad_norm": 1.3189536333084106,
      "learning_rate": 4.3408149923730665e-05,
      "loss": 2.6415,
      "step": 3630
    },
    {
      "epoch": 0.39656275959744525,
      "grad_norm": 1.1750606298446655,
      "learning_rate": 4.338999055712937e-05,
      "loss": 2.5864,
      "step": 3640
    },
    {
      "epoch": 0.3976522177282074,
      "grad_norm": 1.3746545314788818,
      "learning_rate": 4.3371831190528076e-05,
      "loss": 2.5807,
      "step": 3650
    },
    {
      "epoch": 0.39874167585896964,
      "grad_norm": 1.348978042602539,
      "learning_rate": 4.335367182392678e-05,
      "loss": 2.6645,
      "step": 3660
    },
    {
      "epoch": 0.39983113398973186,
      "grad_norm": 1.3035979270935059,
      "learning_rate": 4.333551245732549e-05,
      "loss": 2.5876,
      "step": 3670
    },
    {
      "epoch": 0.4009205921204941,
      "grad_norm": 1.2767510414123535,
      "learning_rate": 4.33173530907242e-05,
      "loss": 2.5934,
      "step": 3680
    },
    {
      "epoch": 0.4020100502512563,
      "grad_norm": 1.243432879447937,
      "learning_rate": 4.3299193724122905e-05,
      "loss": 2.6083,
      "step": 3690
    },
    {
      "epoch": 0.40309950838201847,
      "grad_norm": 1.2560254335403442,
      "learning_rate": 4.328103435752161e-05,
      "loss": 2.5677,
      "step": 3700
    },
    {
      "epoch": 0.4041889665127807,
      "grad_norm": 1.2671481370925903,
      "learning_rate": 4.326287499092032e-05,
      "loss": 2.6228,
      "step": 3710
    },
    {
      "epoch": 0.4052784246435429,
      "grad_norm": 1.2276414632797241,
      "learning_rate": 4.324471562431903e-05,
      "loss": 2.6556,
      "step": 3720
    },
    {
      "epoch": 0.40636788277430513,
      "grad_norm": 1.271328091621399,
      "learning_rate": 4.322655625771773e-05,
      "loss": 2.5939,
      "step": 3730
    },
    {
      "epoch": 0.40745734090506736,
      "grad_norm": 1.234409213066101,
      "learning_rate": 4.320839689111644e-05,
      "loss": 2.5124,
      "step": 3740
    },
    {
      "epoch": 0.4085467990358296,
      "grad_norm": 1.3208235502243042,
      "learning_rate": 4.319023752451515e-05,
      "loss": 2.6499,
      "step": 3750
    },
    {
      "epoch": 0.40963625716659174,
      "grad_norm": 1.290708065032959,
      "learning_rate": 4.3172078157913856e-05,
      "loss": 2.6443,
      "step": 3760
    },
    {
      "epoch": 0.41072571529735397,
      "grad_norm": 1.2548415660858154,
      "learning_rate": 4.315391879131256e-05,
      "loss": 2.5893,
      "step": 3770
    },
    {
      "epoch": 0.4118151734281162,
      "grad_norm": 1.3586724996566772,
      "learning_rate": 4.313575942471127e-05,
      "loss": 2.6993,
      "step": 3780
    },
    {
      "epoch": 0.4129046315588784,
      "grad_norm": 1.3324952125549316,
      "learning_rate": 4.311760005810997e-05,
      "loss": 2.7161,
      "step": 3790
    },
    {
      "epoch": 0.41399408968964063,
      "grad_norm": 1.2463157176971436,
      "learning_rate": 4.309944069150868e-05,
      "loss": 2.5496,
      "step": 3800
    },
    {
      "epoch": 0.41508354782040285,
      "grad_norm": 1.1807605028152466,
      "learning_rate": 4.308128132490739e-05,
      "loss": 2.635,
      "step": 3810
    },
    {
      "epoch": 0.416173005951165,
      "grad_norm": 1.2900227308273315,
      "learning_rate": 4.3063121958306095e-05,
      "loss": 2.6431,
      "step": 3820
    },
    {
      "epoch": 0.41726246408192724,
      "grad_norm": 1.2869648933410645,
      "learning_rate": 4.30449625917048e-05,
      "loss": 2.5921,
      "step": 3830
    },
    {
      "epoch": 0.41835192221268946,
      "grad_norm": 1.227354884147644,
      "learning_rate": 4.302680322510351e-05,
      "loss": 2.6521,
      "step": 3840
    },
    {
      "epoch": 0.4194413803434517,
      "grad_norm": 1.2218724489212036,
      "learning_rate": 4.300864385850222e-05,
      "loss": 2.6402,
      "step": 3850
    },
    {
      "epoch": 0.4205308384742139,
      "grad_norm": 1.243003487586975,
      "learning_rate": 4.2990484491900924e-05,
      "loss": 2.5621,
      "step": 3860
    },
    {
      "epoch": 0.4216202966049761,
      "grad_norm": 1.2609882354736328,
      "learning_rate": 4.2972325125299636e-05,
      "loss": 2.5822,
      "step": 3870
    },
    {
      "epoch": 0.4227097547357383,
      "grad_norm": 1.3067948818206787,
      "learning_rate": 4.295416575869834e-05,
      "loss": 2.6447,
      "step": 3880
    },
    {
      "epoch": 0.4237992128665005,
      "grad_norm": 1.2779086828231812,
      "learning_rate": 4.293600639209705e-05,
      "loss": 2.6291,
      "step": 3890
    },
    {
      "epoch": 0.42488867099726274,
      "grad_norm": 1.23899507522583,
      "learning_rate": 4.291784702549575e-05,
      "loss": 2.6595,
      "step": 3900
    },
    {
      "epoch": 0.42597812912802496,
      "grad_norm": 1.1978321075439453,
      "learning_rate": 4.289968765889446e-05,
      "loss": 2.6521,
      "step": 3910
    },
    {
      "epoch": 0.4270675872587872,
      "grad_norm": 1.280922532081604,
      "learning_rate": 4.288152829229316e-05,
      "loss": 2.6886,
      "step": 3920
    },
    {
      "epoch": 0.42815704538954935,
      "grad_norm": 1.2661899328231812,
      "learning_rate": 4.286336892569187e-05,
      "loss": 2.7042,
      "step": 3930
    },
    {
      "epoch": 0.4292465035203116,
      "grad_norm": 1.2384647130966187,
      "learning_rate": 4.284520955909058e-05,
      "loss": 2.6005,
      "step": 3940
    },
    {
      "epoch": 0.4303359616510738,
      "grad_norm": 1.3794759511947632,
      "learning_rate": 4.2827050192489286e-05,
      "loss": 2.7478,
      "step": 3950
    },
    {
      "epoch": 0.431425419781836,
      "grad_norm": 1.2239947319030762,
      "learning_rate": 4.280889082588799e-05,
      "loss": 2.6696,
      "step": 3960
    },
    {
      "epoch": 0.43251487791259824,
      "grad_norm": 1.2943202257156372,
      "learning_rate": 4.2790731459286704e-05,
      "loss": 2.7064,
      "step": 3970
    },
    {
      "epoch": 0.43360433604336046,
      "grad_norm": 1.2938838005065918,
      "learning_rate": 4.277257209268541e-05,
      "loss": 2.7149,
      "step": 3980
    },
    {
      "epoch": 0.4346937941741226,
      "grad_norm": 1.2334879636764526,
      "learning_rate": 4.2754412726084115e-05,
      "loss": 2.4825,
      "step": 3990
    },
    {
      "epoch": 0.43578325230488485,
      "grad_norm": 1.323229193687439,
      "learning_rate": 4.273625335948283e-05,
      "loss": 2.636,
      "step": 4000
    },
    {
      "epoch": 0.43687271043564707,
      "grad_norm": 1.2550708055496216,
      "learning_rate": 4.271809399288153e-05,
      "loss": 2.6124,
      "step": 4010
    },
    {
      "epoch": 0.4379621685664093,
      "grad_norm": 1.2344145774841309,
      "learning_rate": 4.269993462628024e-05,
      "loss": 2.6831,
      "step": 4020
    },
    {
      "epoch": 0.4390516266971715,
      "grad_norm": 1.214687466621399,
      "learning_rate": 4.268177525967895e-05,
      "loss": 2.6097,
      "step": 4030
    },
    {
      "epoch": 0.4401410848279337,
      "grad_norm": 1.16901695728302,
      "learning_rate": 4.2663615893077655e-05,
      "loss": 2.6226,
      "step": 4040
    },
    {
      "epoch": 0.4412305429586959,
      "grad_norm": 1.2217062711715698,
      "learning_rate": 4.264545652647636e-05,
      "loss": 2.5842,
      "step": 4050
    },
    {
      "epoch": 0.4423200010894581,
      "grad_norm": 1.2979375123977661,
      "learning_rate": 4.2627297159875066e-05,
      "loss": 2.6261,
      "step": 4060
    },
    {
      "epoch": 0.44340945922022035,
      "grad_norm": 1.2019087076187134,
      "learning_rate": 4.260913779327377e-05,
      "loss": 2.5428,
      "step": 4070
    },
    {
      "epoch": 0.44449891735098257,
      "grad_norm": 1.1841505765914917,
      "learning_rate": 4.259097842667248e-05,
      "loss": 2.6008,
      "step": 4080
    },
    {
      "epoch": 0.4455883754817448,
      "grad_norm": 1.2923200130462646,
      "learning_rate": 4.257281906007118e-05,
      "loss": 2.6271,
      "step": 4090
    },
    {
      "epoch": 0.44667783361250696,
      "grad_norm": 1.2588744163513184,
      "learning_rate": 4.2554659693469894e-05,
      "loss": 2.5634,
      "step": 4100
    },
    {
      "epoch": 0.4477672917432692,
      "grad_norm": 1.309146523475647,
      "learning_rate": 4.25365003268686e-05,
      "loss": 2.7327,
      "step": 4110
    },
    {
      "epoch": 0.4488567498740314,
      "grad_norm": 1.2642889022827148,
      "learning_rate": 4.2518340960267305e-05,
      "loss": 2.5523,
      "step": 4120
    },
    {
      "epoch": 0.4499462080047936,
      "grad_norm": 1.3353468179702759,
      "learning_rate": 4.250018159366602e-05,
      "loss": 2.5513,
      "step": 4130
    },
    {
      "epoch": 0.45103566613555585,
      "grad_norm": 1.2201601266860962,
      "learning_rate": 4.248202222706472e-05,
      "loss": 2.7201,
      "step": 4140
    },
    {
      "epoch": 0.452125124266318,
      "grad_norm": 1.2860852479934692,
      "learning_rate": 4.246386286046343e-05,
      "loss": 2.723,
      "step": 4150
    },
    {
      "epoch": 0.45321458239708023,
      "grad_norm": 1.2375922203063965,
      "learning_rate": 4.244570349386214e-05,
      "loss": 2.5817,
      "step": 4160
    },
    {
      "epoch": 0.45430404052784246,
      "grad_norm": 1.2648680210113525,
      "learning_rate": 4.2427544127260846e-05,
      "loss": 2.621,
      "step": 4170
    },
    {
      "epoch": 0.4553934986586047,
      "grad_norm": 1.3247697353363037,
      "learning_rate": 4.240938476065955e-05,
      "loss": 2.6418,
      "step": 4180
    },
    {
      "epoch": 0.4564829567893669,
      "grad_norm": 1.202848196029663,
      "learning_rate": 4.239122539405826e-05,
      "loss": 2.5838,
      "step": 4190
    },
    {
      "epoch": 0.4575724149201291,
      "grad_norm": 1.3427059650421143,
      "learning_rate": 4.237306602745696e-05,
      "loss": 2.5637,
      "step": 4200
    },
    {
      "epoch": 0.4586618730508913,
      "grad_norm": 1.3233145475387573,
      "learning_rate": 4.235490666085567e-05,
      "loss": 2.6275,
      "step": 4210
    },
    {
      "epoch": 0.4597513311816535,
      "grad_norm": 1.2730072736740112,
      "learning_rate": 4.233674729425438e-05,
      "loss": 2.6114,
      "step": 4220
    },
    {
      "epoch": 0.46084078931241573,
      "grad_norm": 1.3453598022460938,
      "learning_rate": 4.2318587927653085e-05,
      "loss": 2.6293,
      "step": 4230
    },
    {
      "epoch": 0.46193024744317795,
      "grad_norm": 1.2471169233322144,
      "learning_rate": 4.230042856105179e-05,
      "loss": 2.6829,
      "step": 4240
    },
    {
      "epoch": 0.4630197055739402,
      "grad_norm": 1.2759201526641846,
      "learning_rate": 4.22822691944505e-05,
      "loss": 2.6679,
      "step": 4250
    },
    {
      "epoch": 0.4641091637047024,
      "grad_norm": 1.2401014566421509,
      "learning_rate": 4.226410982784921e-05,
      "loss": 2.5903,
      "step": 4260
    },
    {
      "epoch": 0.46519862183546457,
      "grad_norm": 1.2905519008636475,
      "learning_rate": 4.2245950461247914e-05,
      "loss": 2.701,
      "step": 4270
    },
    {
      "epoch": 0.4662880799662268,
      "grad_norm": 1.2246055603027344,
      "learning_rate": 4.222779109464662e-05,
      "loss": 2.6002,
      "step": 4280
    },
    {
      "epoch": 0.467377538096989,
      "grad_norm": 1.181372880935669,
      "learning_rate": 4.220963172804533e-05,
      "loss": 2.6325,
      "step": 4290
    },
    {
      "epoch": 0.46846699622775123,
      "grad_norm": 1.2834140062332153,
      "learning_rate": 4.219147236144404e-05,
      "loss": 2.6783,
      "step": 4300
    },
    {
      "epoch": 0.46955645435851345,
      "grad_norm": 1.2552765607833862,
      "learning_rate": 4.217331299484274e-05,
      "loss": 2.6427,
      "step": 4310
    },
    {
      "epoch": 0.4706459124892756,
      "grad_norm": 1.3140946626663208,
      "learning_rate": 4.215515362824145e-05,
      "loss": 2.604,
      "step": 4320
    },
    {
      "epoch": 0.47173537062003784,
      "grad_norm": 1.2690593004226685,
      "learning_rate": 4.213699426164015e-05,
      "loss": 2.5478,
      "step": 4330
    },
    {
      "epoch": 0.47282482875080006,
      "grad_norm": 1.317161202430725,
      "learning_rate": 4.211883489503886e-05,
      "loss": 2.6325,
      "step": 4340
    },
    {
      "epoch": 0.4739142868815623,
      "grad_norm": 1.2936820983886719,
      "learning_rate": 4.210067552843757e-05,
      "loss": 2.4911,
      "step": 4350
    },
    {
      "epoch": 0.4750037450123245,
      "grad_norm": 1.2554347515106201,
      "learning_rate": 4.2082516161836276e-05,
      "loss": 2.5957,
      "step": 4360
    },
    {
      "epoch": 0.47609320314308673,
      "grad_norm": 1.2548879384994507,
      "learning_rate": 4.206435679523498e-05,
      "loss": 2.5861,
      "step": 4370
    },
    {
      "epoch": 0.4771826612738489,
      "grad_norm": 1.29630446434021,
      "learning_rate": 4.2046197428633694e-05,
      "loss": 2.6035,
      "step": 4380
    },
    {
      "epoch": 0.4782721194046111,
      "grad_norm": 1.2597066164016724,
      "learning_rate": 4.20280380620324e-05,
      "loss": 2.6699,
      "step": 4390
    },
    {
      "epoch": 0.47936157753537334,
      "grad_norm": 1.286238431930542,
      "learning_rate": 4.2009878695431104e-05,
      "loss": 2.5704,
      "step": 4400
    },
    {
      "epoch": 0.48045103566613556,
      "grad_norm": 1.247723937034607,
      "learning_rate": 4.199171932882982e-05,
      "loss": 2.5402,
      "step": 4410
    },
    {
      "epoch": 0.4815404937968978,
      "grad_norm": 1.2932450771331787,
      "learning_rate": 4.197355996222852e-05,
      "loss": 2.5152,
      "step": 4420
    },
    {
      "epoch": 0.48262995192766,
      "grad_norm": 1.2327542304992676,
      "learning_rate": 4.195540059562723e-05,
      "loss": 2.6423,
      "step": 4430
    },
    {
      "epoch": 0.4837194100584222,
      "grad_norm": 1.3124005794525146,
      "learning_rate": 4.193724122902593e-05,
      "loss": 2.6642,
      "step": 4440
    },
    {
      "epoch": 0.4848088681891844,
      "grad_norm": 1.2483054399490356,
      "learning_rate": 4.1919081862424645e-05,
      "loss": 2.5758,
      "step": 4450
    },
    {
      "epoch": 0.4858983263199466,
      "grad_norm": 1.370447039604187,
      "learning_rate": 4.1900922495823344e-05,
      "loss": 2.5792,
      "step": 4460
    },
    {
      "epoch": 0.48698778445070884,
      "grad_norm": 1.2827264070510864,
      "learning_rate": 4.188276312922205e-05,
      "loss": 2.6063,
      "step": 4470
    },
    {
      "epoch": 0.48807724258147106,
      "grad_norm": 1.2538527250289917,
      "learning_rate": 4.186460376262076e-05,
      "loss": 2.5796,
      "step": 4480
    },
    {
      "epoch": 0.4891667007122332,
      "grad_norm": 1.3754785060882568,
      "learning_rate": 4.184644439601947e-05,
      "loss": 2.5519,
      "step": 4490
    },
    {
      "epoch": 0.49025615884299545,
      "grad_norm": 1.2704946994781494,
      "learning_rate": 4.182828502941817e-05,
      "loss": 2.5966,
      "step": 4500
    },
    {
      "epoch": 0.49134561697375767,
      "grad_norm": 1.2922923564910889,
      "learning_rate": 4.1810125662816884e-05,
      "loss": 2.6269,
      "step": 4510
    },
    {
      "epoch": 0.4924350751045199,
      "grad_norm": 1.306902289390564,
      "learning_rate": 4.179196629621559e-05,
      "loss": 2.6966,
      "step": 4520
    },
    {
      "epoch": 0.4935245332352821,
      "grad_norm": 1.2908183336257935,
      "learning_rate": 4.1773806929614295e-05,
      "loss": 2.5955,
      "step": 4530
    },
    {
      "epoch": 0.49461399136604434,
      "grad_norm": 1.2925876379013062,
      "learning_rate": 4.175564756301301e-05,
      "loss": 2.6873,
      "step": 4540
    },
    {
      "epoch": 0.4957034494968065,
      "grad_norm": 1.2033933401107788,
      "learning_rate": 4.173748819641171e-05,
      "loss": 2.5412,
      "step": 4550
    },
    {
      "epoch": 0.4967929076275687,
      "grad_norm": 1.207045555114746,
      "learning_rate": 4.171932882981042e-05,
      "loss": 2.5253,
      "step": 4560
    },
    {
      "epoch": 0.49788236575833095,
      "grad_norm": 1.2613613605499268,
      "learning_rate": 4.170116946320913e-05,
      "loss": 2.6342,
      "step": 4570
    },
    {
      "epoch": 0.49897182388909317,
      "grad_norm": 1.3016061782836914,
      "learning_rate": 4.1683010096607836e-05,
      "loss": 2.5501,
      "step": 4580
    },
    {
      "epoch": 0.5000612820198553,
      "grad_norm": 1.2899538278579712,
      "learning_rate": 4.166485073000654e-05,
      "loss": 2.646,
      "step": 4590
    },
    {
      "epoch": 0.5011507401506176,
      "grad_norm": 1.305942416191101,
      "learning_rate": 4.164669136340525e-05,
      "loss": 2.6337,
      "step": 4600
    },
    {
      "epoch": 0.5022401982813798,
      "grad_norm": 1.329973816871643,
      "learning_rate": 4.162853199680395e-05,
      "loss": 2.6312,
      "step": 4610
    },
    {
      "epoch": 0.503329656412142,
      "grad_norm": 1.2848920822143555,
      "learning_rate": 4.161037263020266e-05,
      "loss": 2.7405,
      "step": 4620
    },
    {
      "epoch": 0.5044191145429042,
      "grad_norm": 1.2740777730941772,
      "learning_rate": 4.159221326360136e-05,
      "loss": 2.6253,
      "step": 4630
    },
    {
      "epoch": 0.5055085726736664,
      "grad_norm": 1.2832472324371338,
      "learning_rate": 4.1574053897000075e-05,
      "loss": 2.6282,
      "step": 4640
    },
    {
      "epoch": 0.5065980308044287,
      "grad_norm": 1.2856721878051758,
      "learning_rate": 4.155589453039878e-05,
      "loss": 2.619,
      "step": 4650
    },
    {
      "epoch": 0.5076874889351909,
      "grad_norm": 1.2896037101745605,
      "learning_rate": 4.1537735163797486e-05,
      "loss": 2.6002,
      "step": 4660
    },
    {
      "epoch": 0.5087769470659531,
      "grad_norm": 1.2008044719696045,
      "learning_rate": 4.15195757971962e-05,
      "loss": 2.6683,
      "step": 4670
    },
    {
      "epoch": 0.5098664051967153,
      "grad_norm": 1.3200048208236694,
      "learning_rate": 4.1501416430594904e-05,
      "loss": 2.5528,
      "step": 4680
    },
    {
      "epoch": 0.5109558633274774,
      "grad_norm": 1.3334505558013916,
      "learning_rate": 4.148325706399361e-05,
      "loss": 2.6539,
      "step": 4690
    },
    {
      "epoch": 0.5120453214582397,
      "grad_norm": 1.2894065380096436,
      "learning_rate": 4.146509769739232e-05,
      "loss": 2.6519,
      "step": 4700
    },
    {
      "epoch": 0.5131347795890019,
      "grad_norm": 1.2419893741607666,
      "learning_rate": 4.1446938330791027e-05,
      "loss": 2.558,
      "step": 4710
    },
    {
      "epoch": 0.5142242377197641,
      "grad_norm": 1.3489948511123657,
      "learning_rate": 4.142877896418973e-05,
      "loss": 2.7038,
      "step": 4720
    },
    {
      "epoch": 0.5153136958505263,
      "grad_norm": 1.2663897275924683,
      "learning_rate": 4.141061959758844e-05,
      "loss": 2.6377,
      "step": 4730
    },
    {
      "epoch": 0.5164031539812886,
      "grad_norm": 1.2034038305282593,
      "learning_rate": 4.139246023098714e-05,
      "loss": 2.6653,
      "step": 4740
    },
    {
      "epoch": 0.5174926121120508,
      "grad_norm": 1.28199303150177,
      "learning_rate": 4.137430086438585e-05,
      "loss": 2.5769,
      "step": 4750
    },
    {
      "epoch": 0.518582070242813,
      "grad_norm": 1.4629054069519043,
      "learning_rate": 4.135614149778456e-05,
      "loss": 2.6448,
      "step": 4760
    },
    {
      "epoch": 0.5196715283735752,
      "grad_norm": 1.2470110654830933,
      "learning_rate": 4.1337982131183266e-05,
      "loss": 2.6039,
      "step": 4770
    },
    {
      "epoch": 0.5207609865043374,
      "grad_norm": 1.2791825532913208,
      "learning_rate": 4.131982276458197e-05,
      "loss": 2.673,
      "step": 4780
    },
    {
      "epoch": 0.5218504446350997,
      "grad_norm": 1.2615171670913696,
      "learning_rate": 4.130166339798068e-05,
      "loss": 2.5725,
      "step": 4790
    },
    {
      "epoch": 0.5229399027658618,
      "grad_norm": 1.2175527811050415,
      "learning_rate": 4.128350403137939e-05,
      "loss": 2.5331,
      "step": 4800
    },
    {
      "epoch": 0.524029360896624,
      "grad_norm": 1.2578660249710083,
      "learning_rate": 4.1265344664778094e-05,
      "loss": 2.6181,
      "step": 4810
    },
    {
      "epoch": 0.5251188190273862,
      "grad_norm": 1.2767161130905151,
      "learning_rate": 4.12471852981768e-05,
      "loss": 2.6103,
      "step": 4820
    },
    {
      "epoch": 0.5262082771581484,
      "grad_norm": 1.1710063219070435,
      "learning_rate": 4.122902593157551e-05,
      "loss": 2.6152,
      "step": 4830
    },
    {
      "epoch": 0.5272977352889107,
      "grad_norm": 1.206032395362854,
      "learning_rate": 4.121086656497422e-05,
      "loss": 2.5868,
      "step": 4840
    },
    {
      "epoch": 0.5283871934196729,
      "grad_norm": 1.2880313396453857,
      "learning_rate": 4.119270719837292e-05,
      "loss": 2.5561,
      "step": 4850
    },
    {
      "epoch": 0.5294766515504351,
      "grad_norm": 1.3935821056365967,
      "learning_rate": 4.117454783177163e-05,
      "loss": 2.5957,
      "step": 4860
    },
    {
      "epoch": 0.5305661096811973,
      "grad_norm": 1.2115815877914429,
      "learning_rate": 4.1156388465170334e-05,
      "loss": 2.6786,
      "step": 4870
    },
    {
      "epoch": 0.5316555678119596,
      "grad_norm": 1.3294283151626587,
      "learning_rate": 4.113822909856904e-05,
      "loss": 2.5575,
      "step": 4880
    },
    {
      "epoch": 0.5327450259427218,
      "grad_norm": 1.263001799583435,
      "learning_rate": 4.112006973196775e-05,
      "loss": 2.6179,
      "step": 4890
    },
    {
      "epoch": 0.533834484073484,
      "grad_norm": 1.302059292793274,
      "learning_rate": 4.110191036536646e-05,
      "loss": 2.5955,
      "step": 4900
    },
    {
      "epoch": 0.5349239422042462,
      "grad_norm": 1.3155641555786133,
      "learning_rate": 4.108375099876516e-05,
      "loss": 2.6118,
      "step": 4910
    },
    {
      "epoch": 0.5360134003350083,
      "grad_norm": 1.2628419399261475,
      "learning_rate": 4.1065591632163874e-05,
      "loss": 2.7046,
      "step": 4920
    },
    {
      "epoch": 0.5371028584657705,
      "grad_norm": 1.245430588722229,
      "learning_rate": 4.104743226556258e-05,
      "loss": 2.6035,
      "step": 4930
    },
    {
      "epoch": 0.5381923165965328,
      "grad_norm": 1.285886287689209,
      "learning_rate": 4.1029272898961285e-05,
      "loss": 2.6035,
      "step": 4940
    },
    {
      "epoch": 0.539281774727295,
      "grad_norm": 1.2869471311569214,
      "learning_rate": 4.101111353236e-05,
      "loss": 2.5538,
      "step": 4950
    },
    {
      "epoch": 0.5403712328580572,
      "grad_norm": 1.2891591787338257,
      "learning_rate": 4.09929541657587e-05,
      "loss": 2.7064,
      "step": 4960
    },
    {
      "epoch": 0.5414606909888194,
      "grad_norm": 1.3039870262145996,
      "learning_rate": 4.097479479915741e-05,
      "loss": 2.5875,
      "step": 4970
    },
    {
      "epoch": 0.5425501491195817,
      "grad_norm": 1.2688614130020142,
      "learning_rate": 4.0956635432556114e-05,
      "loss": 2.6591,
      "step": 4980
    },
    {
      "epoch": 0.5436396072503439,
      "grad_norm": 1.2786866426467896,
      "learning_rate": 4.0938476065954826e-05,
      "loss": 2.5615,
      "step": 4990
    },
    {
      "epoch": 0.5447290653811061,
      "grad_norm": 1.3024214506149292,
      "learning_rate": 4.092031669935353e-05,
      "loss": 2.5778,
      "step": 5000
    },
    {
      "epoch": 0.5458185235118683,
      "grad_norm": 1.200474739074707,
      "learning_rate": 4.0902157332752237e-05,
      "loss": 2.5886,
      "step": 5010
    },
    {
      "epoch": 0.5469079816426305,
      "grad_norm": 1.2069652080535889,
      "learning_rate": 4.088399796615094e-05,
      "loss": 2.5679,
      "step": 5020
    },
    {
      "epoch": 0.5479974397733927,
      "grad_norm": 1.2976016998291016,
      "learning_rate": 4.086583859954965e-05,
      "loss": 2.6944,
      "step": 5030
    },
    {
      "epoch": 0.5490868979041549,
      "grad_norm": 1.2008711099624634,
      "learning_rate": 4.084767923294835e-05,
      "loss": 2.5727,
      "step": 5040
    },
    {
      "epoch": 0.5501763560349171,
      "grad_norm": 1.2767281532287598,
      "learning_rate": 4.0829519866347065e-05,
      "loss": 2.578,
      "step": 5050
    },
    {
      "epoch": 0.5512658141656793,
      "grad_norm": 1.4101579189300537,
      "learning_rate": 4.081136049974577e-05,
      "loss": 2.6497,
      "step": 5060
    },
    {
      "epoch": 0.5523552722964415,
      "grad_norm": 1.321453332901001,
      "learning_rate": 4.0793201133144476e-05,
      "loss": 2.651,
      "step": 5070
    },
    {
      "epoch": 0.5534447304272038,
      "grad_norm": 1.2649420499801636,
      "learning_rate": 4.077504176654319e-05,
      "loss": 2.5122,
      "step": 5080
    },
    {
      "epoch": 0.554534188557966,
      "grad_norm": 1.283767580986023,
      "learning_rate": 4.0756882399941893e-05,
      "loss": 2.6394,
      "step": 5090
    },
    {
      "epoch": 0.5556236466887282,
      "grad_norm": 1.3783870935440063,
      "learning_rate": 4.07387230333406e-05,
      "loss": 2.6652,
      "step": 5100
    },
    {
      "epoch": 0.5567131048194904,
      "grad_norm": 1.3746758699417114,
      "learning_rate": 4.072056366673931e-05,
      "loss": 2.5708,
      "step": 5110
    },
    {
      "epoch": 0.5578025629502527,
      "grad_norm": 1.2788081169128418,
      "learning_rate": 4.0702404300138016e-05,
      "loss": 2.5946,
      "step": 5120
    },
    {
      "epoch": 0.5588920210810149,
      "grad_norm": 1.2366143465042114,
      "learning_rate": 4.068424493353672e-05,
      "loss": 2.5359,
      "step": 5130
    },
    {
      "epoch": 0.559981479211777,
      "grad_norm": 1.3485671281814575,
      "learning_rate": 4.066608556693543e-05,
      "loss": 2.6164,
      "step": 5140
    },
    {
      "epoch": 0.5610709373425392,
      "grad_norm": 1.1768049001693726,
      "learning_rate": 4.064792620033413e-05,
      "loss": 2.5356,
      "step": 5150
    },
    {
      "epoch": 0.5621603954733014,
      "grad_norm": 1.3041949272155762,
      "learning_rate": 4.062976683373284e-05,
      "loss": 2.6195,
      "step": 5160
    },
    {
      "epoch": 0.5632498536040637,
      "grad_norm": 1.2386595010757446,
      "learning_rate": 4.0611607467131544e-05,
      "loss": 2.6232,
      "step": 5170
    },
    {
      "epoch": 0.5643393117348259,
      "grad_norm": 1.2416555881500244,
      "learning_rate": 4.0593448100530256e-05,
      "loss": 2.5573,
      "step": 5180
    },
    {
      "epoch": 0.5654287698655881,
      "grad_norm": 1.257159948348999,
      "learning_rate": 4.057528873392896e-05,
      "loss": 2.7289,
      "step": 5190
    },
    {
      "epoch": 0.5665182279963503,
      "grad_norm": 1.3356308937072754,
      "learning_rate": 4.0557129367327667e-05,
      "loss": 2.5989,
      "step": 5200
    },
    {
      "epoch": 0.5676076861271125,
      "grad_norm": 1.250443458557129,
      "learning_rate": 4.053897000072638e-05,
      "loss": 2.4755,
      "step": 5210
    },
    {
      "epoch": 0.5686971442578748,
      "grad_norm": 1.261641025543213,
      "learning_rate": 4.0520810634125084e-05,
      "loss": 2.5942,
      "step": 5220
    },
    {
      "epoch": 0.569786602388637,
      "grad_norm": 1.2253021001815796,
      "learning_rate": 4.050265126752379e-05,
      "loss": 2.6075,
      "step": 5230
    },
    {
      "epoch": 0.5708760605193992,
      "grad_norm": 1.2821999788284302,
      "learning_rate": 4.04844919009225e-05,
      "loss": 2.6058,
      "step": 5240
    },
    {
      "epoch": 0.5719655186501614,
      "grad_norm": 1.4169394969940186,
      "learning_rate": 4.046633253432121e-05,
      "loss": 2.6229,
      "step": 5250
    },
    {
      "epoch": 0.5730549767809235,
      "grad_norm": 1.40939462184906,
      "learning_rate": 4.044817316771991e-05,
      "loss": 2.5237,
      "step": 5260
    },
    {
      "epoch": 0.5741444349116858,
      "grad_norm": 1.3785227537155151,
      "learning_rate": 4.043001380111862e-05,
      "loss": 2.5139,
      "step": 5270
    },
    {
      "epoch": 0.575233893042448,
      "grad_norm": 1.281284213066101,
      "learning_rate": 4.0411854434517323e-05,
      "loss": 2.6899,
      "step": 5280
    },
    {
      "epoch": 0.5763233511732102,
      "grad_norm": 1.289809226989746,
      "learning_rate": 4.039369506791603e-05,
      "loss": 2.6225,
      "step": 5290
    },
    {
      "epoch": 0.5774128093039724,
      "grad_norm": 1.3164477348327637,
      "learning_rate": 4.037553570131474e-05,
      "loss": 2.5126,
      "step": 5300
    },
    {
      "epoch": 0.5785022674347347,
      "grad_norm": 1.3175203800201416,
      "learning_rate": 4.0357376334713447e-05,
      "loss": 2.6207,
      "step": 5310
    },
    {
      "epoch": 0.5795917255654969,
      "grad_norm": 1.2464473247528076,
      "learning_rate": 4.033921696811215e-05,
      "loss": 2.6451,
      "step": 5320
    },
    {
      "epoch": 0.5806811836962591,
      "grad_norm": 1.2349750995635986,
      "learning_rate": 4.032105760151086e-05,
      "loss": 2.5893,
      "step": 5330
    },
    {
      "epoch": 0.5817706418270213,
      "grad_norm": 1.266752004623413,
      "learning_rate": 4.030289823490957e-05,
      "loss": 2.5382,
      "step": 5340
    },
    {
      "epoch": 0.5828600999577835,
      "grad_norm": 1.3048838376998901,
      "learning_rate": 4.0284738868308275e-05,
      "loss": 2.5374,
      "step": 5350
    },
    {
      "epoch": 0.5839495580885458,
      "grad_norm": 1.242826223373413,
      "learning_rate": 4.026657950170698e-05,
      "loss": 2.5433,
      "step": 5360
    },
    {
      "epoch": 0.5850390162193079,
      "grad_norm": 1.3043698072433472,
      "learning_rate": 4.024842013510569e-05,
      "loss": 2.5703,
      "step": 5370
    },
    {
      "epoch": 0.5861284743500701,
      "grad_norm": 1.3134386539459229,
      "learning_rate": 4.02302607685044e-05,
      "loss": 2.5275,
      "step": 5380
    },
    {
      "epoch": 0.5872179324808323,
      "grad_norm": 1.3657900094985962,
      "learning_rate": 4.0212101401903103e-05,
      "loss": 2.5685,
      "step": 5390
    },
    {
      "epoch": 0.5883073906115945,
      "grad_norm": 1.3971227407455444,
      "learning_rate": 4.0193942035301816e-05,
      "loss": 2.7144,
      "step": 5400
    },
    {
      "epoch": 0.5893968487423568,
      "grad_norm": 1.2933275699615479,
      "learning_rate": 4.017578266870052e-05,
      "loss": 2.5987,
      "step": 5410
    },
    {
      "epoch": 0.590486306873119,
      "grad_norm": 1.2714979648590088,
      "learning_rate": 4.0157623302099226e-05,
      "loss": 2.5731,
      "step": 5420
    },
    {
      "epoch": 0.5915757650038812,
      "grad_norm": 1.3396692276000977,
      "learning_rate": 4.013946393549793e-05,
      "loss": 2.5745,
      "step": 5430
    },
    {
      "epoch": 0.5926652231346434,
      "grad_norm": 1.320637583732605,
      "learning_rate": 4.012130456889664e-05,
      "loss": 2.5391,
      "step": 5440
    },
    {
      "epoch": 0.5937546812654056,
      "grad_norm": 1.2513421773910522,
      "learning_rate": 4.010314520229534e-05,
      "loss": 2.5397,
      "step": 5450
    },
    {
      "epoch": 0.5948441393961679,
      "grad_norm": 1.3188345432281494,
      "learning_rate": 4.0084985835694055e-05,
      "loss": 2.5411,
      "step": 5460
    },
    {
      "epoch": 0.5959335975269301,
      "grad_norm": 1.311720848083496,
      "learning_rate": 4.006682646909276e-05,
      "loss": 2.5992,
      "step": 5470
    },
    {
      "epoch": 0.5970230556576922,
      "grad_norm": 1.270250916481018,
      "learning_rate": 4.0048667102491466e-05,
      "loss": 2.61,
      "step": 5480
    },
    {
      "epoch": 0.5981125137884544,
      "grad_norm": 1.368113398551941,
      "learning_rate": 4.003050773589017e-05,
      "loss": 2.6469,
      "step": 5490
    },
    {
      "epoch": 0.5992019719192166,
      "grad_norm": 1.3346372842788696,
      "learning_rate": 4.001234836928888e-05,
      "loss": 2.4861,
      "step": 5500
    },
    {
      "epoch": 0.6002914300499789,
      "grad_norm": 1.3145190477371216,
      "learning_rate": 3.999418900268759e-05,
      "loss": 2.6703,
      "step": 5510
    },
    {
      "epoch": 0.6013808881807411,
      "grad_norm": 1.32204270362854,
      "learning_rate": 3.9976029636086294e-05,
      "loss": 2.5335,
      "step": 5520
    },
    {
      "epoch": 0.6024703463115033,
      "grad_norm": 1.3178179264068604,
      "learning_rate": 3.9957870269485006e-05,
      "loss": 2.6904,
      "step": 5530
    },
    {
      "epoch": 0.6035598044422655,
      "grad_norm": 1.3431390523910522,
      "learning_rate": 3.993971090288371e-05,
      "loss": 2.5614,
      "step": 5540
    },
    {
      "epoch": 0.6046492625730278,
      "grad_norm": 1.2804620265960693,
      "learning_rate": 3.992155153628242e-05,
      "loss": 2.6418,
      "step": 5550
    },
    {
      "epoch": 0.60573872070379,
      "grad_norm": 1.2517600059509277,
      "learning_rate": 3.990339216968112e-05,
      "loss": 2.6654,
      "step": 5560
    },
    {
      "epoch": 0.6068281788345522,
      "grad_norm": 1.1669833660125732,
      "learning_rate": 3.988523280307983e-05,
      "loss": 2.6476,
      "step": 5570
    },
    {
      "epoch": 0.6079176369653144,
      "grad_norm": 1.232467770576477,
      "learning_rate": 3.9867073436478533e-05,
      "loss": 2.5616,
      "step": 5580
    },
    {
      "epoch": 0.6090070950960765,
      "grad_norm": 1.2698205709457397,
      "learning_rate": 3.9848914069877246e-05,
      "loss": 2.6063,
      "step": 5590
    },
    {
      "epoch": 0.6100965532268388,
      "grad_norm": 1.2955483198165894,
      "learning_rate": 3.983075470327595e-05,
      "loss": 2.5212,
      "step": 5600
    },
    {
      "epoch": 0.611186011357601,
      "grad_norm": 1.1985201835632324,
      "learning_rate": 3.9812595336674656e-05,
      "loss": 2.5858,
      "step": 5610
    },
    {
      "epoch": 0.6122754694883632,
      "grad_norm": 1.346691608428955,
      "learning_rate": 3.979443597007337e-05,
      "loss": 2.607,
      "step": 5620
    },
    {
      "epoch": 0.6133649276191254,
      "grad_norm": 1.3021644353866577,
      "learning_rate": 3.9776276603472074e-05,
      "loss": 2.6176,
      "step": 5630
    },
    {
      "epoch": 0.6144543857498876,
      "grad_norm": 1.2380131483078003,
      "learning_rate": 3.975811723687078e-05,
      "loss": 2.6437,
      "step": 5640
    },
    {
      "epoch": 0.6155438438806499,
      "grad_norm": 1.2964468002319336,
      "learning_rate": 3.9739957870269485e-05,
      "loss": 2.6066,
      "step": 5650
    },
    {
      "epoch": 0.6166333020114121,
      "grad_norm": 1.247754454612732,
      "learning_rate": 3.97217985036682e-05,
      "loss": 2.6262,
      "step": 5660
    },
    {
      "epoch": 0.6177227601421743,
      "grad_norm": 1.3531839847564697,
      "learning_rate": 3.97036391370669e-05,
      "loss": 2.609,
      "step": 5670
    },
    {
      "epoch": 0.6188122182729365,
      "grad_norm": 1.2966011762619019,
      "learning_rate": 3.968547977046561e-05,
      "loss": 2.6611,
      "step": 5680
    },
    {
      "epoch": 0.6199016764036988,
      "grad_norm": 1.3229373693466187,
      "learning_rate": 3.966732040386431e-05,
      "loss": 2.6173,
      "step": 5690
    },
    {
      "epoch": 0.620991134534461,
      "grad_norm": 1.276739478111267,
      "learning_rate": 3.964916103726302e-05,
      "loss": 2.591,
      "step": 5700
    },
    {
      "epoch": 0.6220805926652231,
      "grad_norm": 1.2754569053649902,
      "learning_rate": 3.9631001670661724e-05,
      "loss": 2.5617,
      "step": 5710
    },
    {
      "epoch": 0.6231700507959853,
      "grad_norm": 1.3617143630981445,
      "learning_rate": 3.9612842304060436e-05,
      "loss": 2.4888,
      "step": 5720
    },
    {
      "epoch": 0.6242595089267475,
      "grad_norm": 1.2134126424789429,
      "learning_rate": 3.959468293745914e-05,
      "loss": 2.5064,
      "step": 5730
    },
    {
      "epoch": 0.6253489670575098,
      "grad_norm": 1.2078765630722046,
      "learning_rate": 3.957652357085785e-05,
      "loss": 2.5037,
      "step": 5740
    },
    {
      "epoch": 0.626438425188272,
      "grad_norm": 1.336477279663086,
      "learning_rate": 3.955836420425656e-05,
      "loss": 2.6335,
      "step": 5750
    },
    {
      "epoch": 0.6275278833190342,
      "grad_norm": 1.2826225757598877,
      "learning_rate": 3.9540204837655265e-05,
      "loss": 2.5773,
      "step": 5760
    },
    {
      "epoch": 0.6286173414497964,
      "grad_norm": 1.3253885507583618,
      "learning_rate": 3.952204547105397e-05,
      "loss": 2.5891,
      "step": 5770
    },
    {
      "epoch": 0.6297067995805586,
      "grad_norm": 1.2600557804107666,
      "learning_rate": 3.950388610445268e-05,
      "loss": 2.5354,
      "step": 5780
    },
    {
      "epoch": 0.6307962577113209,
      "grad_norm": 1.391126275062561,
      "learning_rate": 3.948572673785139e-05,
      "loss": 2.6201,
      "step": 5790
    },
    {
      "epoch": 0.6318857158420831,
      "grad_norm": 1.4006379842758179,
      "learning_rate": 3.946756737125009e-05,
      "loss": 2.6443,
      "step": 5800
    },
    {
      "epoch": 0.6329751739728453,
      "grad_norm": 1.2646276950836182,
      "learning_rate": 3.9449408004648805e-05,
      "loss": 2.535,
      "step": 5810
    },
    {
      "epoch": 0.6340646321036074,
      "grad_norm": 1.2495733499526978,
      "learning_rate": 3.943124863804751e-05,
      "loss": 2.5681,
      "step": 5820
    },
    {
      "epoch": 0.6351540902343696,
      "grad_norm": 1.377212405204773,
      "learning_rate": 3.9413089271446216e-05,
      "loss": 2.5695,
      "step": 5830
    },
    {
      "epoch": 0.6362435483651319,
      "grad_norm": 1.3432857990264893,
      "learning_rate": 3.9394929904844915e-05,
      "loss": 2.5458,
      "step": 5840
    },
    {
      "epoch": 0.6373330064958941,
      "grad_norm": 1.2332487106323242,
      "learning_rate": 3.937677053824363e-05,
      "loss": 2.6583,
      "step": 5850
    },
    {
      "epoch": 0.6384224646266563,
      "grad_norm": 1.3751184940338135,
      "learning_rate": 3.935861117164233e-05,
      "loss": 2.6303,
      "step": 5860
    },
    {
      "epoch": 0.6395119227574185,
      "grad_norm": 1.3088006973266602,
      "learning_rate": 3.934045180504104e-05,
      "loss": 2.5851,
      "step": 5870
    },
    {
      "epoch": 0.6406013808881807,
      "grad_norm": 1.3106168508529663,
      "learning_rate": 3.932229243843975e-05,
      "loss": 2.602,
      "step": 5880
    },
    {
      "epoch": 0.641690839018943,
      "grad_norm": 1.2224524021148682,
      "learning_rate": 3.9304133071838456e-05,
      "loss": 2.5068,
      "step": 5890
    },
    {
      "epoch": 0.6427802971497052,
      "grad_norm": 1.30449378490448,
      "learning_rate": 3.928597370523716e-05,
      "loss": 2.5398,
      "step": 5900
    },
    {
      "epoch": 0.6438697552804674,
      "grad_norm": 1.2341874837875366,
      "learning_rate": 3.926781433863587e-05,
      "loss": 2.578,
      "step": 5910
    },
    {
      "epoch": 0.6449592134112296,
      "grad_norm": 1.2815704345703125,
      "learning_rate": 3.924965497203458e-05,
      "loss": 2.6916,
      "step": 5920
    },
    {
      "epoch": 0.6460486715419917,
      "grad_norm": 1.3512463569641113,
      "learning_rate": 3.9231495605433284e-05,
      "loss": 2.578,
      "step": 5930
    },
    {
      "epoch": 0.647138129672754,
      "grad_norm": 1.2171986103057861,
      "learning_rate": 3.9213336238831996e-05,
      "loss": 2.6195,
      "step": 5940
    },
    {
      "epoch": 0.6482275878035162,
      "grad_norm": 1.330141305923462,
      "learning_rate": 3.91951768722307e-05,
      "loss": 2.5128,
      "step": 5950
    },
    {
      "epoch": 0.6493170459342784,
      "grad_norm": 1.2652360200881958,
      "learning_rate": 3.917701750562941e-05,
      "loss": 2.7588,
      "step": 5960
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 1.2354191541671753,
      "learning_rate": 3.915885813902811e-05,
      "loss": 2.5006,
      "step": 5970
    },
    {
      "epoch": 0.6514959621958029,
      "grad_norm": 1.3614251613616943,
      "learning_rate": 3.914069877242682e-05,
      "loss": 2.535,
      "step": 5980
    },
    {
      "epoch": 0.6525854203265651,
      "grad_norm": 1.3191661834716797,
      "learning_rate": 3.912253940582552e-05,
      "loss": 2.6053,
      "step": 5990
    },
    {
      "epoch": 0.6536748784573273,
      "grad_norm": 1.3013750314712524,
      "learning_rate": 3.910438003922423e-05,
      "loss": 2.571,
      "step": 6000
    },
    {
      "epoch": 0.6547643365880895,
      "grad_norm": 1.2904876470565796,
      "learning_rate": 3.908622067262294e-05,
      "loss": 2.4862,
      "step": 6010
    },
    {
      "epoch": 0.6558537947188517,
      "grad_norm": 1.3037264347076416,
      "learning_rate": 3.9068061306021646e-05,
      "loss": 2.5689,
      "step": 6020
    },
    {
      "epoch": 0.656943252849614,
      "grad_norm": 1.2373616695404053,
      "learning_rate": 3.904990193942035e-05,
      "loss": 2.5405,
      "step": 6030
    },
    {
      "epoch": 0.6580327109803762,
      "grad_norm": 1.3432496786117554,
      "learning_rate": 3.9031742572819064e-05,
      "loss": 2.4965,
      "step": 6040
    },
    {
      "epoch": 0.6591221691111383,
      "grad_norm": 1.313663125038147,
      "learning_rate": 3.901358320621777e-05,
      "loss": 2.5397,
      "step": 6050
    },
    {
      "epoch": 0.6602116272419005,
      "grad_norm": 1.3779311180114746,
      "learning_rate": 3.8995423839616475e-05,
      "loss": 2.5277,
      "step": 6060
    },
    {
      "epoch": 0.6613010853726627,
      "grad_norm": 1.26426100730896,
      "learning_rate": 3.897726447301519e-05,
      "loss": 2.503,
      "step": 6070
    },
    {
      "epoch": 0.662390543503425,
      "grad_norm": 1.339010238647461,
      "learning_rate": 3.895910510641389e-05,
      "loss": 2.6774,
      "step": 6080
    },
    {
      "epoch": 0.6634800016341872,
      "grad_norm": 1.4571638107299805,
      "learning_rate": 3.89409457398126e-05,
      "loss": 2.5886,
      "step": 6090
    },
    {
      "epoch": 0.6645694597649494,
      "grad_norm": 1.3544214963912964,
      "learning_rate": 3.89227863732113e-05,
      "loss": 2.5253,
      "step": 6100
    },
    {
      "epoch": 0.6656589178957116,
      "grad_norm": 1.3753515481948853,
      "learning_rate": 3.890462700661001e-05,
      "loss": 2.6261,
      "step": 6110
    },
    {
      "epoch": 0.6667483760264739,
      "grad_norm": 1.2864115238189697,
      "learning_rate": 3.8886467640008714e-05,
      "loss": 2.5779,
      "step": 6120
    },
    {
      "epoch": 0.6678378341572361,
      "grad_norm": 1.2851821184158325,
      "learning_rate": 3.8868308273407426e-05,
      "loss": 2.541,
      "step": 6130
    },
    {
      "epoch": 0.6689272922879983,
      "grad_norm": 1.2431434392929077,
      "learning_rate": 3.885014890680613e-05,
      "loss": 2.6829,
      "step": 6140
    },
    {
      "epoch": 0.6700167504187605,
      "grad_norm": 1.285117506980896,
      "learning_rate": 3.883198954020484e-05,
      "loss": 2.6145,
      "step": 6150
    },
    {
      "epoch": 0.6711062085495226,
      "grad_norm": 1.3333115577697754,
      "learning_rate": 3.881383017360355e-05,
      "loss": 2.6679,
      "step": 6160
    },
    {
      "epoch": 0.6721956666802849,
      "grad_norm": 1.2579419612884521,
      "learning_rate": 3.8795670807002255e-05,
      "loss": 2.5692,
      "step": 6170
    },
    {
      "epoch": 0.6732851248110471,
      "grad_norm": 1.3541631698608398,
      "learning_rate": 3.877751144040096e-05,
      "loss": 2.6969,
      "step": 6180
    },
    {
      "epoch": 0.6743745829418093,
      "grad_norm": 1.2981263399124146,
      "learning_rate": 3.8759352073799666e-05,
      "loss": 2.6239,
      "step": 6190
    },
    {
      "epoch": 0.6754640410725715,
      "grad_norm": 1.3059766292572021,
      "learning_rate": 3.874119270719838e-05,
      "loss": 2.4664,
      "step": 6200
    },
    {
      "epoch": 0.6765534992033337,
      "grad_norm": 1.419616460800171,
      "learning_rate": 3.872303334059708e-05,
      "loss": 2.5151,
      "step": 6210
    },
    {
      "epoch": 0.677642957334096,
      "grad_norm": 1.2727947235107422,
      "learning_rate": 3.870487397399579e-05,
      "loss": 2.5707,
      "step": 6220
    },
    {
      "epoch": 0.6787324154648582,
      "grad_norm": 1.264042854309082,
      "learning_rate": 3.86867146073945e-05,
      "loss": 2.5489,
      "step": 6230
    },
    {
      "epoch": 0.6798218735956204,
      "grad_norm": 1.3017969131469727,
      "learning_rate": 3.86685552407932e-05,
      "loss": 2.5536,
      "step": 6240
    },
    {
      "epoch": 0.6809113317263826,
      "grad_norm": 1.3717195987701416,
      "learning_rate": 3.8650395874191905e-05,
      "loss": 2.5089,
      "step": 6250
    },
    {
      "epoch": 0.6820007898571449,
      "grad_norm": 1.3574509620666504,
      "learning_rate": 3.863223650759062e-05,
      "loss": 2.5346,
      "step": 6260
    },
    {
      "epoch": 0.683090247987907,
      "grad_norm": 1.340807557106018,
      "learning_rate": 3.861407714098932e-05,
      "loss": 2.5907,
      "step": 6270
    },
    {
      "epoch": 0.6841797061186692,
      "grad_norm": 1.3846575021743774,
      "learning_rate": 3.859591777438803e-05,
      "loss": 2.542,
      "step": 6280
    },
    {
      "epoch": 0.6852691642494314,
      "grad_norm": 1.2880969047546387,
      "learning_rate": 3.857775840778674e-05,
      "loss": 2.7406,
      "step": 6290
    },
    {
      "epoch": 0.6863586223801936,
      "grad_norm": 1.3192957639694214,
      "learning_rate": 3.8559599041185445e-05,
      "loss": 2.5134,
      "step": 6300
    },
    {
      "epoch": 0.6874480805109559,
      "grad_norm": 1.295870065689087,
      "learning_rate": 3.854143967458415e-05,
      "loss": 2.6028,
      "step": 6310
    },
    {
      "epoch": 0.6885375386417181,
      "grad_norm": 1.3449313640594482,
      "learning_rate": 3.852328030798286e-05,
      "loss": 2.539,
      "step": 6320
    },
    {
      "epoch": 0.6896269967724803,
      "grad_norm": 1.3333550691604614,
      "learning_rate": 3.850512094138157e-05,
      "loss": 2.5542,
      "step": 6330
    },
    {
      "epoch": 0.6907164549032425,
      "grad_norm": 1.2967612743377686,
      "learning_rate": 3.8486961574780274e-05,
      "loss": 2.6052,
      "step": 6340
    },
    {
      "epoch": 0.6918059130340047,
      "grad_norm": 1.3569872379302979,
      "learning_rate": 3.846880220817898e-05,
      "loss": 2.5671,
      "step": 6350
    },
    {
      "epoch": 0.692895371164767,
      "grad_norm": 1.2756513357162476,
      "learning_rate": 3.845064284157769e-05,
      "loss": 2.5899,
      "step": 6360
    },
    {
      "epoch": 0.6939848292955292,
      "grad_norm": 1.3081048727035522,
      "learning_rate": 3.84324834749764e-05,
      "loss": 2.6168,
      "step": 6370
    },
    {
      "epoch": 0.6950742874262913,
      "grad_norm": 1.3438420295715332,
      "learning_rate": 3.84143241083751e-05,
      "loss": 2.5784,
      "step": 6380
    },
    {
      "epoch": 0.6961637455570535,
      "grad_norm": 1.2994590997695923,
      "learning_rate": 3.839616474177381e-05,
      "loss": 2.661,
      "step": 6390
    },
    {
      "epoch": 0.6972532036878157,
      "grad_norm": 1.2993464469909668,
      "learning_rate": 3.837800537517251e-05,
      "loss": 2.5206,
      "step": 6400
    },
    {
      "epoch": 0.698342661818578,
      "grad_norm": 1.279249906539917,
      "learning_rate": 3.835984600857122e-05,
      "loss": 2.5681,
      "step": 6410
    },
    {
      "epoch": 0.6994321199493402,
      "grad_norm": 1.3201862573623657,
      "learning_rate": 3.834168664196993e-05,
      "loss": 2.533,
      "step": 6420
    },
    {
      "epoch": 0.7005215780801024,
      "grad_norm": 1.306715726852417,
      "learning_rate": 3.8323527275368636e-05,
      "loss": 2.7096,
      "step": 6430
    },
    {
      "epoch": 0.7016110362108646,
      "grad_norm": 1.2324427366256714,
      "learning_rate": 3.830536790876734e-05,
      "loss": 2.5079,
      "step": 6440
    },
    {
      "epoch": 0.7027004943416268,
      "grad_norm": 1.3924095630645752,
      "learning_rate": 3.8287208542166054e-05,
      "loss": 2.5264,
      "step": 6450
    },
    {
      "epoch": 0.7037899524723891,
      "grad_norm": 1.3240634202957153,
      "learning_rate": 3.826904917556476e-05,
      "loss": 2.526,
      "step": 6460
    },
    {
      "epoch": 0.7048794106031513,
      "grad_norm": 1.2452667951583862,
      "learning_rate": 3.8250889808963465e-05,
      "loss": 2.4733,
      "step": 6470
    },
    {
      "epoch": 0.7059688687339135,
      "grad_norm": 1.2800933122634888,
      "learning_rate": 3.823273044236218e-05,
      "loss": 2.5848,
      "step": 6480
    },
    {
      "epoch": 0.7070583268646757,
      "grad_norm": 1.4017573595046997,
      "learning_rate": 3.821457107576088e-05,
      "loss": 2.5355,
      "step": 6490
    },
    {
      "epoch": 0.7081477849954378,
      "grad_norm": 1.2320095300674438,
      "learning_rate": 3.819641170915959e-05,
      "loss": 2.5269,
      "step": 6500
    },
    {
      "epoch": 0.7092372431262001,
      "grad_norm": 1.2614027261734009,
      "learning_rate": 3.817825234255829e-05,
      "loss": 2.6796,
      "step": 6510
    },
    {
      "epoch": 0.7103267012569623,
      "grad_norm": 1.2362021207809448,
      "learning_rate": 3.8160092975957e-05,
      "loss": 2.5572,
      "step": 6520
    },
    {
      "epoch": 0.7114161593877245,
      "grad_norm": 1.2116960287094116,
      "learning_rate": 3.8141933609355704e-05,
      "loss": 2.5915,
      "step": 6530
    },
    {
      "epoch": 0.7125056175184867,
      "grad_norm": 1.275065541267395,
      "learning_rate": 3.812377424275441e-05,
      "loss": 2.6455,
      "step": 6540
    },
    {
      "epoch": 0.713595075649249,
      "grad_norm": 1.3667460680007935,
      "learning_rate": 3.810561487615312e-05,
      "loss": 2.5634,
      "step": 6550
    },
    {
      "epoch": 0.7146845337800112,
      "grad_norm": 1.316221833229065,
      "learning_rate": 3.808745550955183e-05,
      "loss": 2.5838,
      "step": 6560
    },
    {
      "epoch": 0.7157739919107734,
      "grad_norm": 1.3220678567886353,
      "learning_rate": 3.806929614295053e-05,
      "loss": 2.5006,
      "step": 6570
    },
    {
      "epoch": 0.7168634500415356,
      "grad_norm": 1.2878596782684326,
      "learning_rate": 3.8051136776349245e-05,
      "loss": 2.5978,
      "step": 6580
    },
    {
      "epoch": 0.7179529081722978,
      "grad_norm": 1.2978676557540894,
      "learning_rate": 3.803297740974795e-05,
      "loss": 2.5525,
      "step": 6590
    },
    {
      "epoch": 0.7190423663030601,
      "grad_norm": 1.303465723991394,
      "learning_rate": 3.8014818043146655e-05,
      "loss": 2.5406,
      "step": 6600
    },
    {
      "epoch": 0.7201318244338222,
      "grad_norm": 1.3794217109680176,
      "learning_rate": 3.799665867654537e-05,
      "loss": 2.551,
      "step": 6610
    },
    {
      "epoch": 0.7212212825645844,
      "grad_norm": 1.2071044445037842,
      "learning_rate": 3.797849930994407e-05,
      "loss": 2.5668,
      "step": 6620
    },
    {
      "epoch": 0.7223107406953466,
      "grad_norm": 1.2874895334243774,
      "learning_rate": 3.796033994334278e-05,
      "loss": 2.5658,
      "step": 6630
    },
    {
      "epoch": 0.7234001988261088,
      "grad_norm": 1.3535377979278564,
      "learning_rate": 3.794218057674149e-05,
      "loss": 2.4882,
      "step": 6640
    },
    {
      "epoch": 0.7244896569568711,
      "grad_norm": 1.3949528932571411,
      "learning_rate": 3.792402121014019e-05,
      "loss": 2.6144,
      "step": 6650
    },
    {
      "epoch": 0.7255791150876333,
      "grad_norm": 1.338711142539978,
      "learning_rate": 3.7905861843538895e-05,
      "loss": 2.5106,
      "step": 6660
    },
    {
      "epoch": 0.7266685732183955,
      "grad_norm": 1.3189210891723633,
      "learning_rate": 3.788770247693761e-05,
      "loss": 2.5595,
      "step": 6670
    },
    {
      "epoch": 0.7277580313491577,
      "grad_norm": 1.282431960105896,
      "learning_rate": 3.786954311033631e-05,
      "loss": 2.5622,
      "step": 6680
    },
    {
      "epoch": 0.72884748947992,
      "grad_norm": 1.2739924192428589,
      "learning_rate": 3.785138374373502e-05,
      "loss": 2.5267,
      "step": 6690
    },
    {
      "epoch": 0.7299369476106822,
      "grad_norm": 1.2881640195846558,
      "learning_rate": 3.783322437713372e-05,
      "loss": 2.646,
      "step": 6700
    },
    {
      "epoch": 0.7310264057414444,
      "grad_norm": 1.3780573606491089,
      "learning_rate": 3.7815065010532435e-05,
      "loss": 2.6213,
      "step": 6710
    },
    {
      "epoch": 0.7321158638722065,
      "grad_norm": 1.2480711936950684,
      "learning_rate": 3.779690564393114e-05,
      "loss": 2.5248,
      "step": 6720
    },
    {
      "epoch": 0.7332053220029687,
      "grad_norm": 1.3102173805236816,
      "learning_rate": 3.7778746277329846e-05,
      "loss": 2.5042,
      "step": 6730
    },
    {
      "epoch": 0.734294780133731,
      "grad_norm": 1.2791752815246582,
      "learning_rate": 3.776058691072856e-05,
      "loss": 2.5731,
      "step": 6740
    },
    {
      "epoch": 0.7353842382644932,
      "grad_norm": 1.2707574367523193,
      "learning_rate": 3.7742427544127264e-05,
      "loss": 2.5662,
      "step": 6750
    },
    {
      "epoch": 0.7364736963952554,
      "grad_norm": 1.3831737041473389,
      "learning_rate": 3.772426817752597e-05,
      "loss": 2.5963,
      "step": 6760
    },
    {
      "epoch": 0.7375631545260176,
      "grad_norm": 1.3118500709533691,
      "learning_rate": 3.770610881092468e-05,
      "loss": 2.5838,
      "step": 6770
    },
    {
      "epoch": 0.7386526126567798,
      "grad_norm": 1.3118010759353638,
      "learning_rate": 3.768794944432339e-05,
      "loss": 2.6128,
      "step": 6780
    },
    {
      "epoch": 0.7397420707875421,
      "grad_norm": 1.3089314699172974,
      "learning_rate": 3.766979007772209e-05,
      "loss": 2.5606,
      "step": 6790
    },
    {
      "epoch": 0.7408315289183043,
      "grad_norm": 1.4174256324768066,
      "learning_rate": 3.76516307111208e-05,
      "loss": 2.6225,
      "step": 6800
    },
    {
      "epoch": 0.7419209870490665,
      "grad_norm": 1.324442982673645,
      "learning_rate": 3.76334713445195e-05,
      "loss": 2.5071,
      "step": 6810
    },
    {
      "epoch": 0.7430104451798287,
      "grad_norm": 1.336480975151062,
      "learning_rate": 3.761531197791821e-05,
      "loss": 2.5684,
      "step": 6820
    },
    {
      "epoch": 0.744099903310591,
      "grad_norm": 1.2747656106948853,
      "learning_rate": 3.759715261131692e-05,
      "loss": 2.522,
      "step": 6830
    },
    {
      "epoch": 0.7451893614413531,
      "grad_norm": 1.3126616477966309,
      "learning_rate": 3.7578993244715626e-05,
      "loss": 2.5189,
      "step": 6840
    },
    {
      "epoch": 0.7462788195721153,
      "grad_norm": 1.3105791807174683,
      "learning_rate": 3.756083387811433e-05,
      "loss": 2.5662,
      "step": 6850
    },
    {
      "epoch": 0.7473682777028775,
      "grad_norm": 1.2582216262817383,
      "learning_rate": 3.7542674511513044e-05,
      "loss": 2.5846,
      "step": 6860
    },
    {
      "epoch": 0.7484577358336397,
      "grad_norm": 1.3609734773635864,
      "learning_rate": 3.752451514491175e-05,
      "loss": 2.5864,
      "step": 6870
    },
    {
      "epoch": 0.749547193964402,
      "grad_norm": 1.301622986793518,
      "learning_rate": 3.7506355778310455e-05,
      "loss": 2.5027,
      "step": 6880
    },
    {
      "epoch": 0.7506366520951642,
      "grad_norm": 1.335318684577942,
      "learning_rate": 3.748819641170916e-05,
      "loss": 2.5199,
      "step": 6890
    },
    {
      "epoch": 0.7517261102259264,
      "grad_norm": 1.2530564069747925,
      "learning_rate": 3.747003704510787e-05,
      "loss": 2.4863,
      "step": 6900
    },
    {
      "epoch": 0.7528155683566886,
      "grad_norm": 1.3311065435409546,
      "learning_rate": 3.745187767850658e-05,
      "loss": 2.5182,
      "step": 6910
    },
    {
      "epoch": 0.7539050264874508,
      "grad_norm": 1.3615673780441284,
      "learning_rate": 3.743371831190528e-05,
      "loss": 2.5463,
      "step": 6920
    },
    {
      "epoch": 0.7549944846182131,
      "grad_norm": 1.4242664575576782,
      "learning_rate": 3.741555894530399e-05,
      "loss": 2.5786,
      "step": 6930
    },
    {
      "epoch": 0.7560839427489753,
      "grad_norm": 1.3610503673553467,
      "learning_rate": 3.7397399578702694e-05,
      "loss": 2.4914,
      "step": 6940
    },
    {
      "epoch": 0.7571734008797374,
      "grad_norm": 1.304093837738037,
      "learning_rate": 3.73792402121014e-05,
      "loss": 2.5185,
      "step": 6950
    },
    {
      "epoch": 0.7582628590104996,
      "grad_norm": 1.3148021697998047,
      "learning_rate": 3.736108084550011e-05,
      "loss": 2.5386,
      "step": 6960
    },
    {
      "epoch": 0.7593523171412618,
      "grad_norm": 1.2907075881958008,
      "learning_rate": 3.734292147889882e-05,
      "loss": 2.6502,
      "step": 6970
    },
    {
      "epoch": 0.7604417752720241,
      "grad_norm": 1.228010892868042,
      "learning_rate": 3.732476211229752e-05,
      "loss": 2.5668,
      "step": 6980
    },
    {
      "epoch": 0.7615312334027863,
      "grad_norm": 1.3272366523742676,
      "learning_rate": 3.7306602745696234e-05,
      "loss": 2.5606,
      "step": 6990
    },
    {
      "epoch": 0.7626206915335485,
      "grad_norm": 1.295735239982605,
      "learning_rate": 3.728844337909494e-05,
      "loss": 2.5926,
      "step": 7000
    },
    {
      "epoch": 0.7637101496643107,
      "grad_norm": 1.3287687301635742,
      "learning_rate": 3.7270284012493645e-05,
      "loss": 2.5751,
      "step": 7010
    },
    {
      "epoch": 0.7647996077950729,
      "grad_norm": 1.2539602518081665,
      "learning_rate": 3.725212464589236e-05,
      "loss": 2.5285,
      "step": 7020
    },
    {
      "epoch": 0.7658890659258352,
      "grad_norm": 1.2086716890335083,
      "learning_rate": 3.723396527929106e-05,
      "loss": 2.4654,
      "step": 7030
    },
    {
      "epoch": 0.7669785240565974,
      "grad_norm": 1.3437103033065796,
      "learning_rate": 3.721580591268977e-05,
      "loss": 2.567,
      "step": 7040
    },
    {
      "epoch": 0.7680679821873596,
      "grad_norm": 1.3074060678482056,
      "learning_rate": 3.7197646546088474e-05,
      "loss": 2.5948,
      "step": 7050
    },
    {
      "epoch": 0.7691574403181217,
      "grad_norm": 1.207601547241211,
      "learning_rate": 3.717948717948718e-05,
      "loss": 2.5354,
      "step": 7060
    },
    {
      "epoch": 0.7702468984488839,
      "grad_norm": 1.316432237625122,
      "learning_rate": 3.7161327812885885e-05,
      "loss": 2.595,
      "step": 7070
    },
    {
      "epoch": 0.7713363565796462,
      "grad_norm": 1.306343913078308,
      "learning_rate": 3.714316844628459e-05,
      "loss": 2.6245,
      "step": 7080
    },
    {
      "epoch": 0.7724258147104084,
      "grad_norm": 1.3692222833633423,
      "learning_rate": 3.71250090796833e-05,
      "loss": 2.6461,
      "step": 7090
    },
    {
      "epoch": 0.7735152728411706,
      "grad_norm": 1.2954035997390747,
      "learning_rate": 3.710684971308201e-05,
      "loss": 2.5313,
      "step": 7100
    },
    {
      "epoch": 0.7746047309719328,
      "grad_norm": 1.2582981586456299,
      "learning_rate": 3.708869034648071e-05,
      "loss": 2.5457,
      "step": 7110
    },
    {
      "epoch": 0.775694189102695,
      "grad_norm": 1.3390005826950073,
      "learning_rate": 3.7070530979879425e-05,
      "loss": 2.5073,
      "step": 7120
    },
    {
      "epoch": 0.7767836472334573,
      "grad_norm": 1.3290581703186035,
      "learning_rate": 3.705237161327813e-05,
      "loss": 2.4909,
      "step": 7130
    },
    {
      "epoch": 0.7778731053642195,
      "grad_norm": 1.3170617818832397,
      "learning_rate": 3.7034212246676836e-05,
      "loss": 2.5315,
      "step": 7140
    },
    {
      "epoch": 0.7789625634949817,
      "grad_norm": 1.2599477767944336,
      "learning_rate": 3.701605288007555e-05,
      "loss": 2.5829,
      "step": 7150
    },
    {
      "epoch": 0.7800520216257439,
      "grad_norm": 1.2619397640228271,
      "learning_rate": 3.6997893513474254e-05,
      "loss": 2.5251,
      "step": 7160
    },
    {
      "epoch": 0.7811414797565062,
      "grad_norm": 1.2297135591506958,
      "learning_rate": 3.697973414687296e-05,
      "loss": 2.5196,
      "step": 7170
    },
    {
      "epoch": 0.7822309378872683,
      "grad_norm": 1.3450766801834106,
      "learning_rate": 3.696157478027167e-05,
      "loss": 2.5141,
      "step": 7180
    },
    {
      "epoch": 0.7833203960180305,
      "grad_norm": 1.3472654819488525,
      "learning_rate": 3.694341541367038e-05,
      "loss": 2.5111,
      "step": 7190
    },
    {
      "epoch": 0.7844098541487927,
      "grad_norm": 1.3707389831542969,
      "learning_rate": 3.692525604706908e-05,
      "loss": 2.5817,
      "step": 7200
    },
    {
      "epoch": 0.7854993122795549,
      "grad_norm": 1.3694690465927124,
      "learning_rate": 3.690709668046779e-05,
      "loss": 2.5303,
      "step": 7210
    },
    {
      "epoch": 0.7865887704103172,
      "grad_norm": 1.3729416131973267,
      "learning_rate": 3.688893731386649e-05,
      "loss": 2.5452,
      "step": 7220
    },
    {
      "epoch": 0.7876782285410794,
      "grad_norm": 1.3400652408599854,
      "learning_rate": 3.68707779472652e-05,
      "loss": 2.5689,
      "step": 7230
    },
    {
      "epoch": 0.7887676866718416,
      "grad_norm": 1.2663320302963257,
      "learning_rate": 3.6852618580663904e-05,
      "loss": 2.6143,
      "step": 7240
    },
    {
      "epoch": 0.7898571448026038,
      "grad_norm": 1.2656735181808472,
      "learning_rate": 3.6834459214062616e-05,
      "loss": 2.5706,
      "step": 7250
    },
    {
      "epoch": 0.790946602933366,
      "grad_norm": 1.2523143291473389,
      "learning_rate": 3.681629984746132e-05,
      "loss": 2.4946,
      "step": 7260
    },
    {
      "epoch": 0.7920360610641283,
      "grad_norm": 1.41012704372406,
      "learning_rate": 3.679814048086003e-05,
      "loss": 2.5127,
      "step": 7270
    },
    {
      "epoch": 0.7931255191948905,
      "grad_norm": 1.2434747219085693,
      "learning_rate": 3.677998111425874e-05,
      "loss": 2.5171,
      "step": 7280
    },
    {
      "epoch": 0.7942149773256526,
      "grad_norm": 1.3593156337738037,
      "learning_rate": 3.6761821747657444e-05,
      "loss": 2.4989,
      "step": 7290
    },
    {
      "epoch": 0.7953044354564148,
      "grad_norm": 1.3072477579116821,
      "learning_rate": 3.674366238105615e-05,
      "loss": 2.5869,
      "step": 7300
    },
    {
      "epoch": 0.796393893587177,
      "grad_norm": 1.3305295705795288,
      "learning_rate": 3.672550301445486e-05,
      "loss": 2.6578,
      "step": 7310
    },
    {
      "epoch": 0.7974833517179393,
      "grad_norm": 1.3688091039657593,
      "learning_rate": 3.670734364785357e-05,
      "loss": 2.6379,
      "step": 7320
    },
    {
      "epoch": 0.7985728098487015,
      "grad_norm": 1.2317434549331665,
      "learning_rate": 3.668918428125227e-05,
      "loss": 2.544,
      "step": 7330
    },
    {
      "epoch": 0.7996622679794637,
      "grad_norm": 1.335211157798767,
      "learning_rate": 3.667102491465098e-05,
      "loss": 2.6791,
      "step": 7340
    },
    {
      "epoch": 0.8007517261102259,
      "grad_norm": 1.2831296920776367,
      "learning_rate": 3.6652865548049684e-05,
      "loss": 2.6406,
      "step": 7350
    },
    {
      "epoch": 0.8018411842409882,
      "grad_norm": 1.3792128562927246,
      "learning_rate": 3.663470618144839e-05,
      "loss": 2.6538,
      "step": 7360
    },
    {
      "epoch": 0.8029306423717504,
      "grad_norm": 1.3672269582748413,
      "learning_rate": 3.66165468148471e-05,
      "loss": 2.5695,
      "step": 7370
    },
    {
      "epoch": 0.8040201005025126,
      "grad_norm": 1.2549195289611816,
      "learning_rate": 3.659838744824581e-05,
      "loss": 2.5649,
      "step": 7380
    },
    {
      "epoch": 0.8051095586332748,
      "grad_norm": 1.2717353105545044,
      "learning_rate": 3.658022808164451e-05,
      "loss": 2.5857,
      "step": 7390
    },
    {
      "epoch": 0.8061990167640369,
      "grad_norm": 1.327440619468689,
      "learning_rate": 3.656206871504322e-05,
      "loss": 2.5671,
      "step": 7400
    },
    {
      "epoch": 0.8072884748947992,
      "grad_norm": 1.3276190757751465,
      "learning_rate": 3.654390934844193e-05,
      "loss": 2.6308,
      "step": 7410
    },
    {
      "epoch": 0.8083779330255614,
      "grad_norm": 1.3131520748138428,
      "learning_rate": 3.6525749981840635e-05,
      "loss": 2.6261,
      "step": 7420
    },
    {
      "epoch": 0.8094673911563236,
      "grad_norm": 1.331555962562561,
      "learning_rate": 3.650759061523934e-05,
      "loss": 2.5259,
      "step": 7430
    },
    {
      "epoch": 0.8105568492870858,
      "grad_norm": 1.3560233116149902,
      "learning_rate": 3.648943124863805e-05,
      "loss": 2.6134,
      "step": 7440
    },
    {
      "epoch": 0.811646307417848,
      "grad_norm": 1.3783695697784424,
      "learning_rate": 3.647127188203676e-05,
      "loss": 2.6277,
      "step": 7450
    },
    {
      "epoch": 0.8127357655486103,
      "grad_norm": 1.4187835454940796,
      "learning_rate": 3.6453112515435464e-05,
      "loss": 2.5827,
      "step": 7460
    },
    {
      "epoch": 0.8138252236793725,
      "grad_norm": 1.3254941701889038,
      "learning_rate": 3.643495314883417e-05,
      "loss": 2.5275,
      "step": 7470
    },
    {
      "epoch": 0.8149146818101347,
      "grad_norm": 1.2933403253555298,
      "learning_rate": 3.6416793782232874e-05,
      "loss": 2.6079,
      "step": 7480
    },
    {
      "epoch": 0.8160041399408969,
      "grad_norm": 1.3959741592407227,
      "learning_rate": 3.639863441563158e-05,
      "loss": 2.545,
      "step": 7490
    },
    {
      "epoch": 0.8170935980716592,
      "grad_norm": 1.3079352378845215,
      "learning_rate": 3.638047504903029e-05,
      "loss": 2.556,
      "step": 7500
    },
    {
      "epoch": 0.8181830562024213,
      "grad_norm": 1.2994848489761353,
      "learning_rate": 3.6362315682429e-05,
      "loss": 2.6045,
      "step": 7510
    },
    {
      "epoch": 0.8192725143331835,
      "grad_norm": 1.341063141822815,
      "learning_rate": 3.63441563158277e-05,
      "loss": 2.4763,
      "step": 7520
    },
    {
      "epoch": 0.8203619724639457,
      "grad_norm": 1.2978545427322388,
      "learning_rate": 3.6325996949226415e-05,
      "loss": 2.5932,
      "step": 7530
    },
    {
      "epoch": 0.8214514305947079,
      "grad_norm": 1.3244386911392212,
      "learning_rate": 3.630783758262512e-05,
      "loss": 2.5952,
      "step": 7540
    },
    {
      "epoch": 0.8225408887254702,
      "grad_norm": 1.3208222389221191,
      "learning_rate": 3.6289678216023826e-05,
      "loss": 2.4393,
      "step": 7550
    },
    {
      "epoch": 0.8236303468562324,
      "grad_norm": 1.35552978515625,
      "learning_rate": 3.627151884942254e-05,
      "loss": 2.5305,
      "step": 7560
    },
    {
      "epoch": 0.8247198049869946,
      "grad_norm": 1.3207961320877075,
      "learning_rate": 3.6253359482821244e-05,
      "loss": 2.4323,
      "step": 7570
    },
    {
      "epoch": 0.8258092631177568,
      "grad_norm": 1.2813608646392822,
      "learning_rate": 3.623520011621995e-05,
      "loss": 2.569,
      "step": 7580
    },
    {
      "epoch": 0.826898721248519,
      "grad_norm": 1.4262521266937256,
      "learning_rate": 3.6217040749618654e-05,
      "loss": 2.4712,
      "step": 7590
    },
    {
      "epoch": 0.8279881793792813,
      "grad_norm": 1.3478070497512817,
      "learning_rate": 3.6198881383017367e-05,
      "loss": 2.6217,
      "step": 7600
    },
    {
      "epoch": 0.8290776375100435,
      "grad_norm": 1.3062940835952759,
      "learning_rate": 3.618072201641607e-05,
      "loss": 2.6813,
      "step": 7610
    },
    {
      "epoch": 0.8301670956408057,
      "grad_norm": 1.355141043663025,
      "learning_rate": 3.616256264981477e-05,
      "loss": 2.5958,
      "step": 7620
    },
    {
      "epoch": 0.8312565537715678,
      "grad_norm": 1.3781160116195679,
      "learning_rate": 3.614440328321348e-05,
      "loss": 2.5196,
      "step": 7630
    },
    {
      "epoch": 0.83234601190233,
      "grad_norm": 1.307904601097107,
      "learning_rate": 3.612624391661219e-05,
      "loss": 2.6498,
      "step": 7640
    },
    {
      "epoch": 0.8334354700330923,
      "grad_norm": 1.4347566366195679,
      "learning_rate": 3.6108084550010894e-05,
      "loss": 2.5932,
      "step": 7650
    },
    {
      "epoch": 0.8345249281638545,
      "grad_norm": 1.4210503101348877,
      "learning_rate": 3.6089925183409606e-05,
      "loss": 2.5228,
      "step": 7660
    },
    {
      "epoch": 0.8356143862946167,
      "grad_norm": 1.2649550437927246,
      "learning_rate": 3.607176581680831e-05,
      "loss": 2.5567,
      "step": 7670
    },
    {
      "epoch": 0.8367038444253789,
      "grad_norm": 1.3309372663497925,
      "learning_rate": 3.605360645020702e-05,
      "loss": 2.5576,
      "step": 7680
    },
    {
      "epoch": 0.8377933025561412,
      "grad_norm": 1.2810239791870117,
      "learning_rate": 3.603544708360573e-05,
      "loss": 2.6231,
      "step": 7690
    },
    {
      "epoch": 0.8388827606869034,
      "grad_norm": 1.2917453050613403,
      "learning_rate": 3.6017287717004434e-05,
      "loss": 2.484,
      "step": 7700
    },
    {
      "epoch": 0.8399722188176656,
      "grad_norm": 1.338913083076477,
      "learning_rate": 3.599912835040314e-05,
      "loss": 2.6135,
      "step": 7710
    },
    {
      "epoch": 0.8410616769484278,
      "grad_norm": 1.3657678365707397,
      "learning_rate": 3.598096898380185e-05,
      "loss": 2.595,
      "step": 7720
    },
    {
      "epoch": 0.84215113507919,
      "grad_norm": 1.2627512216567993,
      "learning_rate": 3.596280961720056e-05,
      "loss": 2.5043,
      "step": 7730
    },
    {
      "epoch": 0.8432405932099521,
      "grad_norm": 1.3127557039260864,
      "learning_rate": 3.594465025059926e-05,
      "loss": 2.4913,
      "step": 7740
    },
    {
      "epoch": 0.8443300513407144,
      "grad_norm": 1.3660616874694824,
      "learning_rate": 3.592649088399797e-05,
      "loss": 2.57,
      "step": 7750
    },
    {
      "epoch": 0.8454195094714766,
      "grad_norm": 1.3117810487747192,
      "learning_rate": 3.5908331517396674e-05,
      "loss": 2.5311,
      "step": 7760
    },
    {
      "epoch": 0.8465089676022388,
      "grad_norm": 1.2578065395355225,
      "learning_rate": 3.589017215079538e-05,
      "loss": 2.5578,
      "step": 7770
    },
    {
      "epoch": 0.847598425733001,
      "grad_norm": 1.2295793294906616,
      "learning_rate": 3.5872012784194084e-05,
      "loss": 2.6015,
      "step": 7780
    },
    {
      "epoch": 0.8486878838637633,
      "grad_norm": 1.3737229108810425,
      "learning_rate": 3.58538534175928e-05,
      "loss": 2.4943,
      "step": 7790
    },
    {
      "epoch": 0.8497773419945255,
      "grad_norm": 1.2993789911270142,
      "learning_rate": 3.58356940509915e-05,
      "loss": 2.5265,
      "step": 7800
    },
    {
      "epoch": 0.8508668001252877,
      "grad_norm": 1.3167567253112793,
      "learning_rate": 3.581753468439021e-05,
      "loss": 2.6226,
      "step": 7810
    },
    {
      "epoch": 0.8519562582560499,
      "grad_norm": 1.256113886833191,
      "learning_rate": 3.579937531778892e-05,
      "loss": 2.6377,
      "step": 7820
    },
    {
      "epoch": 0.8530457163868121,
      "grad_norm": 1.3579521179199219,
      "learning_rate": 3.5781215951187625e-05,
      "loss": 2.4409,
      "step": 7830
    },
    {
      "epoch": 0.8541351745175744,
      "grad_norm": 1.3441632986068726,
      "learning_rate": 3.576305658458633e-05,
      "loss": 2.6111,
      "step": 7840
    },
    {
      "epoch": 0.8552246326483365,
      "grad_norm": 1.3373693227767944,
      "learning_rate": 3.574489721798504e-05,
      "loss": 2.5034,
      "step": 7850
    },
    {
      "epoch": 0.8563140907790987,
      "grad_norm": 1.3072519302368164,
      "learning_rate": 3.572673785138375e-05,
      "loss": 2.6714,
      "step": 7860
    },
    {
      "epoch": 0.8574035489098609,
      "grad_norm": 1.2601258754730225,
      "learning_rate": 3.5708578484782454e-05,
      "loss": 2.5177,
      "step": 7870
    },
    {
      "epoch": 0.8584930070406231,
      "grad_norm": 1.393092155456543,
      "learning_rate": 3.569041911818116e-05,
      "loss": 2.5948,
      "step": 7880
    },
    {
      "epoch": 0.8595824651713854,
      "grad_norm": 1.2627211809158325,
      "learning_rate": 3.5672259751579864e-05,
      "loss": 2.6055,
      "step": 7890
    },
    {
      "epoch": 0.8606719233021476,
      "grad_norm": 1.2604230642318726,
      "learning_rate": 3.565410038497857e-05,
      "loss": 2.5038,
      "step": 7900
    },
    {
      "epoch": 0.8617613814329098,
      "grad_norm": 1.3985546827316284,
      "learning_rate": 3.5635941018377275e-05,
      "loss": 2.4654,
      "step": 7910
    },
    {
      "epoch": 0.862850839563672,
      "grad_norm": 1.3681679964065552,
      "learning_rate": 3.561778165177599e-05,
      "loss": 2.5516,
      "step": 7920
    },
    {
      "epoch": 0.8639402976944343,
      "grad_norm": 1.3112961053848267,
      "learning_rate": 3.559962228517469e-05,
      "loss": 2.5787,
      "step": 7930
    },
    {
      "epoch": 0.8650297558251965,
      "grad_norm": 1.2861701250076294,
      "learning_rate": 3.55814629185734e-05,
      "loss": 2.5039,
      "step": 7940
    },
    {
      "epoch": 0.8661192139559587,
      "grad_norm": 1.3440614938735962,
      "learning_rate": 3.556330355197211e-05,
      "loss": 2.503,
      "step": 7950
    },
    {
      "epoch": 0.8672086720867209,
      "grad_norm": 1.2357797622680664,
      "learning_rate": 3.5545144185370816e-05,
      "loss": 2.5329,
      "step": 7960
    },
    {
      "epoch": 0.868298130217483,
      "grad_norm": 1.3228071928024292,
      "learning_rate": 3.552698481876952e-05,
      "loss": 2.6255,
      "step": 7970
    },
    {
      "epoch": 0.8693875883482453,
      "grad_norm": 1.3155401945114136,
      "learning_rate": 3.5508825452168233e-05,
      "loss": 2.5307,
      "step": 7980
    },
    {
      "epoch": 0.8704770464790075,
      "grad_norm": 1.2987465858459473,
      "learning_rate": 3.549066608556694e-05,
      "loss": 2.5039,
      "step": 7990
    },
    {
      "epoch": 0.8715665046097697,
      "grad_norm": 1.2899340391159058,
      "learning_rate": 3.5472506718965644e-05,
      "loss": 2.5698,
      "step": 8000
    },
    {
      "epoch": 0.8726559627405319,
      "grad_norm": 1.3496758937835693,
      "learning_rate": 3.5454347352364356e-05,
      "loss": 2.5276,
      "step": 8010
    },
    {
      "epoch": 0.8737454208712941,
      "grad_norm": 1.4113894701004028,
      "learning_rate": 3.543618798576306e-05,
      "loss": 2.6141,
      "step": 8020
    },
    {
      "epoch": 0.8748348790020564,
      "grad_norm": 1.3573737144470215,
      "learning_rate": 3.541802861916176e-05,
      "loss": 2.4964,
      "step": 8030
    },
    {
      "epoch": 0.8759243371328186,
      "grad_norm": 1.495186448097229,
      "learning_rate": 3.539986925256047e-05,
      "loss": 2.5591,
      "step": 8040
    },
    {
      "epoch": 0.8770137952635808,
      "grad_norm": 1.3223897218704224,
      "learning_rate": 3.538170988595918e-05,
      "loss": 2.4678,
      "step": 8050
    },
    {
      "epoch": 0.878103253394343,
      "grad_norm": 1.4317214488983154,
      "learning_rate": 3.5363550519357884e-05,
      "loss": 2.5252,
      "step": 8060
    },
    {
      "epoch": 0.8791927115251053,
      "grad_norm": 1.3044910430908203,
      "learning_rate": 3.5345391152756596e-05,
      "loss": 2.569,
      "step": 8070
    },
    {
      "epoch": 0.8802821696558674,
      "grad_norm": 1.3747597932815552,
      "learning_rate": 3.53272317861553e-05,
      "loss": 2.5469,
      "step": 8080
    },
    {
      "epoch": 0.8813716277866296,
      "grad_norm": 1.3669229745864868,
      "learning_rate": 3.5309072419554007e-05,
      "loss": 2.4972,
      "step": 8090
    },
    {
      "epoch": 0.8824610859173918,
      "grad_norm": 1.3588945865631104,
      "learning_rate": 3.529091305295271e-05,
      "loss": 2.508,
      "step": 8100
    },
    {
      "epoch": 0.883550544048154,
      "grad_norm": 1.4184778928756714,
      "learning_rate": 3.5272753686351424e-05,
      "loss": 2.5799,
      "step": 8110
    },
    {
      "epoch": 0.8846400021789163,
      "grad_norm": 1.3195281028747559,
      "learning_rate": 3.525459431975013e-05,
      "loss": 2.5897,
      "step": 8120
    },
    {
      "epoch": 0.8857294603096785,
      "grad_norm": 1.3430262804031372,
      "learning_rate": 3.5236434953148835e-05,
      "loss": 2.5552,
      "step": 8130
    },
    {
      "epoch": 0.8868189184404407,
      "grad_norm": 1.35919189453125,
      "learning_rate": 3.521827558654755e-05,
      "loss": 2.4947,
      "step": 8140
    },
    {
      "epoch": 0.8879083765712029,
      "grad_norm": 1.3246891498565674,
      "learning_rate": 3.520011621994625e-05,
      "loss": 2.5326,
      "step": 8150
    },
    {
      "epoch": 0.8889978347019651,
      "grad_norm": 1.2551270723342896,
      "learning_rate": 3.518195685334496e-05,
      "loss": 2.5289,
      "step": 8160
    },
    {
      "epoch": 0.8900872928327274,
      "grad_norm": 1.4557690620422363,
      "learning_rate": 3.5163797486743663e-05,
      "loss": 2.5652,
      "step": 8170
    },
    {
      "epoch": 0.8911767509634896,
      "grad_norm": 1.308652639389038,
      "learning_rate": 3.514563812014237e-05,
      "loss": 2.4721,
      "step": 8180
    },
    {
      "epoch": 0.8922662090942517,
      "grad_norm": 1.343991994857788,
      "learning_rate": 3.5127478753541074e-05,
      "loss": 2.5776,
      "step": 8190
    },
    {
      "epoch": 0.8933556672250139,
      "grad_norm": 1.3106944561004639,
      "learning_rate": 3.5109319386939787e-05,
      "loss": 2.6113,
      "step": 8200
    },
    {
      "epoch": 0.8944451253557761,
      "grad_norm": 1.3217663764953613,
      "learning_rate": 3.509116002033849e-05,
      "loss": 2.5902,
      "step": 8210
    },
    {
      "epoch": 0.8955345834865384,
      "grad_norm": 1.352128505706787,
      "learning_rate": 3.50730006537372e-05,
      "loss": 2.6112,
      "step": 8220
    },
    {
      "epoch": 0.8966240416173006,
      "grad_norm": 1.3373228311538696,
      "learning_rate": 3.505484128713591e-05,
      "loss": 2.5409,
      "step": 8230
    },
    {
      "epoch": 0.8977134997480628,
      "grad_norm": 1.332205891609192,
      "learning_rate": 3.5036681920534615e-05,
      "loss": 2.4654,
      "step": 8240
    },
    {
      "epoch": 0.898802957878825,
      "grad_norm": 1.3145478963851929,
      "learning_rate": 3.501852255393332e-05,
      "loss": 2.5494,
      "step": 8250
    },
    {
      "epoch": 0.8998924160095872,
      "grad_norm": 1.3662018775939941,
      "learning_rate": 3.5000363187332026e-05,
      "loss": 2.6111,
      "step": 8260
    },
    {
      "epoch": 0.9009818741403495,
      "grad_norm": 1.326093316078186,
      "learning_rate": 3.498220382073074e-05,
      "loss": 2.5183,
      "step": 8270
    },
    {
      "epoch": 0.9020713322711117,
      "grad_norm": 1.362795114517212,
      "learning_rate": 3.4964044454129443e-05,
      "loss": 2.54,
      "step": 8280
    },
    {
      "epoch": 0.9031607904018739,
      "grad_norm": 1.2786494493484497,
      "learning_rate": 3.494588508752815e-05,
      "loss": 2.539,
      "step": 8290
    },
    {
      "epoch": 0.904250248532636,
      "grad_norm": 1.3661243915557861,
      "learning_rate": 3.4927725720926854e-05,
      "loss": 2.6097,
      "step": 8300
    },
    {
      "epoch": 0.9053397066633982,
      "grad_norm": 1.2488378286361694,
      "learning_rate": 3.490956635432556e-05,
      "loss": 2.5822,
      "step": 8310
    },
    {
      "epoch": 0.9064291647941605,
      "grad_norm": 1.373623013496399,
      "learning_rate": 3.4891406987724265e-05,
      "loss": 2.5144,
      "step": 8320
    },
    {
      "epoch": 0.9075186229249227,
      "grad_norm": 1.3111279010772705,
      "learning_rate": 3.487324762112298e-05,
      "loss": 2.5735,
      "step": 8330
    },
    {
      "epoch": 0.9086080810556849,
      "grad_norm": 1.3357256650924683,
      "learning_rate": 3.485508825452168e-05,
      "loss": 2.5361,
      "step": 8340
    },
    {
      "epoch": 0.9096975391864471,
      "grad_norm": 1.3254865407943726,
      "learning_rate": 3.483692888792039e-05,
      "loss": 2.5238,
      "step": 8350
    },
    {
      "epoch": 0.9107869973172094,
      "grad_norm": 1.3347721099853516,
      "learning_rate": 3.48187695213191e-05,
      "loss": 2.4483,
      "step": 8360
    },
    {
      "epoch": 0.9118764554479716,
      "grad_norm": 1.3389830589294434,
      "learning_rate": 3.4800610154717806e-05,
      "loss": 2.4962,
      "step": 8370
    },
    {
      "epoch": 0.9129659135787338,
      "grad_norm": 1.3300445079803467,
      "learning_rate": 3.478245078811651e-05,
      "loss": 2.5142,
      "step": 8380
    },
    {
      "epoch": 0.914055371709496,
      "grad_norm": 1.427504539489746,
      "learning_rate": 3.476429142151522e-05,
      "loss": 2.5542,
      "step": 8390
    },
    {
      "epoch": 0.9151448298402582,
      "grad_norm": 1.3179478645324707,
      "learning_rate": 3.474613205491393e-05,
      "loss": 2.501,
      "step": 8400
    },
    {
      "epoch": 0.9162342879710205,
      "grad_norm": 1.407570481300354,
      "learning_rate": 3.4727972688312634e-05,
      "loss": 2.4978,
      "step": 8410
    },
    {
      "epoch": 0.9173237461017826,
      "grad_norm": 1.3439488410949707,
      "learning_rate": 3.4709813321711346e-05,
      "loss": 2.5453,
      "step": 8420
    },
    {
      "epoch": 0.9184132042325448,
      "grad_norm": 1.345639705657959,
      "learning_rate": 3.4691653955110045e-05,
      "loss": 2.5119,
      "step": 8430
    },
    {
      "epoch": 0.919502662363307,
      "grad_norm": 1.4087927341461182,
      "learning_rate": 3.467349458850875e-05,
      "loss": 2.5784,
      "step": 8440
    },
    {
      "epoch": 0.9205921204940692,
      "grad_norm": 1.334237813949585,
      "learning_rate": 3.4655335221907456e-05,
      "loss": 2.5202,
      "step": 8450
    },
    {
      "epoch": 0.9216815786248315,
      "grad_norm": 1.3709940910339355,
      "learning_rate": 3.463717585530617e-05,
      "loss": 2.528,
      "step": 8460
    },
    {
      "epoch": 0.9227710367555937,
      "grad_norm": 1.4035178422927856,
      "learning_rate": 3.4619016488704873e-05,
      "loss": 2.6192,
      "step": 8470
    },
    {
      "epoch": 0.9238604948863559,
      "grad_norm": 1.3445112705230713,
      "learning_rate": 3.460085712210358e-05,
      "loss": 2.5683,
      "step": 8480
    },
    {
      "epoch": 0.9249499530171181,
      "grad_norm": 1.3280805349349976,
      "learning_rate": 3.458269775550229e-05,
      "loss": 2.4849,
      "step": 8490
    },
    {
      "epoch": 0.9260394111478804,
      "grad_norm": 1.3330713510513306,
      "learning_rate": 3.4564538388900996e-05,
      "loss": 2.4704,
      "step": 8500
    },
    {
      "epoch": 0.9271288692786426,
      "grad_norm": 1.471934199333191,
      "learning_rate": 3.45463790222997e-05,
      "loss": 2.5787,
      "step": 8510
    },
    {
      "epoch": 0.9282183274094048,
      "grad_norm": 1.3054531812667847,
      "learning_rate": 3.4528219655698414e-05,
      "loss": 2.4733,
      "step": 8520
    },
    {
      "epoch": 0.9293077855401669,
      "grad_norm": 1.36215341091156,
      "learning_rate": 3.451006028909712e-05,
      "loss": 2.537,
      "step": 8530
    },
    {
      "epoch": 0.9303972436709291,
      "grad_norm": 1.358449101448059,
      "learning_rate": 3.4491900922495825e-05,
      "loss": 2.5628,
      "step": 8540
    },
    {
      "epoch": 0.9314867018016914,
      "grad_norm": 1.3124996423721313,
      "learning_rate": 3.447374155589454e-05,
      "loss": 2.572,
      "step": 8550
    },
    {
      "epoch": 0.9325761599324536,
      "grad_norm": 1.3357727527618408,
      "learning_rate": 3.445558218929324e-05,
      "loss": 2.5368,
      "step": 8560
    },
    {
      "epoch": 0.9336656180632158,
      "grad_norm": 1.295047402381897,
      "learning_rate": 3.443742282269195e-05,
      "loss": 2.5143,
      "step": 8570
    },
    {
      "epoch": 0.934755076193978,
      "grad_norm": 1.330744743347168,
      "learning_rate": 3.441926345609065e-05,
      "loss": 2.5725,
      "step": 8580
    },
    {
      "epoch": 0.9358445343247402,
      "grad_norm": 1.2862385511398315,
      "learning_rate": 3.440110408948936e-05,
      "loss": 2.4902,
      "step": 8590
    },
    {
      "epoch": 0.9369339924555025,
      "grad_norm": 1.3028849363327026,
      "learning_rate": 3.4382944722888064e-05,
      "loss": 2.5098,
      "step": 8600
    },
    {
      "epoch": 0.9380234505862647,
      "grad_norm": 1.2791831493377686,
      "learning_rate": 3.436478535628677e-05,
      "loss": 2.6849,
      "step": 8610
    },
    {
      "epoch": 0.9391129087170269,
      "grad_norm": 1.3514878749847412,
      "learning_rate": 3.434662598968548e-05,
      "loss": 2.4822,
      "step": 8620
    },
    {
      "epoch": 0.9402023668477891,
      "grad_norm": 1.3791040182113647,
      "learning_rate": 3.432846662308419e-05,
      "loss": 2.5742,
      "step": 8630
    },
    {
      "epoch": 0.9412918249785512,
      "grad_norm": 1.3389652967453003,
      "learning_rate": 3.431030725648289e-05,
      "loss": 2.5989,
      "step": 8640
    },
    {
      "epoch": 0.9423812831093135,
      "grad_norm": 1.290557622909546,
      "learning_rate": 3.4292147889881605e-05,
      "loss": 2.5111,
      "step": 8650
    },
    {
      "epoch": 0.9434707412400757,
      "grad_norm": 1.3835633993148804,
      "learning_rate": 3.427398852328031e-05,
      "loss": 2.4603,
      "step": 8660
    },
    {
      "epoch": 0.9445601993708379,
      "grad_norm": 1.4122869968414307,
      "learning_rate": 3.4255829156679016e-05,
      "loss": 2.4814,
      "step": 8670
    },
    {
      "epoch": 0.9456496575016001,
      "grad_norm": 1.270751953125,
      "learning_rate": 3.423766979007773e-05,
      "loss": 2.5626,
      "step": 8680
    },
    {
      "epoch": 0.9467391156323623,
      "grad_norm": 1.2924162149429321,
      "learning_rate": 3.421951042347643e-05,
      "loss": 2.5529,
      "step": 8690
    },
    {
      "epoch": 0.9478285737631246,
      "grad_norm": 1.3136699199676514,
      "learning_rate": 3.420135105687514e-05,
      "loss": 2.5637,
      "step": 8700
    },
    {
      "epoch": 0.9489180318938868,
      "grad_norm": 1.2586321830749512,
      "learning_rate": 3.4183191690273844e-05,
      "loss": 2.5759,
      "step": 8710
    },
    {
      "epoch": 0.950007490024649,
      "grad_norm": 1.472129225730896,
      "learning_rate": 3.416503232367255e-05,
      "loss": 2.583,
      "step": 8720
    },
    {
      "epoch": 0.9510969481554112,
      "grad_norm": 1.2491248846054077,
      "learning_rate": 3.4146872957071255e-05,
      "loss": 2.4928,
      "step": 8730
    },
    {
      "epoch": 0.9521864062861735,
      "grad_norm": 1.3718327283859253,
      "learning_rate": 3.412871359046997e-05,
      "loss": 2.5574,
      "step": 8740
    },
    {
      "epoch": 0.9532758644169357,
      "grad_norm": 1.36324942111969,
      "learning_rate": 3.411055422386867e-05,
      "loss": 2.6164,
      "step": 8750
    },
    {
      "epoch": 0.9543653225476978,
      "grad_norm": 1.2659369707107544,
      "learning_rate": 3.409239485726738e-05,
      "loss": 2.5277,
      "step": 8760
    },
    {
      "epoch": 0.95545478067846,
      "grad_norm": 1.324113368988037,
      "learning_rate": 3.407423549066609e-05,
      "loss": 2.5777,
      "step": 8770
    },
    {
      "epoch": 0.9565442388092222,
      "grad_norm": 1.3589776754379272,
      "learning_rate": 3.4056076124064796e-05,
      "loss": 2.5633,
      "step": 8780
    },
    {
      "epoch": 0.9576336969399845,
      "grad_norm": 1.314211368560791,
      "learning_rate": 3.40379167574635e-05,
      "loss": 2.5255,
      "step": 8790
    },
    {
      "epoch": 0.9587231550707467,
      "grad_norm": 1.2763786315917969,
      "learning_rate": 3.4019757390862206e-05,
      "loss": 2.4525,
      "step": 8800
    },
    {
      "epoch": 0.9598126132015089,
      "grad_norm": 1.3702757358551025,
      "learning_rate": 3.400159802426092e-05,
      "loss": 2.569,
      "step": 8810
    },
    {
      "epoch": 0.9609020713322711,
      "grad_norm": 1.349073886871338,
      "learning_rate": 3.3983438657659624e-05,
      "loss": 2.4652,
      "step": 8820
    },
    {
      "epoch": 0.9619915294630333,
      "grad_norm": 1.3293581008911133,
      "learning_rate": 3.396527929105833e-05,
      "loss": 2.5212,
      "step": 8830
    },
    {
      "epoch": 0.9630809875937956,
      "grad_norm": 1.3905961513519287,
      "learning_rate": 3.3947119924457035e-05,
      "loss": 2.5008,
      "step": 8840
    },
    {
      "epoch": 0.9641704457245578,
      "grad_norm": 1.3281079530715942,
      "learning_rate": 3.392896055785574e-05,
      "loss": 2.5647,
      "step": 8850
    },
    {
      "epoch": 0.96525990385532,
      "grad_norm": 1.2584996223449707,
      "learning_rate": 3.3910801191254446e-05,
      "loss": 2.4877,
      "step": 8860
    },
    {
      "epoch": 0.9663493619860821,
      "grad_norm": 1.3758931159973145,
      "learning_rate": 3.389264182465316e-05,
      "loss": 2.5013,
      "step": 8870
    },
    {
      "epoch": 0.9674388201168443,
      "grad_norm": 1.3401172161102295,
      "learning_rate": 3.387448245805186e-05,
      "loss": 2.6284,
      "step": 8880
    },
    {
      "epoch": 0.9685282782476066,
      "grad_norm": 1.2418091297149658,
      "learning_rate": 3.385632309145057e-05,
      "loss": 2.4594,
      "step": 8890
    },
    {
      "epoch": 0.9696177363783688,
      "grad_norm": 1.3662282228469849,
      "learning_rate": 3.383816372484928e-05,
      "loss": 2.6358,
      "step": 8900
    },
    {
      "epoch": 0.970707194509131,
      "grad_norm": 1.344273567199707,
      "learning_rate": 3.3820004358247986e-05,
      "loss": 2.5818,
      "step": 8910
    },
    {
      "epoch": 0.9717966526398932,
      "grad_norm": 1.3776558637619019,
      "learning_rate": 3.380184499164669e-05,
      "loss": 2.565,
      "step": 8920
    },
    {
      "epoch": 0.9728861107706555,
      "grad_norm": 1.2783881425857544,
      "learning_rate": 3.3783685625045404e-05,
      "loss": 2.521,
      "step": 8930
    },
    {
      "epoch": 0.9739755689014177,
      "grad_norm": 1.284926176071167,
      "learning_rate": 3.376552625844411e-05,
      "loss": 2.5048,
      "step": 8940
    },
    {
      "epoch": 0.9750650270321799,
      "grad_norm": 1.3371087312698364,
      "learning_rate": 3.3747366891842815e-05,
      "loss": 2.5337,
      "step": 8950
    },
    {
      "epoch": 0.9761544851629421,
      "grad_norm": 1.352876901626587,
      "learning_rate": 3.372920752524152e-05,
      "loss": 2.5314,
      "step": 8960
    },
    {
      "epoch": 0.9772439432937043,
      "grad_norm": 1.5057188272476196,
      "learning_rate": 3.371104815864023e-05,
      "loss": 2.6617,
      "step": 8970
    },
    {
      "epoch": 0.9783334014244665,
      "grad_norm": 1.3314374685287476,
      "learning_rate": 3.369288879203894e-05,
      "loss": 2.5433,
      "step": 8980
    },
    {
      "epoch": 0.9794228595552287,
      "grad_norm": 1.3170642852783203,
      "learning_rate": 3.367472942543764e-05,
      "loss": 2.5819,
      "step": 8990
    },
    {
      "epoch": 0.9805123176859909,
      "grad_norm": 1.3180088996887207,
      "learning_rate": 3.365657005883635e-05,
      "loss": 2.5615,
      "step": 9000
    },
    {
      "epoch": 0.9816017758167531,
      "grad_norm": 1.3691085577011108,
      "learning_rate": 3.3638410692235054e-05,
      "loss": 2.5365,
      "step": 9010
    },
    {
      "epoch": 0.9826912339475153,
      "grad_norm": 1.312119483947754,
      "learning_rate": 3.362025132563376e-05,
      "loss": 2.4425,
      "step": 9020
    },
    {
      "epoch": 0.9837806920782776,
      "grad_norm": 1.3270549774169922,
      "learning_rate": 3.360209195903247e-05,
      "loss": 2.4818,
      "step": 9030
    },
    {
      "epoch": 0.9848701502090398,
      "grad_norm": 1.3627318143844604,
      "learning_rate": 3.358393259243118e-05,
      "loss": 2.4909,
      "step": 9040
    },
    {
      "epoch": 0.985959608339802,
      "grad_norm": 1.3543661832809448,
      "learning_rate": 3.356577322582988e-05,
      "loss": 2.592,
      "step": 9050
    },
    {
      "epoch": 0.9870490664705642,
      "grad_norm": 1.40944242477417,
      "learning_rate": 3.3547613859228595e-05,
      "loss": 2.5876,
      "step": 9060
    },
    {
      "epoch": 0.9881385246013265,
      "grad_norm": 1.2712464332580566,
      "learning_rate": 3.35294544926273e-05,
      "loss": 2.4111,
      "step": 9070
    },
    {
      "epoch": 0.9892279827320887,
      "grad_norm": 1.2828580141067505,
      "learning_rate": 3.3511295126026006e-05,
      "loss": 2.4857,
      "step": 9080
    },
    {
      "epoch": 0.9903174408628508,
      "grad_norm": 1.4267432689666748,
      "learning_rate": 3.349313575942472e-05,
      "loss": 2.4771,
      "step": 9090
    },
    {
      "epoch": 0.991406898993613,
      "grad_norm": 1.4251790046691895,
      "learning_rate": 3.347497639282342e-05,
      "loss": 2.4738,
      "step": 9100
    },
    {
      "epoch": 0.9924963571243752,
      "grad_norm": 1.369043231010437,
      "learning_rate": 3.345681702622213e-05,
      "loss": 2.578,
      "step": 9110
    },
    {
      "epoch": 0.9935858152551375,
      "grad_norm": 1.354128360748291,
      "learning_rate": 3.3438657659620834e-05,
      "loss": 2.5199,
      "step": 9120
    },
    {
      "epoch": 0.9946752733858997,
      "grad_norm": 1.242071270942688,
      "learning_rate": 3.342049829301954e-05,
      "loss": 2.5674,
      "step": 9130
    },
    {
      "epoch": 0.9957647315166619,
      "grad_norm": 1.3741544485092163,
      "learning_rate": 3.3402338926418245e-05,
      "loss": 2.5685,
      "step": 9140
    },
    {
      "epoch": 0.9968541896474241,
      "grad_norm": 1.4188231229782104,
      "learning_rate": 3.338417955981695e-05,
      "loss": 2.453,
      "step": 9150
    },
    {
      "epoch": 0.9979436477781863,
      "grad_norm": 1.2886159420013428,
      "learning_rate": 3.336602019321566e-05,
      "loss": 2.4334,
      "step": 9160
    },
    {
      "epoch": 0.9990331059089486,
      "grad_norm": 1.3017574548721313,
      "learning_rate": 3.334786082661437e-05,
      "loss": 2.465,
      "step": 9170
    },
    {
      "epoch": 1.0001225640397107,
      "grad_norm": 1.3453446626663208,
      "learning_rate": 3.332970146001307e-05,
      "loss": 2.5215,
      "step": 9180
    },
    {
      "epoch": 1.001212022170473,
      "grad_norm": 1.4028282165527344,
      "learning_rate": 3.3311542093411785e-05,
      "loss": 2.4235,
      "step": 9190
    },
    {
      "epoch": 1.0023014803012351,
      "grad_norm": 1.401831030845642,
      "learning_rate": 3.329338272681049e-05,
      "loss": 2.4413,
      "step": 9200
    },
    {
      "epoch": 1.0033909384319974,
      "grad_norm": 1.2965372800827026,
      "learning_rate": 3.3275223360209196e-05,
      "loss": 2.5466,
      "step": 9210
    },
    {
      "epoch": 1.0044803965627596,
      "grad_norm": 1.2893774509429932,
      "learning_rate": 3.325706399360791e-05,
      "loss": 2.4543,
      "step": 9220
    },
    {
      "epoch": 1.005569854693522,
      "grad_norm": 1.3687303066253662,
      "learning_rate": 3.3238904627006614e-05,
      "loss": 2.4736,
      "step": 9230
    },
    {
      "epoch": 1.006659312824284,
      "grad_norm": 1.3330765962600708,
      "learning_rate": 3.322074526040532e-05,
      "loss": 2.4415,
      "step": 9240
    },
    {
      "epoch": 1.0077487709550463,
      "grad_norm": 1.2920315265655518,
      "learning_rate": 3.3202585893804025e-05,
      "loss": 2.519,
      "step": 9250
    },
    {
      "epoch": 1.0088382290858084,
      "grad_norm": 1.3469589948654175,
      "learning_rate": 3.318442652720273e-05,
      "loss": 2.4438,
      "step": 9260
    },
    {
      "epoch": 1.0099276872165706,
      "grad_norm": 1.3193774223327637,
      "learning_rate": 3.3166267160601436e-05,
      "loss": 2.4482,
      "step": 9270
    },
    {
      "epoch": 1.011017145347333,
      "grad_norm": 1.4172773361206055,
      "learning_rate": 3.314810779400015e-05,
      "loss": 2.5699,
      "step": 9280
    },
    {
      "epoch": 1.012106603478095,
      "grad_norm": 1.3748819828033447,
      "learning_rate": 3.312994842739885e-05,
      "loss": 2.4625,
      "step": 9290
    },
    {
      "epoch": 1.0131960616088573,
      "grad_norm": 1.2380719184875488,
      "learning_rate": 3.311178906079756e-05,
      "loss": 2.4092,
      "step": 9300
    },
    {
      "epoch": 1.0142855197396194,
      "grad_norm": 1.3178201913833618,
      "learning_rate": 3.3093629694196264e-05,
      "loss": 2.4901,
      "step": 9310
    },
    {
      "epoch": 1.0153749778703818,
      "grad_norm": 1.287377953529358,
      "learning_rate": 3.3075470327594976e-05,
      "loss": 2.4937,
      "step": 9320
    },
    {
      "epoch": 1.016464436001144,
      "grad_norm": 1.3768625259399414,
      "learning_rate": 3.305731096099368e-05,
      "loss": 2.5934,
      "step": 9330
    },
    {
      "epoch": 1.0175538941319062,
      "grad_norm": 1.3585376739501953,
      "learning_rate": 3.303915159439239e-05,
      "loss": 2.5501,
      "step": 9340
    },
    {
      "epoch": 1.0186433522626683,
      "grad_norm": 1.4639081954956055,
      "learning_rate": 3.30209922277911e-05,
      "loss": 2.4545,
      "step": 9350
    },
    {
      "epoch": 1.0197328103934307,
      "grad_norm": 1.4369927644729614,
      "learning_rate": 3.3002832861189805e-05,
      "loss": 2.5434,
      "step": 9360
    },
    {
      "epoch": 1.0208222685241928,
      "grad_norm": 1.320633053779602,
      "learning_rate": 3.298467349458851e-05,
      "loss": 2.4692,
      "step": 9370
    },
    {
      "epoch": 1.0219117266549549,
      "grad_norm": 1.3336535692214966,
      "learning_rate": 3.296651412798722e-05,
      "loss": 2.6799,
      "step": 9380
    },
    {
      "epoch": 1.0230011847857172,
      "grad_norm": 1.2940706014633179,
      "learning_rate": 3.294835476138593e-05,
      "loss": 2.46,
      "step": 9390
    },
    {
      "epoch": 1.0240906429164793,
      "grad_norm": 1.2860074043273926,
      "learning_rate": 3.293019539478463e-05,
      "loss": 2.4874,
      "step": 9400
    },
    {
      "epoch": 1.0251801010472417,
      "grad_norm": 1.3859734535217285,
      "learning_rate": 3.291203602818334e-05,
      "loss": 2.4599,
      "step": 9410
    },
    {
      "epoch": 1.0262695591780038,
      "grad_norm": 1.380854606628418,
      "learning_rate": 3.2893876661582044e-05,
      "loss": 2.6364,
      "step": 9420
    },
    {
      "epoch": 1.027359017308766,
      "grad_norm": 1.3976329565048218,
      "learning_rate": 3.287571729498075e-05,
      "loss": 2.5841,
      "step": 9430
    },
    {
      "epoch": 1.0284484754395282,
      "grad_norm": 1.3722872734069824,
      "learning_rate": 3.285755792837946e-05,
      "loss": 2.5824,
      "step": 9440
    },
    {
      "epoch": 1.0295379335702906,
      "grad_norm": 1.3993349075317383,
      "learning_rate": 3.283939856177817e-05,
      "loss": 2.4946,
      "step": 9450
    },
    {
      "epoch": 1.0306273917010527,
      "grad_norm": 1.3710285425186157,
      "learning_rate": 3.282123919517687e-05,
      "loss": 2.5729,
      "step": 9460
    },
    {
      "epoch": 1.031716849831815,
      "grad_norm": 1.314058780670166,
      "learning_rate": 3.2803079828575585e-05,
      "loss": 2.4858,
      "step": 9470
    },
    {
      "epoch": 1.032806307962577,
      "grad_norm": 1.3710973262786865,
      "learning_rate": 3.278492046197429e-05,
      "loss": 2.4826,
      "step": 9480
    },
    {
      "epoch": 1.0338957660933392,
      "grad_norm": 1.341903567314148,
      "learning_rate": 3.2766761095372995e-05,
      "loss": 2.4022,
      "step": 9490
    },
    {
      "epoch": 1.0349852242241016,
      "grad_norm": 1.3702458143234253,
      "learning_rate": 3.27486017287717e-05,
      "loss": 2.5757,
      "step": 9500
    },
    {
      "epoch": 1.0360746823548637,
      "grad_norm": 1.4838576316833496,
      "learning_rate": 3.273044236217041e-05,
      "loss": 2.603,
      "step": 9510
    },
    {
      "epoch": 1.037164140485626,
      "grad_norm": 1.3907400369644165,
      "learning_rate": 3.271228299556912e-05,
      "loss": 2.5391,
      "step": 9520
    },
    {
      "epoch": 1.038253598616388,
      "grad_norm": 1.3631808757781982,
      "learning_rate": 3.2694123628967824e-05,
      "loss": 2.413,
      "step": 9530
    },
    {
      "epoch": 1.0393430567471504,
      "grad_norm": 1.4203822612762451,
      "learning_rate": 3.267596426236653e-05,
      "loss": 2.5458,
      "step": 9540
    },
    {
      "epoch": 1.0404325148779126,
      "grad_norm": 1.3299020528793335,
      "learning_rate": 3.2657804895765235e-05,
      "loss": 2.5629,
      "step": 9550
    },
    {
      "epoch": 1.0415219730086749,
      "grad_norm": 1.406855583190918,
      "learning_rate": 3.263964552916394e-05,
      "loss": 2.4676,
      "step": 9560
    },
    {
      "epoch": 1.042611431139437,
      "grad_norm": 1.330406904220581,
      "learning_rate": 3.262148616256265e-05,
      "loss": 2.433,
      "step": 9570
    },
    {
      "epoch": 1.0437008892701993,
      "grad_norm": 1.3241218328475952,
      "learning_rate": 3.260332679596136e-05,
      "loss": 2.5221,
      "step": 9580
    },
    {
      "epoch": 1.0447903474009614,
      "grad_norm": 1.4053359031677246,
      "learning_rate": 3.258516742936006e-05,
      "loss": 2.5566,
      "step": 9590
    },
    {
      "epoch": 1.0458798055317238,
      "grad_norm": 1.310621738433838,
      "learning_rate": 3.2567008062758775e-05,
      "loss": 2.4965,
      "step": 9600
    },
    {
      "epoch": 1.0469692636624859,
      "grad_norm": 1.2337934970855713,
      "learning_rate": 3.254884869615748e-05,
      "loss": 2.4284,
      "step": 9610
    },
    {
      "epoch": 1.048058721793248,
      "grad_norm": 1.3064117431640625,
      "learning_rate": 3.2530689329556186e-05,
      "loss": 2.52,
      "step": 9620
    },
    {
      "epoch": 1.0491481799240103,
      "grad_norm": 1.313520908355713,
      "learning_rate": 3.25125299629549e-05,
      "loss": 2.5385,
      "step": 9630
    },
    {
      "epoch": 1.0502376380547724,
      "grad_norm": 1.3129874467849731,
      "learning_rate": 3.2494370596353604e-05,
      "loss": 2.4475,
      "step": 9640
    },
    {
      "epoch": 1.0513270961855348,
      "grad_norm": 1.3631916046142578,
      "learning_rate": 3.247621122975231e-05,
      "loss": 2.4939,
      "step": 9650
    },
    {
      "epoch": 1.0524165543162969,
      "grad_norm": 1.4043689966201782,
      "learning_rate": 3.2458051863151015e-05,
      "loss": 2.4825,
      "step": 9660
    },
    {
      "epoch": 1.0535060124470592,
      "grad_norm": 1.3830137252807617,
      "learning_rate": 3.243989249654972e-05,
      "loss": 2.4285,
      "step": 9670
    },
    {
      "epoch": 1.0545954705778213,
      "grad_norm": 1.3106573820114136,
      "learning_rate": 3.2421733129948425e-05,
      "loss": 2.4826,
      "step": 9680
    },
    {
      "epoch": 1.0556849287085837,
      "grad_norm": 1.3278648853302002,
      "learning_rate": 3.240357376334713e-05,
      "loss": 2.4994,
      "step": 9690
    },
    {
      "epoch": 1.0567743868393458,
      "grad_norm": 1.2827351093292236,
      "learning_rate": 3.238541439674584e-05,
      "loss": 2.4966,
      "step": 9700
    },
    {
      "epoch": 1.0578638449701079,
      "grad_norm": 1.347005009651184,
      "learning_rate": 3.236725503014455e-05,
      "loss": 2.4526,
      "step": 9710
    },
    {
      "epoch": 1.0589533031008702,
      "grad_norm": 1.3810304403305054,
      "learning_rate": 3.2349095663543254e-05,
      "loss": 2.5935,
      "step": 9720
    },
    {
      "epoch": 1.0600427612316323,
      "grad_norm": 1.4561764001846313,
      "learning_rate": 3.2330936296941966e-05,
      "loss": 2.5261,
      "step": 9730
    },
    {
      "epoch": 1.0611322193623947,
      "grad_norm": 1.4559029340744019,
      "learning_rate": 3.231277693034067e-05,
      "loss": 2.548,
      "step": 9740
    },
    {
      "epoch": 1.0622216774931568,
      "grad_norm": 1.3862091302871704,
      "learning_rate": 3.229461756373938e-05,
      "loss": 2.4765,
      "step": 9750
    },
    {
      "epoch": 1.063311135623919,
      "grad_norm": 1.2767205238342285,
      "learning_rate": 3.227645819713809e-05,
      "loss": 2.4971,
      "step": 9760
    },
    {
      "epoch": 1.0644005937546812,
      "grad_norm": 1.4164104461669922,
      "learning_rate": 3.2258298830536795e-05,
      "loss": 2.4487,
      "step": 9770
    },
    {
      "epoch": 1.0654900518854435,
      "grad_norm": 1.4085021018981934,
      "learning_rate": 3.22401394639355e-05,
      "loss": 2.5077,
      "step": 9780
    },
    {
      "epoch": 1.0665795100162057,
      "grad_norm": 1.3010510206222534,
      "learning_rate": 3.222198009733421e-05,
      "loss": 2.5096,
      "step": 9790
    },
    {
      "epoch": 1.067668968146968,
      "grad_norm": 1.399674654006958,
      "learning_rate": 3.220382073073292e-05,
      "loss": 2.4562,
      "step": 9800
    },
    {
      "epoch": 1.06875842627773,
      "grad_norm": 1.309088110923767,
      "learning_rate": 3.2185661364131616e-05,
      "loss": 2.4344,
      "step": 9810
    },
    {
      "epoch": 1.0698478844084924,
      "grad_norm": 1.2813423871994019,
      "learning_rate": 3.216750199753033e-05,
      "loss": 2.4692,
      "step": 9820
    },
    {
      "epoch": 1.0709373425392545,
      "grad_norm": 1.422896146774292,
      "learning_rate": 3.2149342630929034e-05,
      "loss": 2.446,
      "step": 9830
    },
    {
      "epoch": 1.0720268006700167,
      "grad_norm": 1.3068856000900269,
      "learning_rate": 3.213118326432774e-05,
      "loss": 2.4177,
      "step": 9840
    },
    {
      "epoch": 1.073116258800779,
      "grad_norm": 1.339309811592102,
      "learning_rate": 3.2113023897726445e-05,
      "loss": 2.4816,
      "step": 9850
    },
    {
      "epoch": 1.074205716931541,
      "grad_norm": 1.4223480224609375,
      "learning_rate": 3.209486453112516e-05,
      "loss": 2.5152,
      "step": 9860
    },
    {
      "epoch": 1.0752951750623034,
      "grad_norm": 1.402697205543518,
      "learning_rate": 3.207670516452386e-05,
      "loss": 2.5588,
      "step": 9870
    },
    {
      "epoch": 1.0763846331930655,
      "grad_norm": 1.3859752416610718,
      "learning_rate": 3.205854579792257e-05,
      "loss": 2.479,
      "step": 9880
    },
    {
      "epoch": 1.0774740913238279,
      "grad_norm": 1.426720142364502,
      "learning_rate": 3.204038643132128e-05,
      "loss": 2.4914,
      "step": 9890
    },
    {
      "epoch": 1.07856354945459,
      "grad_norm": 1.3373944759368896,
      "learning_rate": 3.2022227064719985e-05,
      "loss": 2.4913,
      "step": 9900
    },
    {
      "epoch": 1.0796530075853523,
      "grad_norm": 1.3793375492095947,
      "learning_rate": 3.200406769811869e-05,
      "loss": 2.4396,
      "step": 9910
    },
    {
      "epoch": 1.0807424657161144,
      "grad_norm": 1.4464441537857056,
      "learning_rate": 3.19859083315174e-05,
      "loss": 2.5354,
      "step": 9920
    },
    {
      "epoch": 1.0818319238468765,
      "grad_norm": 1.3431590795516968,
      "learning_rate": 3.196774896491611e-05,
      "loss": 2.5355,
      "step": 9930
    },
    {
      "epoch": 1.0829213819776389,
      "grad_norm": 1.4138940572738647,
      "learning_rate": 3.1949589598314814e-05,
      "loss": 2.5361,
      "step": 9940
    },
    {
      "epoch": 1.084010840108401,
      "grad_norm": 1.3809393644332886,
      "learning_rate": 3.193143023171352e-05,
      "loss": 2.4982,
      "step": 9950
    },
    {
      "epoch": 1.0851002982391633,
      "grad_norm": 1.3313581943511963,
      "learning_rate": 3.1913270865112225e-05,
      "loss": 2.4274,
      "step": 9960
    },
    {
      "epoch": 1.0861897563699254,
      "grad_norm": 1.3171788454055786,
      "learning_rate": 3.189511149851093e-05,
      "loss": 2.5663,
      "step": 9970
    },
    {
      "epoch": 1.0872792145006878,
      "grad_norm": 1.3672195672988892,
      "learning_rate": 3.187695213190964e-05,
      "loss": 2.4934,
      "step": 9980
    },
    {
      "epoch": 1.0883686726314499,
      "grad_norm": 1.4389774799346924,
      "learning_rate": 3.185879276530835e-05,
      "loss": 2.5792,
      "step": 9990
    },
    {
      "epoch": 1.0894581307622122,
      "grad_norm": 1.3515815734863281,
      "learning_rate": 3.184063339870705e-05,
      "loss": 2.4503,
      "step": 10000
    },
    {
      "epoch": 1.0905475888929743,
      "grad_norm": 1.2968186140060425,
      "learning_rate": 3.182247403210576e-05,
      "loss": 2.5117,
      "step": 10010
    },
    {
      "epoch": 1.0916370470237367,
      "grad_norm": 1.3248845338821411,
      "learning_rate": 3.180431466550447e-05,
      "loss": 2.4254,
      "step": 10020
    },
    {
      "epoch": 1.0927265051544988,
      "grad_norm": 1.3185418844223022,
      "learning_rate": 3.1786155298903176e-05,
      "loss": 2.4582,
      "step": 10030
    },
    {
      "epoch": 1.093815963285261,
      "grad_norm": 1.3917475938796997,
      "learning_rate": 3.176799593230188e-05,
      "loss": 2.5427,
      "step": 10040
    },
    {
      "epoch": 1.0949054214160232,
      "grad_norm": 1.348387360572815,
      "learning_rate": 3.1749836565700594e-05,
      "loss": 2.4748,
      "step": 10050
    },
    {
      "epoch": 1.0959948795467853,
      "grad_norm": 1.3954962491989136,
      "learning_rate": 3.17316771990993e-05,
      "loss": 2.5363,
      "step": 10060
    },
    {
      "epoch": 1.0970843376775476,
      "grad_norm": 1.363168716430664,
      "learning_rate": 3.1713517832498005e-05,
      "loss": 2.5823,
      "step": 10070
    },
    {
      "epoch": 1.0981737958083098,
      "grad_norm": 1.3636705875396729,
      "learning_rate": 3.169535846589671e-05,
      "loss": 2.4054,
      "step": 10080
    },
    {
      "epoch": 1.099263253939072,
      "grad_norm": 1.3761084079742432,
      "learning_rate": 3.1677199099295415e-05,
      "loss": 2.4693,
      "step": 10090
    },
    {
      "epoch": 1.1003527120698342,
      "grad_norm": 1.3556829690933228,
      "learning_rate": 3.165903973269412e-05,
      "loss": 2.5313,
      "step": 10100
    },
    {
      "epoch": 1.1014421702005965,
      "grad_norm": 1.3241021633148193,
      "learning_rate": 3.164088036609283e-05,
      "loss": 2.514,
      "step": 10110
    },
    {
      "epoch": 1.1025316283313586,
      "grad_norm": 1.4162218570709229,
      "learning_rate": 3.162272099949154e-05,
      "loss": 2.5359,
      "step": 10120
    },
    {
      "epoch": 1.103621086462121,
      "grad_norm": 1.3599252700805664,
      "learning_rate": 3.1604561632890244e-05,
      "loss": 2.5281,
      "step": 10130
    },
    {
      "epoch": 1.104710544592883,
      "grad_norm": 1.3711071014404297,
      "learning_rate": 3.1586402266288956e-05,
      "loss": 2.4487,
      "step": 10140
    },
    {
      "epoch": 1.1058000027236454,
      "grad_norm": 1.2863603830337524,
      "learning_rate": 3.156824289968766e-05,
      "loss": 2.3754,
      "step": 10150
    },
    {
      "epoch": 1.1068894608544075,
      "grad_norm": 1.3734947443008423,
      "learning_rate": 3.155008353308637e-05,
      "loss": 2.4365,
      "step": 10160
    },
    {
      "epoch": 1.1079789189851696,
      "grad_norm": 1.3506819009780884,
      "learning_rate": 3.153192416648507e-05,
      "loss": 2.407,
      "step": 10170
    },
    {
      "epoch": 1.109068377115932,
      "grad_norm": 1.3995258808135986,
      "learning_rate": 3.1513764799883784e-05,
      "loss": 2.5682,
      "step": 10180
    },
    {
      "epoch": 1.110157835246694,
      "grad_norm": 1.3162089586257935,
      "learning_rate": 3.149560543328249e-05,
      "loss": 2.4577,
      "step": 10190
    },
    {
      "epoch": 1.1112472933774564,
      "grad_norm": 1.4383982419967651,
      "learning_rate": 3.1477446066681195e-05,
      "loss": 2.5802,
      "step": 10200
    },
    {
      "epoch": 1.1123367515082185,
      "grad_norm": 1.4683514833450317,
      "learning_rate": 3.14592867000799e-05,
      "loss": 2.4258,
      "step": 10210
    },
    {
      "epoch": 1.1134262096389809,
      "grad_norm": 1.3735781908035278,
      "learning_rate": 3.1441127333478606e-05,
      "loss": 2.4674,
      "step": 10220
    },
    {
      "epoch": 1.114515667769743,
      "grad_norm": 1.3437435626983643,
      "learning_rate": 3.142296796687731e-05,
      "loss": 2.4666,
      "step": 10230
    },
    {
      "epoch": 1.1156051259005053,
      "grad_norm": 1.2823272943496704,
      "learning_rate": 3.1404808600276024e-05,
      "loss": 2.4726,
      "step": 10240
    },
    {
      "epoch": 1.1166945840312674,
      "grad_norm": 1.3553162813186646,
      "learning_rate": 3.138664923367473e-05,
      "loss": 2.4047,
      "step": 10250
    },
    {
      "epoch": 1.1177840421620298,
      "grad_norm": 1.3282262086868286,
      "learning_rate": 3.1368489867073435e-05,
      "loss": 2.4368,
      "step": 10260
    },
    {
      "epoch": 1.1188735002927919,
      "grad_norm": 1.3976129293441772,
      "learning_rate": 3.135033050047215e-05,
      "loss": 2.4888,
      "step": 10270
    },
    {
      "epoch": 1.119962958423554,
      "grad_norm": 1.4060097932815552,
      "learning_rate": 3.133217113387085e-05,
      "loss": 2.5059,
      "step": 10280
    },
    {
      "epoch": 1.1210524165543163,
      "grad_norm": 1.396369457244873,
      "learning_rate": 3.131401176726956e-05,
      "loss": 2.4888,
      "step": 10290
    },
    {
      "epoch": 1.1221418746850784,
      "grad_norm": 1.463059425354004,
      "learning_rate": 3.129585240066827e-05,
      "loss": 2.5664,
      "step": 10300
    },
    {
      "epoch": 1.1232313328158408,
      "grad_norm": 1.3665271997451782,
      "learning_rate": 3.1277693034066975e-05,
      "loss": 2.5812,
      "step": 10310
    },
    {
      "epoch": 1.1243207909466029,
      "grad_norm": 1.315230131149292,
      "learning_rate": 3.125953366746568e-05,
      "loss": 2.4623,
      "step": 10320
    },
    {
      "epoch": 1.1254102490773652,
      "grad_norm": 1.3372288942337036,
      "learning_rate": 3.124137430086439e-05,
      "loss": 2.5473,
      "step": 10330
    },
    {
      "epoch": 1.1264997072081273,
      "grad_norm": 1.4731817245483398,
      "learning_rate": 3.12232149342631e-05,
      "loss": 2.4942,
      "step": 10340
    },
    {
      "epoch": 1.1275891653388896,
      "grad_norm": 1.2820320129394531,
      "learning_rate": 3.1205055567661804e-05,
      "loss": 2.4814,
      "step": 10350
    },
    {
      "epoch": 1.1286786234696518,
      "grad_norm": 1.4080151319503784,
      "learning_rate": 3.118689620106051e-05,
      "loss": 2.5307,
      "step": 10360
    },
    {
      "epoch": 1.129768081600414,
      "grad_norm": 1.4771918058395386,
      "learning_rate": 3.1168736834459215e-05,
      "loss": 2.5319,
      "step": 10370
    },
    {
      "epoch": 1.1308575397311762,
      "grad_norm": 1.5633314847946167,
      "learning_rate": 3.115057746785792e-05,
      "loss": 2.497,
      "step": 10380
    },
    {
      "epoch": 1.1319469978619385,
      "grad_norm": 1.4056553840637207,
      "learning_rate": 3.1132418101256625e-05,
      "loss": 2.5353,
      "step": 10390
    },
    {
      "epoch": 1.1330364559927006,
      "grad_norm": 1.3126260042190552,
      "learning_rate": 3.111425873465534e-05,
      "loss": 2.5723,
      "step": 10400
    },
    {
      "epoch": 1.1341259141234628,
      "grad_norm": 1.3520524501800537,
      "learning_rate": 3.109609936805404e-05,
      "loss": 2.5113,
      "step": 10410
    },
    {
      "epoch": 1.135215372254225,
      "grad_norm": 1.4770500659942627,
      "learning_rate": 3.107794000145275e-05,
      "loss": 2.5991,
      "step": 10420
    },
    {
      "epoch": 1.1363048303849872,
      "grad_norm": 1.3522679805755615,
      "learning_rate": 3.105978063485146e-05,
      "loss": 2.541,
      "step": 10430
    },
    {
      "epoch": 1.1373942885157495,
      "grad_norm": 1.2705943584442139,
      "learning_rate": 3.1041621268250166e-05,
      "loss": 2.4941,
      "step": 10440
    },
    {
      "epoch": 1.1384837466465116,
      "grad_norm": 1.421333909034729,
      "learning_rate": 3.102346190164887e-05,
      "loss": 2.6036,
      "step": 10450
    },
    {
      "epoch": 1.139573204777274,
      "grad_norm": 1.4063628911972046,
      "learning_rate": 3.1005302535047584e-05,
      "loss": 2.4937,
      "step": 10460
    },
    {
      "epoch": 1.140662662908036,
      "grad_norm": 1.3617064952850342,
      "learning_rate": 3.098714316844629e-05,
      "loss": 2.4817,
      "step": 10470
    },
    {
      "epoch": 1.1417521210387984,
      "grad_norm": 1.3220322132110596,
      "learning_rate": 3.0968983801844994e-05,
      "loss": 2.5224,
      "step": 10480
    },
    {
      "epoch": 1.1428415791695605,
      "grad_norm": 1.37176513671875,
      "learning_rate": 3.09508244352437e-05,
      "loss": 2.4762,
      "step": 10490
    },
    {
      "epoch": 1.1439310373003226,
      "grad_norm": 1.3758469820022583,
      "learning_rate": 3.0932665068642405e-05,
      "loss": 2.435,
      "step": 10500
    },
    {
      "epoch": 1.145020495431085,
      "grad_norm": 1.3971619606018066,
      "learning_rate": 3.091450570204111e-05,
      "loss": 2.5064,
      "step": 10510
    },
    {
      "epoch": 1.146109953561847,
      "grad_norm": 1.3436394929885864,
      "learning_rate": 3.0896346335439816e-05,
      "loss": 2.5446,
      "step": 10520
    },
    {
      "epoch": 1.1471994116926094,
      "grad_norm": 1.421423077583313,
      "learning_rate": 3.087818696883853e-05,
      "loss": 2.5235,
      "step": 10530
    },
    {
      "epoch": 1.1482888698233715,
      "grad_norm": 1.3266445398330688,
      "learning_rate": 3.0860027602237234e-05,
      "loss": 2.57,
      "step": 10540
    },
    {
      "epoch": 1.1493783279541339,
      "grad_norm": 1.4222866296768188,
      "learning_rate": 3.084186823563594e-05,
      "loss": 2.5274,
      "step": 10550
    },
    {
      "epoch": 1.150467786084896,
      "grad_norm": 1.4229432344436646,
      "learning_rate": 3.082370886903465e-05,
      "loss": 2.5073,
      "step": 10560
    },
    {
      "epoch": 1.1515572442156583,
      "grad_norm": 1.4034960269927979,
      "learning_rate": 3.080554950243336e-05,
      "loss": 2.4689,
      "step": 10570
    },
    {
      "epoch": 1.1526467023464204,
      "grad_norm": 1.4235414266586304,
      "learning_rate": 3.078739013583206e-05,
      "loss": 2.4964,
      "step": 10580
    },
    {
      "epoch": 1.1537361604771827,
      "grad_norm": 1.3466213941574097,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 2.451,
      "step": 10590
    },
    {
      "epoch": 1.1548256186079449,
      "grad_norm": 1.353370189666748,
      "learning_rate": 3.075107140262948e-05,
      "loss": 2.5217,
      "step": 10600
    },
    {
      "epoch": 1.1559150767387072,
      "grad_norm": 1.3683768510818481,
      "learning_rate": 3.0732912036028185e-05,
      "loss": 2.5292,
      "step": 10610
    },
    {
      "epoch": 1.1570045348694693,
      "grad_norm": 1.446213722229004,
      "learning_rate": 3.071475266942689e-05,
      "loss": 2.5496,
      "step": 10620
    },
    {
      "epoch": 1.1580939930002314,
      "grad_norm": 1.3867435455322266,
      "learning_rate": 3.0696593302825596e-05,
      "loss": 2.4998,
      "step": 10630
    },
    {
      "epoch": 1.1591834511309937,
      "grad_norm": 1.328274130821228,
      "learning_rate": 3.06784339362243e-05,
      "loss": 2.4089,
      "step": 10640
    },
    {
      "epoch": 1.1602729092617559,
      "grad_norm": 1.4020050764083862,
      "learning_rate": 3.0660274569623014e-05,
      "loss": 2.4573,
      "step": 10650
    },
    {
      "epoch": 1.1613623673925182,
      "grad_norm": 1.3480836153030396,
      "learning_rate": 3.064211520302172e-05,
      "loss": 2.4459,
      "step": 10660
    },
    {
      "epoch": 1.1624518255232803,
      "grad_norm": 1.3636256456375122,
      "learning_rate": 3.0623955836420424e-05,
      "loss": 2.5497,
      "step": 10670
    },
    {
      "epoch": 1.1635412836540426,
      "grad_norm": 1.3666125535964966,
      "learning_rate": 3.060579646981914e-05,
      "loss": 2.5166,
      "step": 10680
    },
    {
      "epoch": 1.1646307417848047,
      "grad_norm": 1.305951476097107,
      "learning_rate": 3.058763710321784e-05,
      "loss": 2.5908,
      "step": 10690
    },
    {
      "epoch": 1.165720199915567,
      "grad_norm": 1.2856776714324951,
      "learning_rate": 3.056947773661655e-05,
      "loss": 2.576,
      "step": 10700
    },
    {
      "epoch": 1.1668096580463292,
      "grad_norm": 1.3754812479019165,
      "learning_rate": 3.055131837001525e-05,
      "loss": 2.5158,
      "step": 10710
    },
    {
      "epoch": 1.1678991161770913,
      "grad_norm": 1.3304625749588013,
      "learning_rate": 3.0533159003413965e-05,
      "loss": 2.479,
      "step": 10720
    },
    {
      "epoch": 1.1689885743078536,
      "grad_norm": 1.3588836193084717,
      "learning_rate": 3.051499963681267e-05,
      "loss": 2.4375,
      "step": 10730
    },
    {
      "epoch": 1.170078032438616,
      "grad_norm": 1.394124150276184,
      "learning_rate": 3.0496840270211373e-05,
      "loss": 2.4363,
      "step": 10740
    },
    {
      "epoch": 1.171167490569378,
      "grad_norm": 1.317989706993103,
      "learning_rate": 3.0478680903610085e-05,
      "loss": 2.4997,
      "step": 10750
    },
    {
      "epoch": 1.1722569487001402,
      "grad_norm": 1.3677204847335815,
      "learning_rate": 3.046052153700879e-05,
      "loss": 2.5344,
      "step": 10760
    },
    {
      "epoch": 1.1733464068309025,
      "grad_norm": 1.3405169248580933,
      "learning_rate": 3.0442362170407496e-05,
      "loss": 2.4411,
      "step": 10770
    },
    {
      "epoch": 1.1744358649616646,
      "grad_norm": 1.4458714723587036,
      "learning_rate": 3.0424202803806208e-05,
      "loss": 2.4502,
      "step": 10780
    },
    {
      "epoch": 1.175525323092427,
      "grad_norm": 1.4371354579925537,
      "learning_rate": 3.0406043437204913e-05,
      "loss": 2.6136,
      "step": 10790
    },
    {
      "epoch": 1.176614781223189,
      "grad_norm": 1.3739728927612305,
      "learning_rate": 3.038788407060362e-05,
      "loss": 2.5622,
      "step": 10800
    },
    {
      "epoch": 1.1777042393539514,
      "grad_norm": 1.4464668035507202,
      "learning_rate": 3.0369724704002327e-05,
      "loss": 2.4439,
      "step": 10810
    },
    {
      "epoch": 1.1787936974847135,
      "grad_norm": 1.4524134397506714,
      "learning_rate": 3.0351565337401033e-05,
      "loss": 2.4701,
      "step": 10820
    },
    {
      "epoch": 1.1798831556154759,
      "grad_norm": 1.4032219648361206,
      "learning_rate": 3.0333405970799738e-05,
      "loss": 2.52,
      "step": 10830
    },
    {
      "epoch": 1.180972613746238,
      "grad_norm": 1.383326530456543,
      "learning_rate": 3.031524660419845e-05,
      "loss": 2.415,
      "step": 10840
    },
    {
      "epoch": 1.182062071877,
      "grad_norm": 1.3631534576416016,
      "learning_rate": 3.0297087237597156e-05,
      "loss": 2.4672,
      "step": 10850
    },
    {
      "epoch": 1.1831515300077624,
      "grad_norm": 1.3601689338684082,
      "learning_rate": 3.027892787099586e-05,
      "loss": 2.6031,
      "step": 10860
    },
    {
      "epoch": 1.1842409881385245,
      "grad_norm": 1.4429795742034912,
      "learning_rate": 3.0260768504394567e-05,
      "loss": 2.5295,
      "step": 10870
    },
    {
      "epoch": 1.1853304462692869,
      "grad_norm": 1.4624494314193726,
      "learning_rate": 3.0242609137793276e-05,
      "loss": 2.5671,
      "step": 10880
    },
    {
      "epoch": 1.186419904400049,
      "grad_norm": 1.3104832172393799,
      "learning_rate": 3.022444977119198e-05,
      "loss": 2.4954,
      "step": 10890
    },
    {
      "epoch": 1.1875093625308113,
      "grad_norm": 1.4036126136779785,
      "learning_rate": 3.0206290404590686e-05,
      "loss": 2.5081,
      "step": 10900
    },
    {
      "epoch": 1.1885988206615734,
      "grad_norm": 1.3798668384552002,
      "learning_rate": 3.01881310379894e-05,
      "loss": 2.4077,
      "step": 10910
    },
    {
      "epoch": 1.1896882787923357,
      "grad_norm": 1.4031749963760376,
      "learning_rate": 3.0169971671388104e-05,
      "loss": 2.4081,
      "step": 10920
    },
    {
      "epoch": 1.1907777369230979,
      "grad_norm": 1.3949666023254395,
      "learning_rate": 3.015181230478681e-05,
      "loss": 2.4155,
      "step": 10930
    },
    {
      "epoch": 1.19186719505386,
      "grad_norm": 1.3981645107269287,
      "learning_rate": 3.0133652938185518e-05,
      "loss": 2.5122,
      "step": 10940
    },
    {
      "epoch": 1.1929566531846223,
      "grad_norm": 1.4044588804244995,
      "learning_rate": 3.0115493571584224e-05,
      "loss": 2.4346,
      "step": 10950
    },
    {
      "epoch": 1.1940461113153846,
      "grad_norm": 1.2968069314956665,
      "learning_rate": 3.009733420498293e-05,
      "loss": 2.5103,
      "step": 10960
    },
    {
      "epoch": 1.1951355694461467,
      "grad_norm": 1.402533769607544,
      "learning_rate": 3.007917483838164e-05,
      "loss": 2.449,
      "step": 10970
    },
    {
      "epoch": 1.1962250275769089,
      "grad_norm": 1.4023972749710083,
      "learning_rate": 3.0061015471780347e-05,
      "loss": 2.4983,
      "step": 10980
    },
    {
      "epoch": 1.1973144857076712,
      "grad_norm": 1.3570349216461182,
      "learning_rate": 3.0042856105179052e-05,
      "loss": 2.5257,
      "step": 10990
    },
    {
      "epoch": 1.1984039438384333,
      "grad_norm": 1.4828224182128906,
      "learning_rate": 3.002469673857776e-05,
      "loss": 2.4212,
      "step": 11000
    },
    {
      "epoch": 1.1994934019691956,
      "grad_norm": 1.3406907320022583,
      "learning_rate": 3.0006537371976466e-05,
      "loss": 2.5571,
      "step": 11010
    },
    {
      "epoch": 1.2005828600999577,
      "grad_norm": 1.3224656581878662,
      "learning_rate": 2.998837800537517e-05,
      "loss": 2.3598,
      "step": 11020
    },
    {
      "epoch": 1.20167231823072,
      "grad_norm": 1.4131242036819458,
      "learning_rate": 2.9970218638773884e-05,
      "loss": 2.5551,
      "step": 11030
    },
    {
      "epoch": 1.2027617763614822,
      "grad_norm": 1.4447916746139526,
      "learning_rate": 2.995205927217259e-05,
      "loss": 2.5158,
      "step": 11040
    },
    {
      "epoch": 1.2038512344922445,
      "grad_norm": 1.419684886932373,
      "learning_rate": 2.9933899905571295e-05,
      "loss": 2.48,
      "step": 11050
    },
    {
      "epoch": 1.2049406926230066,
      "grad_norm": 1.3532401323318481,
      "learning_rate": 2.991574053897e-05,
      "loss": 2.4783,
      "step": 11060
    },
    {
      "epoch": 1.2060301507537687,
      "grad_norm": 1.4276707172393799,
      "learning_rate": 2.989758117236871e-05,
      "loss": 2.4491,
      "step": 11070
    },
    {
      "epoch": 1.207119608884531,
      "grad_norm": 1.3775290250778198,
      "learning_rate": 2.9879421805767414e-05,
      "loss": 2.5939,
      "step": 11080
    },
    {
      "epoch": 1.2082090670152932,
      "grad_norm": 1.3676923513412476,
      "learning_rate": 2.986126243916612e-05,
      "loss": 2.4419,
      "step": 11090
    },
    {
      "epoch": 1.2092985251460555,
      "grad_norm": 1.4163992404937744,
      "learning_rate": 2.9843103072564832e-05,
      "loss": 2.5029,
      "step": 11100
    },
    {
      "epoch": 1.2103879832768176,
      "grad_norm": 1.3345417976379395,
      "learning_rate": 2.9824943705963537e-05,
      "loss": 2.5152,
      "step": 11110
    },
    {
      "epoch": 1.21147744140758,
      "grad_norm": 1.3499586582183838,
      "learning_rate": 2.9806784339362243e-05,
      "loss": 2.4408,
      "step": 11120
    },
    {
      "epoch": 1.212566899538342,
      "grad_norm": 1.3319578170776367,
      "learning_rate": 2.9788624972760955e-05,
      "loss": 2.4921,
      "step": 11130
    },
    {
      "epoch": 1.2136563576691044,
      "grad_norm": 1.4416558742523193,
      "learning_rate": 2.977046560615966e-05,
      "loss": 2.5763,
      "step": 11140
    },
    {
      "epoch": 1.2147458157998665,
      "grad_norm": 1.4956717491149902,
      "learning_rate": 2.9752306239558362e-05,
      "loss": 2.4438,
      "step": 11150
    },
    {
      "epoch": 1.2158352739306288,
      "grad_norm": 1.3475052118301392,
      "learning_rate": 2.9734146872957075e-05,
      "loss": 2.5602,
      "step": 11160
    },
    {
      "epoch": 1.216924732061391,
      "grad_norm": 1.5110764503479004,
      "learning_rate": 2.971598750635578e-05,
      "loss": 2.5131,
      "step": 11170
    },
    {
      "epoch": 1.2180141901921533,
      "grad_norm": 1.373146891593933,
      "learning_rate": 2.9697828139754485e-05,
      "loss": 2.4931,
      "step": 11180
    },
    {
      "epoch": 1.2191036483229154,
      "grad_norm": 1.441369891166687,
      "learning_rate": 2.9679668773153198e-05,
      "loss": 2.5005,
      "step": 11190
    },
    {
      "epoch": 1.2201931064536775,
      "grad_norm": 1.4556617736816406,
      "learning_rate": 2.9661509406551903e-05,
      "loss": 2.5123,
      "step": 11200
    },
    {
      "epoch": 1.2212825645844398,
      "grad_norm": 1.3191087245941162,
      "learning_rate": 2.964335003995061e-05,
      "loss": 2.3726,
      "step": 11210
    },
    {
      "epoch": 1.222372022715202,
      "grad_norm": 1.3814642429351807,
      "learning_rate": 2.9625190673349314e-05,
      "loss": 2.5149,
      "step": 11220
    },
    {
      "epoch": 1.2234614808459643,
      "grad_norm": 1.2891093492507935,
      "learning_rate": 2.9607031306748023e-05,
      "loss": 2.4623,
      "step": 11230
    },
    {
      "epoch": 1.2245509389767264,
      "grad_norm": 1.3898814916610718,
      "learning_rate": 2.9588871940146728e-05,
      "loss": 2.5658,
      "step": 11240
    },
    {
      "epoch": 1.2256403971074887,
      "grad_norm": 1.4185785055160522,
      "learning_rate": 2.9570712573545434e-05,
      "loss": 2.4905,
      "step": 11250
    },
    {
      "epoch": 1.2267298552382508,
      "grad_norm": 1.3393008708953857,
      "learning_rate": 2.9552553206944146e-05,
      "loss": 2.4121,
      "step": 11260
    },
    {
      "epoch": 1.2278193133690132,
      "grad_norm": 1.4135998487472534,
      "learning_rate": 2.953439384034285e-05,
      "loss": 2.5924,
      "step": 11270
    },
    {
      "epoch": 1.2289087714997753,
      "grad_norm": 1.353128433227539,
      "learning_rate": 2.9516234473741557e-05,
      "loss": 2.4743,
      "step": 11280
    },
    {
      "epoch": 1.2299982296305374,
      "grad_norm": 1.3519353866577148,
      "learning_rate": 2.9498075107140265e-05,
      "loss": 2.5248,
      "step": 11290
    },
    {
      "epoch": 1.2310876877612997,
      "grad_norm": 1.2336335182189941,
      "learning_rate": 2.947991574053897e-05,
      "loss": 2.6103,
      "step": 11300
    },
    {
      "epoch": 1.2321771458920618,
      "grad_norm": 1.3775323629379272,
      "learning_rate": 2.9461756373937676e-05,
      "loss": 2.5128,
      "step": 11310
    },
    {
      "epoch": 1.2332666040228242,
      "grad_norm": 1.5048823356628418,
      "learning_rate": 2.944359700733639e-05,
      "loss": 2.5334,
      "step": 11320
    },
    {
      "epoch": 1.2343560621535863,
      "grad_norm": 1.3277842998504639,
      "learning_rate": 2.9425437640735094e-05,
      "loss": 2.4725,
      "step": 11330
    },
    {
      "epoch": 1.2354455202843486,
      "grad_norm": 1.3618828058242798,
      "learning_rate": 2.94072782741338e-05,
      "loss": 2.4827,
      "step": 11340
    },
    {
      "epoch": 1.2365349784151107,
      "grad_norm": 1.3723433017730713,
      "learning_rate": 2.9389118907532508e-05,
      "loss": 2.5768,
      "step": 11350
    },
    {
      "epoch": 1.237624436545873,
      "grad_norm": 1.4282244443893433,
      "learning_rate": 2.9370959540931213e-05,
      "loss": 2.4415,
      "step": 11360
    },
    {
      "epoch": 1.2387138946766352,
      "grad_norm": 1.3972104787826538,
      "learning_rate": 2.935280017432992e-05,
      "loss": 2.5181,
      "step": 11370
    },
    {
      "epoch": 1.2398033528073975,
      "grad_norm": 1.2973077297210693,
      "learning_rate": 2.933464080772863e-05,
      "loss": 2.4114,
      "step": 11380
    },
    {
      "epoch": 1.2408928109381596,
      "grad_norm": 1.3526947498321533,
      "learning_rate": 2.9316481441127337e-05,
      "loss": 2.4921,
      "step": 11390
    },
    {
      "epoch": 1.241982269068922,
      "grad_norm": 1.333020567893982,
      "learning_rate": 2.9298322074526042e-05,
      "loss": 2.6037,
      "step": 11400
    },
    {
      "epoch": 1.243071727199684,
      "grad_norm": 1.3631192445755005,
      "learning_rate": 2.9280162707924747e-05,
      "loss": 2.4998,
      "step": 11410
    },
    {
      "epoch": 1.2441611853304462,
      "grad_norm": 1.4080801010131836,
      "learning_rate": 2.9262003341323456e-05,
      "loss": 2.4508,
      "step": 11420
    },
    {
      "epoch": 1.2452506434612085,
      "grad_norm": 1.484287977218628,
      "learning_rate": 2.924384397472216e-05,
      "loss": 2.5623,
      "step": 11430
    },
    {
      "epoch": 1.2463401015919706,
      "grad_norm": 1.44730806350708,
      "learning_rate": 2.9225684608120867e-05,
      "loss": 2.4673,
      "step": 11440
    },
    {
      "epoch": 1.247429559722733,
      "grad_norm": 1.391685962677002,
      "learning_rate": 2.920752524151958e-05,
      "loss": 2.5208,
      "step": 11450
    },
    {
      "epoch": 1.248519017853495,
      "grad_norm": 1.448412537574768,
      "learning_rate": 2.9189365874918285e-05,
      "loss": 2.5012,
      "step": 11460
    },
    {
      "epoch": 1.2496084759842574,
      "grad_norm": 1.4865000247955322,
      "learning_rate": 2.917120650831699e-05,
      "loss": 2.487,
      "step": 11470
    },
    {
      "epoch": 1.2506979341150195,
      "grad_norm": 1.3069342374801636,
      "learning_rate": 2.91530471417157e-05,
      "loss": 2.4337,
      "step": 11480
    },
    {
      "epoch": 1.2517873922457818,
      "grad_norm": 1.4710160493850708,
      "learning_rate": 2.9134887775114404e-05,
      "loss": 2.4945,
      "step": 11490
    },
    {
      "epoch": 1.252876850376544,
      "grad_norm": 1.367855191230774,
      "learning_rate": 2.911672840851311e-05,
      "loss": 2.4414,
      "step": 11500
    },
    {
      "epoch": 1.253966308507306,
      "grad_norm": 1.46821928024292,
      "learning_rate": 2.9098569041911822e-05,
      "loss": 2.4643,
      "step": 11510
    },
    {
      "epoch": 1.2550557666380684,
      "grad_norm": 1.3552004098892212,
      "learning_rate": 2.9080409675310527e-05,
      "loss": 2.4982,
      "step": 11520
    },
    {
      "epoch": 1.2561452247688307,
      "grad_norm": 1.401036262512207,
      "learning_rate": 2.9062250308709233e-05,
      "loss": 2.5425,
      "step": 11530
    },
    {
      "epoch": 1.2572346828995928,
      "grad_norm": 1.3287290334701538,
      "learning_rate": 2.9044090942107945e-05,
      "loss": 2.5015,
      "step": 11540
    },
    {
      "epoch": 1.258324141030355,
      "grad_norm": 1.3181476593017578,
      "learning_rate": 2.9025931575506647e-05,
      "loss": 2.5367,
      "step": 11550
    },
    {
      "epoch": 1.2594135991611173,
      "grad_norm": 1.385771632194519,
      "learning_rate": 2.9007772208905352e-05,
      "loss": 2.4866,
      "step": 11560
    },
    {
      "epoch": 1.2605030572918794,
      "grad_norm": 1.3808192014694214,
      "learning_rate": 2.8989612842304058e-05,
      "loss": 2.3741,
      "step": 11570
    },
    {
      "epoch": 1.2615925154226417,
      "grad_norm": 1.3507018089294434,
      "learning_rate": 2.897145347570277e-05,
      "loss": 2.4596,
      "step": 11580
    },
    {
      "epoch": 1.2626819735534038,
      "grad_norm": 1.4762510061264038,
      "learning_rate": 2.8953294109101475e-05,
      "loss": 2.4962,
      "step": 11590
    },
    {
      "epoch": 1.2637714316841662,
      "grad_norm": 1.3041489124298096,
      "learning_rate": 2.893513474250018e-05,
      "loss": 2.4411,
      "step": 11600
    },
    {
      "epoch": 1.2648608898149283,
      "grad_norm": 1.3953191041946411,
      "learning_rate": 2.8916975375898893e-05,
      "loss": 2.4214,
      "step": 11610
    },
    {
      "epoch": 1.2659503479456906,
      "grad_norm": 1.4948625564575195,
      "learning_rate": 2.88988160092976e-05,
      "loss": 2.5037,
      "step": 11620
    },
    {
      "epoch": 1.2670398060764527,
      "grad_norm": 1.3577605485916138,
      "learning_rate": 2.8880656642696304e-05,
      "loss": 2.4153,
      "step": 11630
    },
    {
      "epoch": 1.2681292642072148,
      "grad_norm": 1.3302009105682373,
      "learning_rate": 2.8862497276095013e-05,
      "loss": 2.4604,
      "step": 11640
    },
    {
      "epoch": 1.2692187223379772,
      "grad_norm": 1.3976753950119019,
      "learning_rate": 2.8844337909493718e-05,
      "loss": 2.4235,
      "step": 11650
    },
    {
      "epoch": 1.2703081804687393,
      "grad_norm": 1.4603185653686523,
      "learning_rate": 2.8826178542892423e-05,
      "loss": 2.4313,
      "step": 11660
    },
    {
      "epoch": 1.2713976385995016,
      "grad_norm": 1.4476685523986816,
      "learning_rate": 2.8808019176291136e-05,
      "loss": 2.4535,
      "step": 11670
    },
    {
      "epoch": 1.2724870967302637,
      "grad_norm": 1.491042137145996,
      "learning_rate": 2.878985980968984e-05,
      "loss": 2.4275,
      "step": 11680
    },
    {
      "epoch": 1.273576554861026,
      "grad_norm": 1.34184730052948,
      "learning_rate": 2.8771700443088546e-05,
      "loss": 2.4248,
      "step": 11690
    },
    {
      "epoch": 1.2746660129917882,
      "grad_norm": 1.4539704322814941,
      "learning_rate": 2.8753541076487255e-05,
      "loss": 2.44,
      "step": 11700
    },
    {
      "epoch": 1.2757554711225505,
      "grad_norm": 1.466723918914795,
      "learning_rate": 2.873538170988596e-05,
      "loss": 2.5152,
      "step": 11710
    },
    {
      "epoch": 1.2768449292533126,
      "grad_norm": 1.407536268234253,
      "learning_rate": 2.8717222343284666e-05,
      "loss": 2.523,
      "step": 11720
    },
    {
      "epoch": 1.2779343873840747,
      "grad_norm": 1.4897767305374146,
      "learning_rate": 2.869906297668338e-05,
      "loss": 2.5257,
      "step": 11730
    },
    {
      "epoch": 1.279023845514837,
      "grad_norm": 1.4363545179367065,
      "learning_rate": 2.8680903610082084e-05,
      "loss": 2.4664,
      "step": 11740
    },
    {
      "epoch": 1.2801133036455994,
      "grad_norm": 1.431106448173523,
      "learning_rate": 2.866274424348079e-05,
      "loss": 2.461,
      "step": 11750
    },
    {
      "epoch": 1.2812027617763615,
      "grad_norm": 1.4164639711380005,
      "learning_rate": 2.8644584876879495e-05,
      "loss": 2.3845,
      "step": 11760
    },
    {
      "epoch": 1.2822922199071236,
      "grad_norm": 1.4415185451507568,
      "learning_rate": 2.8626425510278203e-05,
      "loss": 2.5585,
      "step": 11770
    },
    {
      "epoch": 1.283381678037886,
      "grad_norm": 1.5359410047531128,
      "learning_rate": 2.860826614367691e-05,
      "loss": 2.4125,
      "step": 11780
    },
    {
      "epoch": 1.284471136168648,
      "grad_norm": 1.3463279008865356,
      "learning_rate": 2.8590106777075614e-05,
      "loss": 2.4585,
      "step": 11790
    },
    {
      "epoch": 1.2855605942994104,
      "grad_norm": 1.3577264547348022,
      "learning_rate": 2.8571947410474326e-05,
      "loss": 2.6607,
      "step": 11800
    },
    {
      "epoch": 1.2866500524301725,
      "grad_norm": 1.5364923477172852,
      "learning_rate": 2.8553788043873032e-05,
      "loss": 2.5186,
      "step": 11810
    },
    {
      "epoch": 1.2877395105609348,
      "grad_norm": 1.398508071899414,
      "learning_rate": 2.8535628677271737e-05,
      "loss": 2.4392,
      "step": 11820
    },
    {
      "epoch": 1.288828968691697,
      "grad_norm": 1.4598171710968018,
      "learning_rate": 2.8517469310670446e-05,
      "loss": 2.5347,
      "step": 11830
    },
    {
      "epoch": 1.2899184268224593,
      "grad_norm": 1.3683130741119385,
      "learning_rate": 2.849930994406915e-05,
      "loss": 2.4717,
      "step": 11840
    },
    {
      "epoch": 1.2910078849532214,
      "grad_norm": 1.3483715057373047,
      "learning_rate": 2.8481150577467857e-05,
      "loss": 2.4507,
      "step": 11850
    },
    {
      "epoch": 1.2920973430839835,
      "grad_norm": 1.3952720165252686,
      "learning_rate": 2.846299121086657e-05,
      "loss": 2.4708,
      "step": 11860
    },
    {
      "epoch": 1.2931868012147458,
      "grad_norm": 1.3831020593643188,
      "learning_rate": 2.8444831844265274e-05,
      "loss": 2.464,
      "step": 11870
    },
    {
      "epoch": 1.2942762593455082,
      "grad_norm": 1.3369015455245972,
      "learning_rate": 2.842667247766398e-05,
      "loss": 2.3376,
      "step": 11880
    },
    {
      "epoch": 1.2953657174762703,
      "grad_norm": 1.3447426557540894,
      "learning_rate": 2.840851311106269e-05,
      "loss": 2.532,
      "step": 11890
    },
    {
      "epoch": 1.2964551756070324,
      "grad_norm": 1.391828179359436,
      "learning_rate": 2.8390353744461394e-05,
      "loss": 2.4964,
      "step": 11900
    },
    {
      "epoch": 1.2975446337377947,
      "grad_norm": 1.4585479497909546,
      "learning_rate": 2.83721943778601e-05,
      "loss": 2.5717,
      "step": 11910
    },
    {
      "epoch": 1.2986340918685568,
      "grad_norm": 1.4050250053405762,
      "learning_rate": 2.8354035011258805e-05,
      "loss": 2.5064,
      "step": 11920
    },
    {
      "epoch": 1.2997235499993192,
      "grad_norm": 1.3254766464233398,
      "learning_rate": 2.8335875644657517e-05,
      "loss": 2.3814,
      "step": 11930
    },
    {
      "epoch": 1.3008130081300813,
      "grad_norm": 1.4354151487350464,
      "learning_rate": 2.8317716278056223e-05,
      "loss": 2.5151,
      "step": 11940
    },
    {
      "epoch": 1.3019024662608434,
      "grad_norm": 1.4566764831542969,
      "learning_rate": 2.8299556911454928e-05,
      "loss": 2.5006,
      "step": 11950
    },
    {
      "epoch": 1.3029919243916057,
      "grad_norm": 1.4567582607269287,
      "learning_rate": 2.8281397544853637e-05,
      "loss": 2.4506,
      "step": 11960
    },
    {
      "epoch": 1.304081382522368,
      "grad_norm": 1.3781538009643555,
      "learning_rate": 2.8263238178252342e-05,
      "loss": 2.5717,
      "step": 11970
    },
    {
      "epoch": 1.3051708406531302,
      "grad_norm": 1.3864307403564453,
      "learning_rate": 2.8245078811651048e-05,
      "loss": 2.4506,
      "step": 11980
    },
    {
      "epoch": 1.3062602987838923,
      "grad_norm": 1.4722802639007568,
      "learning_rate": 2.822691944504976e-05,
      "loss": 2.4863,
      "step": 11990
    },
    {
      "epoch": 1.3073497569146546,
      "grad_norm": 1.4506012201309204,
      "learning_rate": 2.8208760078448465e-05,
      "loss": 2.4949,
      "step": 12000
    },
    {
      "epoch": 1.3084392150454167,
      "grad_norm": 1.4468166828155518,
      "learning_rate": 2.819060071184717e-05,
      "loss": 2.532,
      "step": 12010
    },
    {
      "epoch": 1.309528673176179,
      "grad_norm": 1.4073622226715088,
      "learning_rate": 2.8172441345245883e-05,
      "loss": 2.4355,
      "step": 12020
    },
    {
      "epoch": 1.3106181313069412,
      "grad_norm": 1.4101412296295166,
      "learning_rate": 2.8154281978644588e-05,
      "loss": 2.5484,
      "step": 12030
    },
    {
      "epoch": 1.3117075894377035,
      "grad_norm": 1.4053126573562622,
      "learning_rate": 2.813612261204329e-05,
      "loss": 2.4826,
      "step": 12040
    },
    {
      "epoch": 1.3127970475684656,
      "grad_norm": 1.3987345695495605,
      "learning_rate": 2.8117963245442002e-05,
      "loss": 2.6293,
      "step": 12050
    },
    {
      "epoch": 1.313886505699228,
      "grad_norm": 1.3500409126281738,
      "learning_rate": 2.8099803878840708e-05,
      "loss": 2.6153,
      "step": 12060
    },
    {
      "epoch": 1.31497596382999,
      "grad_norm": 1.4802206754684448,
      "learning_rate": 2.8081644512239413e-05,
      "loss": 2.6368,
      "step": 12070
    },
    {
      "epoch": 1.3160654219607522,
      "grad_norm": 1.3358707427978516,
      "learning_rate": 2.8063485145638126e-05,
      "loss": 2.4822,
      "step": 12080
    },
    {
      "epoch": 1.3171548800915145,
      "grad_norm": 1.4362033605575562,
      "learning_rate": 2.804532577903683e-05,
      "loss": 2.4238,
      "step": 12090
    },
    {
      "epoch": 1.3182443382222768,
      "grad_norm": 1.4125442504882812,
      "learning_rate": 2.8027166412435536e-05,
      "loss": 2.5718,
      "step": 12100
    },
    {
      "epoch": 1.319333796353039,
      "grad_norm": 1.4201422929763794,
      "learning_rate": 2.8009007045834242e-05,
      "loss": 2.4873,
      "step": 12110
    },
    {
      "epoch": 1.320423254483801,
      "grad_norm": 1.4706676006317139,
      "learning_rate": 2.799084767923295e-05,
      "loss": 2.4839,
      "step": 12120
    },
    {
      "epoch": 1.3215127126145634,
      "grad_norm": 1.4012359380722046,
      "learning_rate": 2.7972688312631656e-05,
      "loss": 2.5557,
      "step": 12130
    },
    {
      "epoch": 1.3226021707453255,
      "grad_norm": 1.4734832048416138,
      "learning_rate": 2.795452894603036e-05,
      "loss": 2.4261,
      "step": 12140
    },
    {
      "epoch": 1.3236916288760878,
      "grad_norm": 1.3254172801971436,
      "learning_rate": 2.7936369579429074e-05,
      "loss": 2.4997,
      "step": 12150
    },
    {
      "epoch": 1.32478108700685,
      "grad_norm": 1.4106084108352661,
      "learning_rate": 2.791821021282778e-05,
      "loss": 2.4659,
      "step": 12160
    },
    {
      "epoch": 1.325870545137612,
      "grad_norm": 1.429090142250061,
      "learning_rate": 2.7900050846226484e-05,
      "loss": 2.5426,
      "step": 12170
    },
    {
      "epoch": 1.3269600032683744,
      "grad_norm": 1.3656628131866455,
      "learning_rate": 2.7881891479625193e-05,
      "loss": 2.5148,
      "step": 12180
    },
    {
      "epoch": 1.3280494613991367,
      "grad_norm": 1.3817100524902344,
      "learning_rate": 2.78637321130239e-05,
      "loss": 2.4559,
      "step": 12190
    },
    {
      "epoch": 1.3291389195298988,
      "grad_norm": 1.4048712253570557,
      "learning_rate": 2.7845572746422604e-05,
      "loss": 2.4752,
      "step": 12200
    },
    {
      "epoch": 1.330228377660661,
      "grad_norm": 1.351657509803772,
      "learning_rate": 2.7827413379821316e-05,
      "loss": 2.3959,
      "step": 12210
    },
    {
      "epoch": 1.3313178357914233,
      "grad_norm": 1.4136990308761597,
      "learning_rate": 2.7809254013220022e-05,
      "loss": 2.4155,
      "step": 12220
    },
    {
      "epoch": 1.3324072939221854,
      "grad_norm": 1.3373780250549316,
      "learning_rate": 2.7791094646618727e-05,
      "loss": 2.5772,
      "step": 12230
    },
    {
      "epoch": 1.3334967520529477,
      "grad_norm": 1.4464733600616455,
      "learning_rate": 2.7772935280017436e-05,
      "loss": 2.4901,
      "step": 12240
    },
    {
      "epoch": 1.3345862101837098,
      "grad_norm": 1.407960057258606,
      "learning_rate": 2.775477591341614e-05,
      "loss": 2.5388,
      "step": 12250
    },
    {
      "epoch": 1.3356756683144722,
      "grad_norm": 1.4040063619613647,
      "learning_rate": 2.7736616546814847e-05,
      "loss": 2.3954,
      "step": 12260
    },
    {
      "epoch": 1.3367651264452343,
      "grad_norm": 1.392272710800171,
      "learning_rate": 2.7718457180213552e-05,
      "loss": 2.4198,
      "step": 12270
    },
    {
      "epoch": 1.3378545845759966,
      "grad_norm": 1.4390621185302734,
      "learning_rate": 2.7700297813612264e-05,
      "loss": 2.5132,
      "step": 12280
    },
    {
      "epoch": 1.3389440427067587,
      "grad_norm": 1.5238547325134277,
      "learning_rate": 2.768213844701097e-05,
      "loss": 2.3938,
      "step": 12290
    },
    {
      "epoch": 1.3400335008375208,
      "grad_norm": 1.382810115814209,
      "learning_rate": 2.7663979080409675e-05,
      "loss": 2.4482,
      "step": 12300
    },
    {
      "epoch": 1.3411229589682832,
      "grad_norm": 1.4338011741638184,
      "learning_rate": 2.7645819713808384e-05,
      "loss": 2.4243,
      "step": 12310
    },
    {
      "epoch": 1.3422124170990455,
      "grad_norm": 1.417258381843567,
      "learning_rate": 2.762766034720709e-05,
      "loss": 2.4813,
      "step": 12320
    },
    {
      "epoch": 1.3433018752298076,
      "grad_norm": 1.3702203035354614,
      "learning_rate": 2.7609500980605795e-05,
      "loss": 2.4266,
      "step": 12330
    },
    {
      "epoch": 1.3443913333605697,
      "grad_norm": 1.42970871925354,
      "learning_rate": 2.7591341614004507e-05,
      "loss": 2.4467,
      "step": 12340
    },
    {
      "epoch": 1.345480791491332,
      "grad_norm": 1.3843196630477905,
      "learning_rate": 2.7573182247403212e-05,
      "loss": 2.5923,
      "step": 12350
    },
    {
      "epoch": 1.3465702496220942,
      "grad_norm": 1.4961276054382324,
      "learning_rate": 2.7555022880801918e-05,
      "loss": 2.4806,
      "step": 12360
    },
    {
      "epoch": 1.3476597077528565,
      "grad_norm": 1.3477332592010498,
      "learning_rate": 2.7536863514200627e-05,
      "loss": 2.5402,
      "step": 12370
    },
    {
      "epoch": 1.3487491658836186,
      "grad_norm": 1.3592759370803833,
      "learning_rate": 2.7518704147599332e-05,
      "loss": 2.4382,
      "step": 12380
    },
    {
      "epoch": 1.349838624014381,
      "grad_norm": 1.385236144065857,
      "learning_rate": 2.7500544780998038e-05,
      "loss": 2.4577,
      "step": 12390
    },
    {
      "epoch": 1.350928082145143,
      "grad_norm": 1.3205920457839966,
      "learning_rate": 2.748238541439675e-05,
      "loss": 2.4524,
      "step": 12400
    },
    {
      "epoch": 1.3520175402759054,
      "grad_norm": 1.4271818399429321,
      "learning_rate": 2.7464226047795455e-05,
      "loss": 2.4797,
      "step": 12410
    },
    {
      "epoch": 1.3531069984066675,
      "grad_norm": 1.3791894912719727,
      "learning_rate": 2.744606668119416e-05,
      "loss": 2.3406,
      "step": 12420
    },
    {
      "epoch": 1.3541964565374296,
      "grad_norm": 1.4197419881820679,
      "learning_rate": 2.7427907314592866e-05,
      "loss": 2.4657,
      "step": 12430
    },
    {
      "epoch": 1.355285914668192,
      "grad_norm": 1.3778445720672607,
      "learning_rate": 2.7409747947991575e-05,
      "loss": 2.5692,
      "step": 12440
    },
    {
      "epoch": 1.356375372798954,
      "grad_norm": 1.4473401308059692,
      "learning_rate": 2.739158858139028e-05,
      "loss": 2.4695,
      "step": 12450
    },
    {
      "epoch": 1.3574648309297164,
      "grad_norm": 1.4221858978271484,
      "learning_rate": 2.7373429214788986e-05,
      "loss": 2.476,
      "step": 12460
    },
    {
      "epoch": 1.3585542890604785,
      "grad_norm": 1.4205011129379272,
      "learning_rate": 2.7355269848187698e-05,
      "loss": 2.5061,
      "step": 12470
    },
    {
      "epoch": 1.3596437471912408,
      "grad_norm": 1.4833234548568726,
      "learning_rate": 2.7337110481586403e-05,
      "loss": 2.4975,
      "step": 12480
    },
    {
      "epoch": 1.360733205322003,
      "grad_norm": 1.3837928771972656,
      "learning_rate": 2.731895111498511e-05,
      "loss": 2.394,
      "step": 12490
    },
    {
      "epoch": 1.3618226634527653,
      "grad_norm": 1.4520939588546753,
      "learning_rate": 2.730079174838382e-05,
      "loss": 2.5288,
      "step": 12500
    },
    {
      "epoch": 1.3629121215835274,
      "grad_norm": 1.3062565326690674,
      "learning_rate": 2.7282632381782526e-05,
      "loss": 2.4134,
      "step": 12510
    },
    {
      "epoch": 1.3640015797142895,
      "grad_norm": 1.370956301689148,
      "learning_rate": 2.726447301518123e-05,
      "loss": 2.4095,
      "step": 12520
    },
    {
      "epoch": 1.3650910378450518,
      "grad_norm": 1.3934845924377441,
      "learning_rate": 2.724631364857994e-05,
      "loss": 2.4584,
      "step": 12530
    },
    {
      "epoch": 1.3661804959758141,
      "grad_norm": 1.3875813484191895,
      "learning_rate": 2.7228154281978646e-05,
      "loss": 2.4301,
      "step": 12540
    },
    {
      "epoch": 1.3672699541065763,
      "grad_norm": 1.3659147024154663,
      "learning_rate": 2.720999491537735e-05,
      "loss": 2.502,
      "step": 12550
    },
    {
      "epoch": 1.3683594122373384,
      "grad_norm": 1.3362367153167725,
      "learning_rate": 2.7191835548776063e-05,
      "loss": 2.4114,
      "step": 12560
    },
    {
      "epoch": 1.3694488703681007,
      "grad_norm": 1.3791476488113403,
      "learning_rate": 2.717367618217477e-05,
      "loss": 2.5363,
      "step": 12570
    },
    {
      "epoch": 1.3705383284988628,
      "grad_norm": 1.4354232549667358,
      "learning_rate": 2.7155516815573474e-05,
      "loss": 2.5084,
      "step": 12580
    },
    {
      "epoch": 1.3716277866296251,
      "grad_norm": 1.3203940391540527,
      "learning_rate": 2.7137357448972183e-05,
      "loss": 2.5457,
      "step": 12590
    },
    {
      "epoch": 1.3727172447603873,
      "grad_norm": 1.3114020824432373,
      "learning_rate": 2.711919808237089e-05,
      "loss": 2.4452,
      "step": 12600
    },
    {
      "epoch": 1.3738067028911496,
      "grad_norm": 1.3949440717697144,
      "learning_rate": 2.7101038715769594e-05,
      "loss": 2.5548,
      "step": 12610
    },
    {
      "epoch": 1.3748961610219117,
      "grad_norm": 1.3351306915283203,
      "learning_rate": 2.70828793491683e-05,
      "loss": 2.5039,
      "step": 12620
    },
    {
      "epoch": 1.375985619152674,
      "grad_norm": 1.4139710664749146,
      "learning_rate": 2.706471998256701e-05,
      "loss": 2.5207,
      "step": 12630
    },
    {
      "epoch": 1.3770750772834361,
      "grad_norm": 1.3712379932403564,
      "learning_rate": 2.7046560615965717e-05,
      "loss": 2.5067,
      "step": 12640
    },
    {
      "epoch": 1.3781645354141983,
      "grad_norm": 1.4842628240585327,
      "learning_rate": 2.7028401249364422e-05,
      "loss": 2.5648,
      "step": 12650
    },
    {
      "epoch": 1.3792539935449606,
      "grad_norm": 1.4094598293304443,
      "learning_rate": 2.701024188276313e-05,
      "loss": 2.4285,
      "step": 12660
    },
    {
      "epoch": 1.380343451675723,
      "grad_norm": 1.3544114828109741,
      "learning_rate": 2.6992082516161837e-05,
      "loss": 2.4566,
      "step": 12670
    },
    {
      "epoch": 1.381432909806485,
      "grad_norm": 1.3866039514541626,
      "learning_rate": 2.6973923149560542e-05,
      "loss": 2.4483,
      "step": 12680
    },
    {
      "epoch": 1.3825223679372471,
      "grad_norm": 1.3294003009796143,
      "learning_rate": 2.6955763782959254e-05,
      "loss": 2.453,
      "step": 12690
    },
    {
      "epoch": 1.3836118260680095,
      "grad_norm": 1.4255644083023071,
      "learning_rate": 2.693760441635796e-05,
      "loss": 2.4559,
      "step": 12700
    },
    {
      "epoch": 1.3847012841987716,
      "grad_norm": 1.3874552249908447,
      "learning_rate": 2.6919445049756665e-05,
      "loss": 2.3976,
      "step": 12710
    },
    {
      "epoch": 1.385790742329534,
      "grad_norm": 1.3577989339828491,
      "learning_rate": 2.6901285683155374e-05,
      "loss": 2.4415,
      "step": 12720
    },
    {
      "epoch": 1.386880200460296,
      "grad_norm": 1.4869105815887451,
      "learning_rate": 2.688312631655408e-05,
      "loss": 2.51,
      "step": 12730
    },
    {
      "epoch": 1.3879696585910581,
      "grad_norm": 1.3882980346679688,
      "learning_rate": 2.6864966949952785e-05,
      "loss": 2.5216,
      "step": 12740
    },
    {
      "epoch": 1.3890591167218205,
      "grad_norm": 1.3677927255630493,
      "learning_rate": 2.6846807583351497e-05,
      "loss": 2.433,
      "step": 12750
    },
    {
      "epoch": 1.3901485748525828,
      "grad_norm": 1.4840641021728516,
      "learning_rate": 2.6828648216750202e-05,
      "loss": 2.5396,
      "step": 12760
    },
    {
      "epoch": 1.391238032983345,
      "grad_norm": 1.419792890548706,
      "learning_rate": 2.6810488850148908e-05,
      "loss": 2.4709,
      "step": 12770
    },
    {
      "epoch": 1.392327491114107,
      "grad_norm": 1.3957856893539429,
      "learning_rate": 2.6792329483547613e-05,
      "loss": 2.4804,
      "step": 12780
    },
    {
      "epoch": 1.3934169492448694,
      "grad_norm": 1.3990585803985596,
      "learning_rate": 2.6774170116946322e-05,
      "loss": 2.4774,
      "step": 12790
    },
    {
      "epoch": 1.3945064073756315,
      "grad_norm": 1.4355051517486572,
      "learning_rate": 2.6756010750345027e-05,
      "loss": 2.5228,
      "step": 12800
    },
    {
      "epoch": 1.3955958655063938,
      "grad_norm": 1.3789132833480835,
      "learning_rate": 2.6737851383743733e-05,
      "loss": 2.4326,
      "step": 12810
    },
    {
      "epoch": 1.396685323637156,
      "grad_norm": 1.3812726736068726,
      "learning_rate": 2.6719692017142445e-05,
      "loss": 2.5165,
      "step": 12820
    },
    {
      "epoch": 1.3977747817679183,
      "grad_norm": 1.3523664474487305,
      "learning_rate": 2.670153265054115e-05,
      "loss": 2.3986,
      "step": 12830
    },
    {
      "epoch": 1.3988642398986804,
      "grad_norm": 1.4252921342849731,
      "learning_rate": 2.6683373283939856e-05,
      "loss": 2.4902,
      "step": 12840
    },
    {
      "epoch": 1.3999536980294427,
      "grad_norm": 1.4069082736968994,
      "learning_rate": 2.6665213917338565e-05,
      "loss": 2.5361,
      "step": 12850
    },
    {
      "epoch": 1.4010431561602048,
      "grad_norm": 1.3969813585281372,
      "learning_rate": 2.664705455073727e-05,
      "loss": 2.5374,
      "step": 12860
    },
    {
      "epoch": 1.402132614290967,
      "grad_norm": 1.4483671188354492,
      "learning_rate": 2.6628895184135975e-05,
      "loss": 2.5652,
      "step": 12870
    },
    {
      "epoch": 1.4032220724217292,
      "grad_norm": 1.4428170919418335,
      "learning_rate": 2.6610735817534688e-05,
      "loss": 2.568,
      "step": 12880
    },
    {
      "epoch": 1.4043115305524916,
      "grad_norm": 1.4166358709335327,
      "learning_rate": 2.6592576450933393e-05,
      "loss": 2.4799,
      "step": 12890
    },
    {
      "epoch": 1.4054009886832537,
      "grad_norm": 1.4133809804916382,
      "learning_rate": 2.65744170843321e-05,
      "loss": 2.4486,
      "step": 12900
    },
    {
      "epoch": 1.4064904468140158,
      "grad_norm": 1.303593635559082,
      "learning_rate": 2.655625771773081e-05,
      "loss": 2.4117,
      "step": 12910
    },
    {
      "epoch": 1.4075799049447781,
      "grad_norm": 1.4570648670196533,
      "learning_rate": 2.6538098351129516e-05,
      "loss": 2.5024,
      "step": 12920
    },
    {
      "epoch": 1.4086693630755402,
      "grad_norm": 1.4283486604690552,
      "learning_rate": 2.6519938984528218e-05,
      "loss": 2.4152,
      "step": 12930
    },
    {
      "epoch": 1.4097588212063026,
      "grad_norm": 1.2994269132614136,
      "learning_rate": 2.650177961792693e-05,
      "loss": 2.509,
      "step": 12940
    },
    {
      "epoch": 1.4108482793370647,
      "grad_norm": 1.3552887439727783,
      "learning_rate": 2.6483620251325636e-05,
      "loss": 2.4785,
      "step": 12950
    },
    {
      "epoch": 1.4119377374678268,
      "grad_norm": 1.4495795965194702,
      "learning_rate": 2.646546088472434e-05,
      "loss": 2.5282,
      "step": 12960
    },
    {
      "epoch": 1.4130271955985891,
      "grad_norm": 1.3932322263717651,
      "learning_rate": 2.6447301518123047e-05,
      "loss": 2.5536,
      "step": 12970
    },
    {
      "epoch": 1.4141166537293515,
      "grad_norm": 1.4969650506973267,
      "learning_rate": 2.642914215152176e-05,
      "loss": 2.4115,
      "step": 12980
    },
    {
      "epoch": 1.4152061118601136,
      "grad_norm": 1.3459277153015137,
      "learning_rate": 2.6410982784920464e-05,
      "loss": 2.4724,
      "step": 12990
    },
    {
      "epoch": 1.4162955699908757,
      "grad_norm": 1.480004072189331,
      "learning_rate": 2.639282341831917e-05,
      "loss": 2.4943,
      "step": 13000
    },
    {
      "epoch": 1.417385028121638,
      "grad_norm": 1.4306448698043823,
      "learning_rate": 2.637466405171788e-05,
      "loss": 2.4838,
      "step": 13010
    },
    {
      "epoch": 1.4184744862524001,
      "grad_norm": 1.6139744520187378,
      "learning_rate": 2.6356504685116584e-05,
      "loss": 2.4762,
      "step": 13020
    },
    {
      "epoch": 1.4195639443831625,
      "grad_norm": 1.307771921157837,
      "learning_rate": 2.633834531851529e-05,
      "loss": 2.4872,
      "step": 13030
    },
    {
      "epoch": 1.4206534025139246,
      "grad_norm": 1.4693210124969482,
      "learning_rate": 2.6320185951914e-05,
      "loss": 2.535,
      "step": 13040
    },
    {
      "epoch": 1.421742860644687,
      "grad_norm": 1.4771710634231567,
      "learning_rate": 2.6302026585312707e-05,
      "loss": 2.6724,
      "step": 13050
    },
    {
      "epoch": 1.422832318775449,
      "grad_norm": 1.4138216972351074,
      "learning_rate": 2.6283867218711412e-05,
      "loss": 2.5354,
      "step": 13060
    },
    {
      "epoch": 1.4239217769062114,
      "grad_norm": 1.4088956117630005,
      "learning_rate": 2.626570785211012e-05,
      "loss": 2.5012,
      "step": 13070
    },
    {
      "epoch": 1.4250112350369735,
      "grad_norm": 1.3868670463562012,
      "learning_rate": 2.6247548485508827e-05,
      "loss": 2.4339,
      "step": 13080
    },
    {
      "epoch": 1.4261006931677356,
      "grad_norm": 1.423176884651184,
      "learning_rate": 2.6229389118907532e-05,
      "loss": 2.5261,
      "step": 13090
    },
    {
      "epoch": 1.427190151298498,
      "grad_norm": 1.3204405307769775,
      "learning_rate": 2.6211229752306244e-05,
      "loss": 2.4353,
      "step": 13100
    },
    {
      "epoch": 1.4282796094292602,
      "grad_norm": 1.4223581552505493,
      "learning_rate": 2.619307038570495e-05,
      "loss": 2.5043,
      "step": 13110
    },
    {
      "epoch": 1.4293690675600224,
      "grad_norm": 1.4510562419891357,
      "learning_rate": 2.6174911019103655e-05,
      "loss": 2.4294,
      "step": 13120
    },
    {
      "epoch": 1.4304585256907845,
      "grad_norm": 1.381255030632019,
      "learning_rate": 2.615675165250236e-05,
      "loss": 2.4834,
      "step": 13130
    },
    {
      "epoch": 1.4315479838215468,
      "grad_norm": 1.4510855674743652,
      "learning_rate": 2.613859228590107e-05,
      "loss": 2.4978,
      "step": 13140
    },
    {
      "epoch": 1.432637441952309,
      "grad_norm": 1.4251694679260254,
      "learning_rate": 2.6120432919299775e-05,
      "loss": 2.5402,
      "step": 13150
    },
    {
      "epoch": 1.4337269000830712,
      "grad_norm": 1.3479708433151245,
      "learning_rate": 2.610227355269848e-05,
      "loss": 2.431,
      "step": 13160
    },
    {
      "epoch": 1.4348163582138334,
      "grad_norm": 1.4406696557998657,
      "learning_rate": 2.6084114186097192e-05,
      "loss": 2.5537,
      "step": 13170
    },
    {
      "epoch": 1.4359058163445957,
      "grad_norm": 1.5165112018585205,
      "learning_rate": 2.6065954819495898e-05,
      "loss": 2.4876,
      "step": 13180
    },
    {
      "epoch": 1.4369952744753578,
      "grad_norm": 1.3208919763565063,
      "learning_rate": 2.6047795452894603e-05,
      "loss": 2.4984,
      "step": 13190
    },
    {
      "epoch": 1.4380847326061201,
      "grad_norm": 1.4782962799072266,
      "learning_rate": 2.6029636086293312e-05,
      "loss": 2.567,
      "step": 13200
    },
    {
      "epoch": 1.4391741907368822,
      "grad_norm": 1.4377787113189697,
      "learning_rate": 2.6011476719692017e-05,
      "loss": 2.3688,
      "step": 13210
    },
    {
      "epoch": 1.4402636488676444,
      "grad_norm": 1.414992094039917,
      "learning_rate": 2.5993317353090723e-05,
      "loss": 2.4339,
      "step": 13220
    },
    {
      "epoch": 1.4413531069984067,
      "grad_norm": 1.4990403652191162,
      "learning_rate": 2.5975157986489435e-05,
      "loss": 2.5505,
      "step": 13230
    },
    {
      "epoch": 1.4424425651291688,
      "grad_norm": 1.3392889499664307,
      "learning_rate": 2.595699861988814e-05,
      "loss": 2.4714,
      "step": 13240
    },
    {
      "epoch": 1.4435320232599311,
      "grad_norm": 1.3936606645584106,
      "learning_rate": 2.5938839253286846e-05,
      "loss": 2.5533,
      "step": 13250
    },
    {
      "epoch": 1.4446214813906932,
      "grad_norm": 1.3422194719314575,
      "learning_rate": 2.5920679886685555e-05,
      "loss": 2.517,
      "step": 13260
    },
    {
      "epoch": 1.4457109395214556,
      "grad_norm": 1.3366148471832275,
      "learning_rate": 2.590252052008426e-05,
      "loss": 2.4365,
      "step": 13270
    },
    {
      "epoch": 1.4468003976522177,
      "grad_norm": 1.417665719985962,
      "learning_rate": 2.5884361153482965e-05,
      "loss": 2.5174,
      "step": 13280
    },
    {
      "epoch": 1.44788985578298,
      "grad_norm": 1.4500588178634644,
      "learning_rate": 2.5866201786881678e-05,
      "loss": 2.4076,
      "step": 13290
    },
    {
      "epoch": 1.4489793139137421,
      "grad_norm": 1.4990496635437012,
      "learning_rate": 2.5848042420280383e-05,
      "loss": 2.4159,
      "step": 13300
    },
    {
      "epoch": 1.4500687720445042,
      "grad_norm": 1.31936776638031,
      "learning_rate": 2.582988305367909e-05,
      "loss": 2.4125,
      "step": 13310
    },
    {
      "epoch": 1.4511582301752666,
      "grad_norm": 1.4704605340957642,
      "learning_rate": 2.5811723687077794e-05,
      "loss": 2.472,
      "step": 13320
    },
    {
      "epoch": 1.452247688306029,
      "grad_norm": 1.3694725036621094,
      "learning_rate": 2.5793564320476503e-05,
      "loss": 2.5366,
      "step": 13330
    },
    {
      "epoch": 1.453337146436791,
      "grad_norm": 1.4341869354248047,
      "learning_rate": 2.5775404953875208e-05,
      "loss": 2.4186,
      "step": 13340
    },
    {
      "epoch": 1.4544266045675531,
      "grad_norm": 1.4580241441726685,
      "learning_rate": 2.5757245587273913e-05,
      "loss": 2.4734,
      "step": 13350
    },
    {
      "epoch": 1.4555160626983155,
      "grad_norm": 1.3886092901229858,
      "learning_rate": 2.5739086220672626e-05,
      "loss": 2.4974,
      "step": 13360
    },
    {
      "epoch": 1.4566055208290776,
      "grad_norm": 1.4947806596755981,
      "learning_rate": 2.572092685407133e-05,
      "loss": 2.481,
      "step": 13370
    },
    {
      "epoch": 1.45769497895984,
      "grad_norm": 1.4571924209594727,
      "learning_rate": 2.5702767487470036e-05,
      "loss": 2.4897,
      "step": 13380
    },
    {
      "epoch": 1.458784437090602,
      "grad_norm": 1.4433485269546509,
      "learning_rate": 2.568460812086875e-05,
      "loss": 2.5333,
      "step": 13390
    },
    {
      "epoch": 1.4598738952213643,
      "grad_norm": 1.4297887086868286,
      "learning_rate": 2.5666448754267454e-05,
      "loss": 2.4709,
      "step": 13400
    },
    {
      "epoch": 1.4609633533521265,
      "grad_norm": 1.4257526397705078,
      "learning_rate": 2.564828938766616e-05,
      "loss": 2.5449,
      "step": 13410
    },
    {
      "epoch": 1.4620528114828888,
      "grad_norm": 1.4865843057632446,
      "learning_rate": 2.563013002106487e-05,
      "loss": 2.4441,
      "step": 13420
    },
    {
      "epoch": 1.463142269613651,
      "grad_norm": 1.5441110134124756,
      "learning_rate": 2.5611970654463574e-05,
      "loss": 2.5045,
      "step": 13430
    },
    {
      "epoch": 1.464231727744413,
      "grad_norm": 1.35459303855896,
      "learning_rate": 2.559381128786228e-05,
      "loss": 2.4464,
      "step": 13440
    },
    {
      "epoch": 1.4653211858751753,
      "grad_norm": 1.5169706344604492,
      "learning_rate": 2.557565192126099e-05,
      "loss": 2.4359,
      "step": 13450
    },
    {
      "epoch": 1.4664106440059377,
      "grad_norm": 1.4402819871902466,
      "learning_rate": 2.5557492554659697e-05,
      "loss": 2.4569,
      "step": 13460
    },
    {
      "epoch": 1.4675001021366998,
      "grad_norm": 1.4392212629318237,
      "learning_rate": 2.5539333188058402e-05,
      "loss": 2.5685,
      "step": 13470
    },
    {
      "epoch": 1.468589560267462,
      "grad_norm": 1.406559705734253,
      "learning_rate": 2.5521173821457108e-05,
      "loss": 2.4668,
      "step": 13480
    },
    {
      "epoch": 1.4696790183982242,
      "grad_norm": 1.4246459007263184,
      "learning_rate": 2.5503014454855816e-05,
      "loss": 2.3941,
      "step": 13490
    },
    {
      "epoch": 1.4707684765289863,
      "grad_norm": 1.453079104423523,
      "learning_rate": 2.5484855088254522e-05,
      "loss": 2.4054,
      "step": 13500
    },
    {
      "epoch": 1.4718579346597487,
      "grad_norm": 1.3480596542358398,
      "learning_rate": 2.5466695721653227e-05,
      "loss": 2.3826,
      "step": 13510
    },
    {
      "epoch": 1.4729473927905108,
      "grad_norm": 1.3829621076583862,
      "learning_rate": 2.544853635505194e-05,
      "loss": 2.4657,
      "step": 13520
    },
    {
      "epoch": 1.474036850921273,
      "grad_norm": 1.4903043508529663,
      "learning_rate": 2.5430376988450645e-05,
      "loss": 2.6159,
      "step": 13530
    },
    {
      "epoch": 1.4751263090520352,
      "grad_norm": 1.4579079151153564,
      "learning_rate": 2.541221762184935e-05,
      "loss": 2.4138,
      "step": 13540
    },
    {
      "epoch": 1.4762157671827976,
      "grad_norm": 1.5272268056869507,
      "learning_rate": 2.539405825524806e-05,
      "loss": 2.4361,
      "step": 13550
    },
    {
      "epoch": 1.4773052253135597,
      "grad_norm": 1.4667046070098877,
      "learning_rate": 2.5375898888646764e-05,
      "loss": 2.4615,
      "step": 13560
    },
    {
      "epoch": 1.4783946834443218,
      "grad_norm": 1.4045321941375732,
      "learning_rate": 2.535773952204547e-05,
      "loss": 2.5663,
      "step": 13570
    },
    {
      "epoch": 1.4794841415750841,
      "grad_norm": 1.4243652820587158,
      "learning_rate": 2.5339580155444182e-05,
      "loss": 2.4328,
      "step": 13580
    },
    {
      "epoch": 1.4805735997058462,
      "grad_norm": 1.357210397720337,
      "learning_rate": 2.5321420788842888e-05,
      "loss": 2.3984,
      "step": 13590
    },
    {
      "epoch": 1.4816630578366086,
      "grad_norm": 1.3283467292785645,
      "learning_rate": 2.5303261422241593e-05,
      "loss": 2.3423,
      "step": 13600
    },
    {
      "epoch": 1.4827525159673707,
      "grad_norm": 1.3978735208511353,
      "learning_rate": 2.5285102055640302e-05,
      "loss": 2.4036,
      "step": 13610
    },
    {
      "epoch": 1.483841974098133,
      "grad_norm": 1.4837745428085327,
      "learning_rate": 2.5266942689039007e-05,
      "loss": 2.4804,
      "step": 13620
    },
    {
      "epoch": 1.4849314322288951,
      "grad_norm": 1.4343762397766113,
      "learning_rate": 2.5248783322437713e-05,
      "loss": 2.402,
      "step": 13630
    },
    {
      "epoch": 1.4860208903596575,
      "grad_norm": 1.3504054546356201,
      "learning_rate": 2.5230623955836425e-05,
      "loss": 2.4365,
      "step": 13640
    },
    {
      "epoch": 1.4871103484904196,
      "grad_norm": 1.4200268983840942,
      "learning_rate": 2.521246458923513e-05,
      "loss": 2.5257,
      "step": 13650
    },
    {
      "epoch": 1.4881998066211817,
      "grad_norm": 1.4129481315612793,
      "learning_rate": 2.5194305222633836e-05,
      "loss": 2.4843,
      "step": 13660
    },
    {
      "epoch": 1.489289264751944,
      "grad_norm": 1.4139838218688965,
      "learning_rate": 2.517614585603254e-05,
      "loss": 2.525,
      "step": 13670
    },
    {
      "epoch": 1.4903787228827063,
      "grad_norm": 1.4463025331497192,
      "learning_rate": 2.515798648943125e-05,
      "loss": 2.5454,
      "step": 13680
    },
    {
      "epoch": 1.4914681810134685,
      "grad_norm": 1.4243956804275513,
      "learning_rate": 2.5139827122829955e-05,
      "loss": 2.449,
      "step": 13690
    },
    {
      "epoch": 1.4925576391442306,
      "grad_norm": 1.3869131803512573,
      "learning_rate": 2.512166775622866e-05,
      "loss": 2.5222,
      "step": 13700
    },
    {
      "epoch": 1.493647097274993,
      "grad_norm": 1.5062488317489624,
      "learning_rate": 2.5103508389627373e-05,
      "loss": 2.4902,
      "step": 13710
    },
    {
      "epoch": 1.494736555405755,
      "grad_norm": 1.5236455202102661,
      "learning_rate": 2.5085349023026078e-05,
      "loss": 2.538,
      "step": 13720
    },
    {
      "epoch": 1.4958260135365173,
      "grad_norm": 1.4160834550857544,
      "learning_rate": 2.5067189656424784e-05,
      "loss": 2.3509,
      "step": 13730
    },
    {
      "epoch": 1.4969154716672795,
      "grad_norm": 1.4786494970321655,
      "learning_rate": 2.5049030289823493e-05,
      "loss": 2.4859,
      "step": 13740
    },
    {
      "epoch": 1.4980049297980416,
      "grad_norm": 1.3992984294891357,
      "learning_rate": 2.5030870923222198e-05,
      "loss": 2.4897,
      "step": 13750
    },
    {
      "epoch": 1.499094387928804,
      "grad_norm": 1.4664559364318848,
      "learning_rate": 2.5012711556620903e-05,
      "loss": 2.5685,
      "step": 13760
    },
    {
      "epoch": 1.5001838460595662,
      "grad_norm": 1.3928465843200684,
      "learning_rate": 2.4994552190019612e-05,
      "loss": 2.4146,
      "step": 13770
    },
    {
      "epoch": 1.5012733041903283,
      "grad_norm": 1.5594656467437744,
      "learning_rate": 2.497639282341832e-05,
      "loss": 2.5703,
      "step": 13780
    },
    {
      "epoch": 1.5023627623210905,
      "grad_norm": 1.3995640277862549,
      "learning_rate": 2.4958233456817026e-05,
      "loss": 2.489,
      "step": 13790
    },
    {
      "epoch": 1.5034522204518528,
      "grad_norm": 1.376552939414978,
      "learning_rate": 2.4940074090215735e-05,
      "loss": 2.471,
      "step": 13800
    },
    {
      "epoch": 1.5045416785826151,
      "grad_norm": 1.4613949060440063,
      "learning_rate": 2.4921914723614444e-05,
      "loss": 2.5131,
      "step": 13810
    },
    {
      "epoch": 1.5056311367133772,
      "grad_norm": 1.444429636001587,
      "learning_rate": 2.4903755357013146e-05,
      "loss": 2.4778,
      "step": 13820
    },
    {
      "epoch": 1.5067205948441393,
      "grad_norm": 1.432966947555542,
      "learning_rate": 2.4885595990411855e-05,
      "loss": 2.4462,
      "step": 13830
    },
    {
      "epoch": 1.5078100529749014,
      "grad_norm": 1.3857861757278442,
      "learning_rate": 2.4867436623810564e-05,
      "loss": 2.487,
      "step": 13840
    },
    {
      "epoch": 1.5088995111056638,
      "grad_norm": 1.4130641222000122,
      "learning_rate": 2.484927725720927e-05,
      "loss": 2.4917,
      "step": 13850
    },
    {
      "epoch": 1.5099889692364261,
      "grad_norm": 1.3531750440597534,
      "learning_rate": 2.4831117890607978e-05,
      "loss": 2.4487,
      "step": 13860
    },
    {
      "epoch": 1.5110784273671882,
      "grad_norm": 1.4735268354415894,
      "learning_rate": 2.4812958524006683e-05,
      "loss": 2.4465,
      "step": 13870
    },
    {
      "epoch": 1.5121678854979503,
      "grad_norm": 1.4863793849945068,
      "learning_rate": 2.4794799157405392e-05,
      "loss": 2.5688,
      "step": 13880
    },
    {
      "epoch": 1.5132573436287127,
      "grad_norm": 1.4531652927398682,
      "learning_rate": 2.4776639790804097e-05,
      "loss": 2.6104,
      "step": 13890
    },
    {
      "epoch": 1.514346801759475,
      "grad_norm": 1.500093936920166,
      "learning_rate": 2.4758480424202803e-05,
      "loss": 2.6143,
      "step": 13900
    },
    {
      "epoch": 1.5154362598902371,
      "grad_norm": 1.378138780593872,
      "learning_rate": 2.4740321057601512e-05,
      "loss": 2.431,
      "step": 13910
    },
    {
      "epoch": 1.5165257180209992,
      "grad_norm": 1.3950144052505493,
      "learning_rate": 2.472216169100022e-05,
      "loss": 2.4694,
      "step": 13920
    },
    {
      "epoch": 1.5176151761517616,
      "grad_norm": 1.3677101135253906,
      "learning_rate": 2.4704002324398926e-05,
      "loss": 2.4859,
      "step": 13930
    },
    {
      "epoch": 1.518704634282524,
      "grad_norm": 1.4400885105133057,
      "learning_rate": 2.4685842957797635e-05,
      "loss": 2.5079,
      "step": 13940
    },
    {
      "epoch": 1.519794092413286,
      "grad_norm": 1.4942773580551147,
      "learning_rate": 2.466768359119634e-05,
      "loss": 2.4938,
      "step": 13950
    },
    {
      "epoch": 1.5208835505440481,
      "grad_norm": 1.4253764152526855,
      "learning_rate": 2.4649524224595046e-05,
      "loss": 2.4699,
      "step": 13960
    },
    {
      "epoch": 1.5219730086748102,
      "grad_norm": 1.490829586982727,
      "learning_rate": 2.4631364857993754e-05,
      "loss": 2.586,
      "step": 13970
    },
    {
      "epoch": 1.5230624668055726,
      "grad_norm": 1.393186330795288,
      "learning_rate": 2.461320549139246e-05,
      "loss": 2.4221,
      "step": 13980
    },
    {
      "epoch": 1.524151924936335,
      "grad_norm": 1.4173355102539062,
      "learning_rate": 2.459504612479117e-05,
      "loss": 2.4403,
      "step": 13990
    },
    {
      "epoch": 1.525241383067097,
      "grad_norm": 1.4394559860229492,
      "learning_rate": 2.4576886758189877e-05,
      "loss": 2.368,
      "step": 14000
    },
    {
      "epoch": 1.5263308411978591,
      "grad_norm": 1.365498661994934,
      "learning_rate": 2.4558727391588583e-05,
      "loss": 2.5017,
      "step": 14010
    },
    {
      "epoch": 1.5274202993286214,
      "grad_norm": 1.4502389430999756,
      "learning_rate": 2.4540568024987288e-05,
      "loss": 2.4566,
      "step": 14020
    },
    {
      "epoch": 1.5285097574593838,
      "grad_norm": 1.2830311059951782,
      "learning_rate": 2.4522408658385997e-05,
      "loss": 2.4522,
      "step": 14030
    },
    {
      "epoch": 1.5295992155901459,
      "grad_norm": 1.3769128322601318,
      "learning_rate": 2.4504249291784702e-05,
      "loss": 2.433,
      "step": 14040
    },
    {
      "epoch": 1.530688673720908,
      "grad_norm": 1.3633081912994385,
      "learning_rate": 2.448608992518341e-05,
      "loss": 2.4461,
      "step": 14050
    },
    {
      "epoch": 1.53177813185167,
      "grad_norm": 1.451170563697815,
      "learning_rate": 2.4467930558582117e-05,
      "loss": 2.4458,
      "step": 14060
    },
    {
      "epoch": 1.5328675899824324,
      "grad_norm": 1.3440665006637573,
      "learning_rate": 2.4449771191980825e-05,
      "loss": 2.4224,
      "step": 14070
    },
    {
      "epoch": 1.5339570481131948,
      "grad_norm": 1.4282217025756836,
      "learning_rate": 2.4431611825379534e-05,
      "loss": 2.5189,
      "step": 14080
    },
    {
      "epoch": 1.5350465062439569,
      "grad_norm": 1.4245541095733643,
      "learning_rate": 2.441345245877824e-05,
      "loss": 2.4244,
      "step": 14090
    },
    {
      "epoch": 1.536135964374719,
      "grad_norm": 1.4588011503219604,
      "learning_rate": 2.4395293092176945e-05,
      "loss": 2.513,
      "step": 14100
    },
    {
      "epoch": 1.5372254225054813,
      "grad_norm": 1.4227136373519897,
      "learning_rate": 2.4377133725575654e-05,
      "loss": 2.5427,
      "step": 14110
    },
    {
      "epoch": 1.5383148806362437,
      "grad_norm": 1.458400845527649,
      "learning_rate": 2.435897435897436e-05,
      "loss": 2.5298,
      "step": 14120
    },
    {
      "epoch": 1.5394043387670058,
      "grad_norm": 1.4884964227676392,
      "learning_rate": 2.4340814992373068e-05,
      "loss": 2.4457,
      "step": 14130
    },
    {
      "epoch": 1.5404937968977679,
      "grad_norm": 1.408635139465332,
      "learning_rate": 2.4322655625771774e-05,
      "loss": 2.3872,
      "step": 14140
    },
    {
      "epoch": 1.5415832550285302,
      "grad_norm": 1.4438179731369019,
      "learning_rate": 2.4304496259170482e-05,
      "loss": 2.4809,
      "step": 14150
    },
    {
      "epoch": 1.5426727131592926,
      "grad_norm": 1.380361795425415,
      "learning_rate": 2.4286336892569188e-05,
      "loss": 2.5688,
      "step": 14160
    },
    {
      "epoch": 1.5437621712900547,
      "grad_norm": 1.4925516843795776,
      "learning_rate": 2.4268177525967893e-05,
      "loss": 2.4203,
      "step": 14170
    },
    {
      "epoch": 1.5448516294208168,
      "grad_norm": 1.4215575456619263,
      "learning_rate": 2.4250018159366602e-05,
      "loss": 2.4649,
      "step": 14180
    },
    {
      "epoch": 1.5459410875515789,
      "grad_norm": 1.4250168800354004,
      "learning_rate": 2.423185879276531e-05,
      "loss": 2.4705,
      "step": 14190
    },
    {
      "epoch": 1.5470305456823412,
      "grad_norm": 1.4030580520629883,
      "learning_rate": 2.4213699426164016e-05,
      "loss": 2.4789,
      "step": 14200
    },
    {
      "epoch": 1.5481200038131036,
      "grad_norm": 1.4476383924484253,
      "learning_rate": 2.4195540059562725e-05,
      "loss": 2.4656,
      "step": 14210
    },
    {
      "epoch": 1.5492094619438657,
      "grad_norm": 1.3269424438476562,
      "learning_rate": 2.417738069296143e-05,
      "loss": 2.408,
      "step": 14220
    },
    {
      "epoch": 1.5502989200746278,
      "grad_norm": 1.4501327276229858,
      "learning_rate": 2.4159221326360136e-05,
      "loss": 2.5369,
      "step": 14230
    },
    {
      "epoch": 1.55138837820539,
      "grad_norm": 1.4475059509277344,
      "learning_rate": 2.4141061959758845e-05,
      "loss": 2.4962,
      "step": 14240
    },
    {
      "epoch": 1.5524778363361524,
      "grad_norm": 1.5026350021362305,
      "learning_rate": 2.412290259315755e-05,
      "loss": 2.4657,
      "step": 14250
    },
    {
      "epoch": 1.5535672944669146,
      "grad_norm": 1.5176286697387695,
      "learning_rate": 2.410474322655626e-05,
      "loss": 2.6167,
      "step": 14260
    },
    {
      "epoch": 1.5546567525976767,
      "grad_norm": 1.4032061100006104,
      "learning_rate": 2.4086583859954968e-05,
      "loss": 2.4719,
      "step": 14270
    },
    {
      "epoch": 1.555746210728439,
      "grad_norm": 1.3933333158493042,
      "learning_rate": 2.4068424493353673e-05,
      "loss": 2.4729,
      "step": 14280
    },
    {
      "epoch": 1.556835668859201,
      "grad_norm": 1.4326362609863281,
      "learning_rate": 2.4050265126752382e-05,
      "loss": 2.5371,
      "step": 14290
    },
    {
      "epoch": 1.5579251269899634,
      "grad_norm": 1.405231237411499,
      "learning_rate": 2.4032105760151087e-05,
      "loss": 2.4555,
      "step": 14300
    },
    {
      "epoch": 1.5590145851207255,
      "grad_norm": 1.5642554759979248,
      "learning_rate": 2.4013946393549793e-05,
      "loss": 2.5913,
      "step": 14310
    },
    {
      "epoch": 1.5601040432514877,
      "grad_norm": 1.3754171133041382,
      "learning_rate": 2.39957870269485e-05,
      "loss": 2.4908,
      "step": 14320
    },
    {
      "epoch": 1.56119350138225,
      "grad_norm": 1.4203357696533203,
      "learning_rate": 2.3977627660347207e-05,
      "loss": 2.5332,
      "step": 14330
    },
    {
      "epoch": 1.5622829595130123,
      "grad_norm": 1.4784451723098755,
      "learning_rate": 2.3959468293745916e-05,
      "loss": 2.4707,
      "step": 14340
    },
    {
      "epoch": 1.5633724176437744,
      "grad_norm": 1.3866689205169678,
      "learning_rate": 2.3941308927144625e-05,
      "loss": 2.4775,
      "step": 14350
    },
    {
      "epoch": 1.5644618757745365,
      "grad_norm": 1.4835693836212158,
      "learning_rate": 2.392314956054333e-05,
      "loss": 2.4756,
      "step": 14360
    },
    {
      "epoch": 1.5655513339052989,
      "grad_norm": 1.3773667812347412,
      "learning_rate": 2.3904990193942035e-05,
      "loss": 2.403,
      "step": 14370
    },
    {
      "epoch": 1.5666407920360612,
      "grad_norm": 1.4725146293640137,
      "learning_rate": 2.3886830827340744e-05,
      "loss": 2.4394,
      "step": 14380
    },
    {
      "epoch": 1.5677302501668233,
      "grad_norm": 1.430276870727539,
      "learning_rate": 2.386867146073945e-05,
      "loss": 2.3992,
      "step": 14390
    },
    {
      "epoch": 1.5688197082975854,
      "grad_norm": 1.4561280012130737,
      "learning_rate": 2.385051209413816e-05,
      "loss": 2.4549,
      "step": 14400
    },
    {
      "epoch": 1.5699091664283475,
      "grad_norm": 1.6015245914459229,
      "learning_rate": 2.3832352727536864e-05,
      "loss": 2.4388,
      "step": 14410
    },
    {
      "epoch": 1.5709986245591099,
      "grad_norm": 1.4044398069381714,
      "learning_rate": 2.3814193360935573e-05,
      "loss": 2.5326,
      "step": 14420
    },
    {
      "epoch": 1.5720880826898722,
      "grad_norm": 1.4093250036239624,
      "learning_rate": 2.3796033994334278e-05,
      "loss": 2.4093,
      "step": 14430
    },
    {
      "epoch": 1.5731775408206343,
      "grad_norm": 1.4311035871505737,
      "learning_rate": 2.3777874627732984e-05,
      "loss": 2.573,
      "step": 14440
    },
    {
      "epoch": 1.5742669989513964,
      "grad_norm": 1.3723275661468506,
      "learning_rate": 2.3759715261131692e-05,
      "loss": 2.4342,
      "step": 14450
    },
    {
      "epoch": 1.5753564570821588,
      "grad_norm": 1.4611529111862183,
      "learning_rate": 2.37415558945304e-05,
      "loss": 2.4699,
      "step": 14460
    },
    {
      "epoch": 1.576445915212921,
      "grad_norm": 1.4086964130401611,
      "learning_rate": 2.3723396527929107e-05,
      "loss": 2.3993,
      "step": 14470
    },
    {
      "epoch": 1.5775353733436832,
      "grad_norm": 1.4400932788848877,
      "learning_rate": 2.3705237161327815e-05,
      "loss": 2.4062,
      "step": 14480
    },
    {
      "epoch": 1.5786248314744453,
      "grad_norm": 1.424643635749817,
      "learning_rate": 2.368707779472652e-05,
      "loss": 2.425,
      "step": 14490
    },
    {
      "epoch": 1.5797142896052077,
      "grad_norm": 1.4320775270462036,
      "learning_rate": 2.366891842812523e-05,
      "loss": 2.6236,
      "step": 14500
    },
    {
      "epoch": 1.5808990753224115,
      "grad_norm": 1.4321575164794922,
      "learning_rate": 2.3650759061523935e-05,
      "loss": 2.4269,
      "step": 14510
    },
    {
      "epoch": 1.5819885334531736,
      "grad_norm": 1.434334635734558,
      "learning_rate": 2.363259969492264e-05,
      "loss": 2.4738,
      "step": 14520
    },
    {
      "epoch": 1.583077991583936,
      "grad_norm": 1.4637179374694824,
      "learning_rate": 2.361444032832135e-05,
      "loss": 2.5019,
      "step": 14530
    },
    {
      "epoch": 1.5841674497146983,
      "grad_norm": 1.445751667022705,
      "learning_rate": 2.3596280961720058e-05,
      "loss": 2.4663,
      "step": 14540
    },
    {
      "epoch": 1.5852569078454604,
      "grad_norm": 1.3868381977081299,
      "learning_rate": 2.3578121595118763e-05,
      "loss": 2.4767,
      "step": 14550
    },
    {
      "epoch": 1.5863463659762225,
      "grad_norm": 1.4533799886703491,
      "learning_rate": 2.3559962228517472e-05,
      "loss": 2.4311,
      "step": 14560
    },
    {
      "epoch": 1.5874358241069848,
      "grad_norm": 1.3560389280319214,
      "learning_rate": 2.3541802861916178e-05,
      "loss": 2.4202,
      "step": 14570
    },
    {
      "epoch": 1.5885252822377471,
      "grad_norm": 1.4747238159179688,
      "learning_rate": 2.3523643495314883e-05,
      "loss": 2.4638,
      "step": 14580
    },
    {
      "epoch": 1.5896147403685092,
      "grad_norm": 1.3510122299194336,
      "learning_rate": 2.3505484128713592e-05,
      "loss": 2.3987,
      "step": 14590
    },
    {
      "epoch": 1.5907041984992714,
      "grad_norm": 1.4747579097747803,
      "learning_rate": 2.3487324762112297e-05,
      "loss": 2.4747,
      "step": 14600
    },
    {
      "epoch": 1.5917936566300335,
      "grad_norm": 1.401343822479248,
      "learning_rate": 2.3469165395511006e-05,
      "loss": 2.4915,
      "step": 14610
    },
    {
      "epoch": 1.5928831147607958,
      "grad_norm": 1.3545949459075928,
      "learning_rate": 2.3451006028909715e-05,
      "loss": 2.384,
      "step": 14620
    },
    {
      "epoch": 1.5939725728915581,
      "grad_norm": 1.4511553049087524,
      "learning_rate": 2.343284666230842e-05,
      "loss": 2.4851,
      "step": 14630
    },
    {
      "epoch": 1.5950620310223202,
      "grad_norm": 1.4551963806152344,
      "learning_rate": 2.3414687295707126e-05,
      "loss": 2.4637,
      "step": 14640
    },
    {
      "epoch": 1.5961514891530824,
      "grad_norm": 1.3564651012420654,
      "learning_rate": 2.339652792910583e-05,
      "loss": 2.4983,
      "step": 14650
    },
    {
      "epoch": 1.5972409472838447,
      "grad_norm": 1.4634684324264526,
      "learning_rate": 2.337836856250454e-05,
      "loss": 2.5139,
      "step": 14660
    },
    {
      "epoch": 1.598330405414607,
      "grad_norm": 1.4056057929992676,
      "learning_rate": 2.336020919590325e-05,
      "loss": 2.3995,
      "step": 14670
    },
    {
      "epoch": 1.5994198635453691,
      "grad_norm": 1.421722412109375,
      "learning_rate": 2.3342049829301954e-05,
      "loss": 2.5182,
      "step": 14680
    },
    {
      "epoch": 1.6005093216761312,
      "grad_norm": 1.446478009223938,
      "learning_rate": 2.3323890462700663e-05,
      "loss": 2.4807,
      "step": 14690
    },
    {
      "epoch": 1.6015987798068936,
      "grad_norm": 1.457385778427124,
      "learning_rate": 2.3305731096099372e-05,
      "loss": 2.3861,
      "step": 14700
    },
    {
      "epoch": 1.602688237937656,
      "grad_norm": 1.428066372871399,
      "learning_rate": 2.3287571729498074e-05,
      "loss": 2.4658,
      "step": 14710
    },
    {
      "epoch": 1.603777696068418,
      "grad_norm": 1.6028752326965332,
      "learning_rate": 2.3269412362896783e-05,
      "loss": 2.434,
      "step": 14720
    },
    {
      "epoch": 1.6048671541991801,
      "grad_norm": 1.4634137153625488,
      "learning_rate": 2.325125299629549e-05,
      "loss": 2.5168,
      "step": 14730
    },
    {
      "epoch": 1.6059566123299422,
      "grad_norm": 1.4004892110824585,
      "learning_rate": 2.3233093629694197e-05,
      "loss": 2.4535,
      "step": 14740
    },
    {
      "epoch": 1.6070460704607046,
      "grad_norm": 1.4291473627090454,
      "learning_rate": 2.3214934263092906e-05,
      "loss": 2.5237,
      "step": 14750
    },
    {
      "epoch": 1.608135528591467,
      "grad_norm": 1.5024349689483643,
      "learning_rate": 2.319677489649161e-05,
      "loss": 2.4415,
      "step": 14760
    },
    {
      "epoch": 1.609224986722229,
      "grad_norm": 1.4218130111694336,
      "learning_rate": 2.317861552989032e-05,
      "loss": 2.4539,
      "step": 14770
    },
    {
      "epoch": 1.6103144448529911,
      "grad_norm": 1.3859529495239258,
      "learning_rate": 2.3160456163289025e-05,
      "loss": 2.3599,
      "step": 14780
    },
    {
      "epoch": 1.6114039029837535,
      "grad_norm": 1.5566375255584717,
      "learning_rate": 2.314229679668773e-05,
      "loss": 2.5053,
      "step": 14790
    },
    {
      "epoch": 1.6124933611145158,
      "grad_norm": 1.4467493295669556,
      "learning_rate": 2.312413743008644e-05,
      "loss": 2.4731,
      "step": 14800
    },
    {
      "epoch": 1.613582819245278,
      "grad_norm": 1.5001977682113647,
      "learning_rate": 2.310597806348515e-05,
      "loss": 2.5639,
      "step": 14810
    },
    {
      "epoch": 1.61467227737604,
      "grad_norm": 1.4475690126419067,
      "learning_rate": 2.3087818696883854e-05,
      "loss": 2.4555,
      "step": 14820
    },
    {
      "epoch": 1.6157617355068024,
      "grad_norm": 1.454641580581665,
      "learning_rate": 2.3069659330282563e-05,
      "loss": 2.4966,
      "step": 14830
    },
    {
      "epoch": 1.6168511936375645,
      "grad_norm": 1.4087491035461426,
      "learning_rate": 2.3051499963681268e-05,
      "loss": 2.4507,
      "step": 14840
    },
    {
      "epoch": 1.6179406517683268,
      "grad_norm": 1.386803388595581,
      "learning_rate": 2.3033340597079973e-05,
      "loss": 2.463,
      "step": 14850
    },
    {
      "epoch": 1.619030109899089,
      "grad_norm": 1.3916926383972168,
      "learning_rate": 2.3015181230478682e-05,
      "loss": 2.3805,
      "step": 14860
    },
    {
      "epoch": 1.620119568029851,
      "grad_norm": 1.461370587348938,
      "learning_rate": 2.2997021863877388e-05,
      "loss": 2.4228,
      "step": 14870
    },
    {
      "epoch": 1.6212090261606134,
      "grad_norm": 1.559906005859375,
      "learning_rate": 2.2978862497276096e-05,
      "loss": 2.5276,
      "step": 14880
    },
    {
      "epoch": 1.6222984842913757,
      "grad_norm": 1.4432088136672974,
      "learning_rate": 2.2960703130674805e-05,
      "loss": 2.5884,
      "step": 14890
    },
    {
      "epoch": 1.6233879424221378,
      "grad_norm": 1.3829222917556763,
      "learning_rate": 2.294254376407351e-05,
      "loss": 2.4525,
      "step": 14900
    },
    {
      "epoch": 1.6244774005529,
      "grad_norm": 1.399985432624817,
      "learning_rate": 2.2924384397472216e-05,
      "loss": 2.5249,
      "step": 14910
    },
    {
      "epoch": 1.6255668586836622,
      "grad_norm": 1.407356858253479,
      "learning_rate": 2.290622503087092e-05,
      "loss": 2.5759,
      "step": 14920
    },
    {
      "epoch": 1.6266563168144246,
      "grad_norm": 1.437504768371582,
      "learning_rate": 2.288806566426963e-05,
      "loss": 2.435,
      "step": 14930
    },
    {
      "epoch": 1.6277457749451867,
      "grad_norm": 1.428614854812622,
      "learning_rate": 2.286990629766834e-05,
      "loss": 2.5052,
      "step": 14940
    },
    {
      "epoch": 1.6288352330759488,
      "grad_norm": 1.4057446718215942,
      "learning_rate": 2.2851746931067045e-05,
      "loss": 2.545,
      "step": 14950
    },
    {
      "epoch": 1.629924691206711,
      "grad_norm": 1.4616127014160156,
      "learning_rate": 2.2833587564465753e-05,
      "loss": 2.5215,
      "step": 14960
    },
    {
      "epoch": 1.6310141493374732,
      "grad_norm": 1.4442753791809082,
      "learning_rate": 2.2815428197864462e-05,
      "loss": 2.4397,
      "step": 14970
    },
    {
      "epoch": 1.6321036074682356,
      "grad_norm": 1.4133179187774658,
      "learning_rate": 2.2797268831263168e-05,
      "loss": 2.4514,
      "step": 14980
    },
    {
      "epoch": 1.6331930655989977,
      "grad_norm": 1.4329651594161987,
      "learning_rate": 2.2779109464661873e-05,
      "loss": 2.5002,
      "step": 14990
    },
    {
      "epoch": 1.6342825237297598,
      "grad_norm": 1.4048227071762085,
      "learning_rate": 2.276095009806058e-05,
      "loss": 2.3495,
      "step": 15000
    },
    {
      "epoch": 1.6353719818605221,
      "grad_norm": 1.4769926071166992,
      "learning_rate": 2.2742790731459287e-05,
      "loss": 2.539,
      "step": 15010
    },
    {
      "epoch": 1.6364614399912845,
      "grad_norm": 1.3471921682357788,
      "learning_rate": 2.2724631364857996e-05,
      "loss": 2.4875,
      "step": 15020
    },
    {
      "epoch": 1.6375508981220466,
      "grad_norm": 1.4161627292633057,
      "learning_rate": 2.27064719982567e-05,
      "loss": 2.3847,
      "step": 15030
    },
    {
      "epoch": 1.6386403562528087,
      "grad_norm": 1.4721182584762573,
      "learning_rate": 2.268831263165541e-05,
      "loss": 2.5217,
      "step": 15040
    },
    {
      "epoch": 1.639729814383571,
      "grad_norm": 1.5904914140701294,
      "learning_rate": 2.2670153265054116e-05,
      "loss": 2.4061,
      "step": 15050
    },
    {
      "epoch": 1.6408192725143333,
      "grad_norm": 1.4189395904541016,
      "learning_rate": 2.265199389845282e-05,
      "loss": 2.4559,
      "step": 15060
    },
    {
      "epoch": 1.6419087306450955,
      "grad_norm": 1.390586256980896,
      "learning_rate": 2.263383453185153e-05,
      "loss": 2.474,
      "step": 15070
    },
    {
      "epoch": 1.6429981887758576,
      "grad_norm": 1.464647650718689,
      "learning_rate": 2.2615675165250235e-05,
      "loss": 2.4625,
      "step": 15080
    },
    {
      "epoch": 1.6440876469066197,
      "grad_norm": 1.5385327339172363,
      "learning_rate": 2.2597515798648944e-05,
      "loss": 2.511,
      "step": 15090
    },
    {
      "epoch": 1.645177105037382,
      "grad_norm": 1.3682022094726562,
      "learning_rate": 2.2579356432047653e-05,
      "loss": 2.3686,
      "step": 15100
    },
    {
      "epoch": 1.6462665631681443,
      "grad_norm": 1.4131486415863037,
      "learning_rate": 2.256119706544636e-05,
      "loss": 2.5064,
      "step": 15110
    },
    {
      "epoch": 1.6473560212989065,
      "grad_norm": 1.4492850303649902,
      "learning_rate": 2.2543037698845064e-05,
      "loss": 2.4154,
      "step": 15120
    },
    {
      "epoch": 1.6484454794296686,
      "grad_norm": 1.5103089809417725,
      "learning_rate": 2.2524878332243773e-05,
      "loss": 2.5003,
      "step": 15130
    },
    {
      "epoch": 1.649534937560431,
      "grad_norm": 1.4025722742080688,
      "learning_rate": 2.2506718965642478e-05,
      "loss": 2.4788,
      "step": 15140
    },
    {
      "epoch": 1.6506243956911932,
      "grad_norm": 1.4574778079986572,
      "learning_rate": 2.2488559599041187e-05,
      "loss": 2.515,
      "step": 15150
    },
    {
      "epoch": 1.6517138538219553,
      "grad_norm": 1.4869800806045532,
      "learning_rate": 2.2470400232439896e-05,
      "loss": 2.4882,
      "step": 15160
    },
    {
      "epoch": 1.6528033119527175,
      "grad_norm": 1.4409531354904175,
      "learning_rate": 2.24522408658386e-05,
      "loss": 2.5279,
      "step": 15170
    },
    {
      "epoch": 1.6538927700834796,
      "grad_norm": 1.4572230577468872,
      "learning_rate": 2.243408149923731e-05,
      "loss": 2.3437,
      "step": 15180
    },
    {
      "epoch": 1.654982228214242,
      "grad_norm": 1.4154658317565918,
      "learning_rate": 2.2415922132636015e-05,
      "loss": 2.5117,
      "step": 15190
    },
    {
      "epoch": 1.6560716863450042,
      "grad_norm": 1.4429218769073486,
      "learning_rate": 2.239776276603472e-05,
      "loss": 2.3541,
      "step": 15200
    },
    {
      "epoch": 1.6571611444757663,
      "grad_norm": 1.4849499464035034,
      "learning_rate": 2.237960339943343e-05,
      "loss": 2.4721,
      "step": 15210
    },
    {
      "epoch": 1.6582506026065285,
      "grad_norm": 1.394562005996704,
      "learning_rate": 2.2361444032832135e-05,
      "loss": 2.4436,
      "step": 15220
    },
    {
      "epoch": 1.6593400607372908,
      "grad_norm": 1.4110181331634521,
      "learning_rate": 2.2343284666230844e-05,
      "loss": 2.4779,
      "step": 15230
    },
    {
      "epoch": 1.6604295188680531,
      "grad_norm": 1.4096864461898804,
      "learning_rate": 2.2325125299629552e-05,
      "loss": 2.4361,
      "step": 15240
    },
    {
      "epoch": 1.6615189769988152,
      "grad_norm": 1.448320746421814,
      "learning_rate": 2.2306965933028258e-05,
      "loss": 2.4409,
      "step": 15250
    },
    {
      "epoch": 1.6626084351295773,
      "grad_norm": 1.4446806907653809,
      "learning_rate": 2.2288806566426963e-05,
      "loss": 2.5336,
      "step": 15260
    },
    {
      "epoch": 1.6636978932603397,
      "grad_norm": 1.381014108657837,
      "learning_rate": 2.227064719982567e-05,
      "loss": 2.4143,
      "step": 15270
    },
    {
      "epoch": 1.664787351391102,
      "grad_norm": 1.47381591796875,
      "learning_rate": 2.2252487833224378e-05,
      "loss": 2.4814,
      "step": 15280
    },
    {
      "epoch": 1.6658768095218641,
      "grad_norm": 1.4398058652877808,
      "learning_rate": 2.2234328466623086e-05,
      "loss": 2.4496,
      "step": 15290
    },
    {
      "epoch": 1.6669662676526262,
      "grad_norm": 1.4391241073608398,
      "learning_rate": 2.2216169100021792e-05,
      "loss": 2.4494,
      "step": 15300
    },
    {
      "epoch": 1.6680557257833883,
      "grad_norm": 1.40144681930542,
      "learning_rate": 2.21980097334205e-05,
      "loss": 2.5164,
      "step": 15310
    },
    {
      "epoch": 1.6691451839141507,
      "grad_norm": 1.5232433080673218,
      "learning_rate": 2.2179850366819206e-05,
      "loss": 2.4666,
      "step": 15320
    },
    {
      "epoch": 1.670234642044913,
      "grad_norm": 1.38738214969635,
      "learning_rate": 2.216169100021791e-05,
      "loss": 2.5084,
      "step": 15330
    },
    {
      "epoch": 1.6713241001756751,
      "grad_norm": 1.4667565822601318,
      "learning_rate": 2.214353163361662e-05,
      "loss": 2.5357,
      "step": 15340
    },
    {
      "epoch": 1.6724135583064372,
      "grad_norm": 1.4726942777633667,
      "learning_rate": 2.2125372267015326e-05,
      "loss": 2.5482,
      "step": 15350
    },
    {
      "epoch": 1.6735030164371996,
      "grad_norm": 1.4304784536361694,
      "learning_rate": 2.2107212900414034e-05,
      "loss": 2.5505,
      "step": 15360
    },
    {
      "epoch": 1.674592474567962,
      "grad_norm": 1.3798112869262695,
      "learning_rate": 2.2089053533812743e-05,
      "loss": 2.4796,
      "step": 15370
    },
    {
      "epoch": 1.675681932698724,
      "grad_norm": 1.4841560125350952,
      "learning_rate": 2.207089416721145e-05,
      "loss": 2.358,
      "step": 15380
    },
    {
      "epoch": 1.6767713908294861,
      "grad_norm": 1.5793609619140625,
      "learning_rate": 2.2052734800610157e-05,
      "loss": 2.3984,
      "step": 15390
    },
    {
      "epoch": 1.6778608489602482,
      "grad_norm": 1.4504326581954956,
      "learning_rate": 2.2034575434008863e-05,
      "loss": 2.4505,
      "step": 15400
    },
    {
      "epoch": 1.6789503070910106,
      "grad_norm": 1.5672147274017334,
      "learning_rate": 2.2016416067407568e-05,
      "loss": 2.4684,
      "step": 15410
    },
    {
      "epoch": 1.680039765221773,
      "grad_norm": 1.4611930847167969,
      "learning_rate": 2.1998256700806277e-05,
      "loss": 2.525,
      "step": 15420
    },
    {
      "epoch": 1.681129223352535,
      "grad_norm": 1.4330368041992188,
      "learning_rate": 2.1980097334204983e-05,
      "loss": 2.4002,
      "step": 15430
    },
    {
      "epoch": 1.6822186814832971,
      "grad_norm": 1.3793693780899048,
      "learning_rate": 2.196193796760369e-05,
      "loss": 2.5825,
      "step": 15440
    },
    {
      "epoch": 1.6833081396140595,
      "grad_norm": 1.4493248462677002,
      "learning_rate": 2.19437786010024e-05,
      "loss": 2.5313,
      "step": 15450
    },
    {
      "epoch": 1.6843975977448218,
      "grad_norm": 1.457271695137024,
      "learning_rate": 2.1925619234401106e-05,
      "loss": 2.513,
      "step": 15460
    },
    {
      "epoch": 1.685487055875584,
      "grad_norm": 1.3714351654052734,
      "learning_rate": 2.190745986779981e-05,
      "loss": 2.4983,
      "step": 15470
    },
    {
      "epoch": 1.686576514006346,
      "grad_norm": 1.3944274187088013,
      "learning_rate": 2.188930050119852e-05,
      "loss": 2.4933,
      "step": 15480
    },
    {
      "epoch": 1.6876659721371083,
      "grad_norm": 1.4417996406555176,
      "learning_rate": 2.1871141134597225e-05,
      "loss": 2.5212,
      "step": 15490
    },
    {
      "epoch": 1.6887554302678707,
      "grad_norm": 1.5393855571746826,
      "learning_rate": 2.1852981767995934e-05,
      "loss": 2.5515,
      "step": 15500
    },
    {
      "epoch": 1.6898448883986328,
      "grad_norm": 1.4942216873168945,
      "learning_rate": 2.1834822401394643e-05,
      "loss": 2.4406,
      "step": 15510
    },
    {
      "epoch": 1.690934346529395,
      "grad_norm": 1.477414608001709,
      "learning_rate": 2.1816663034793348e-05,
      "loss": 2.449,
      "step": 15520
    },
    {
      "epoch": 1.692023804660157,
      "grad_norm": 1.4069390296936035,
      "learning_rate": 2.1798503668192054e-05,
      "loss": 2.5221,
      "step": 15530
    },
    {
      "epoch": 1.6931132627909193,
      "grad_norm": 1.465408444404602,
      "learning_rate": 2.178034430159076e-05,
      "loss": 2.5639,
      "step": 15540
    },
    {
      "epoch": 1.6942027209216817,
      "grad_norm": 1.4379726648330688,
      "learning_rate": 2.1762184934989468e-05,
      "loss": 2.5166,
      "step": 15550
    },
    {
      "epoch": 1.6952921790524438,
      "grad_norm": 1.5496551990509033,
      "learning_rate": 2.1744025568388177e-05,
      "loss": 2.4392,
      "step": 15560
    },
    {
      "epoch": 1.696381637183206,
      "grad_norm": 1.4851713180541992,
      "learning_rate": 2.1725866201786882e-05,
      "loss": 2.5389,
      "step": 15570
    },
    {
      "epoch": 1.6974710953139682,
      "grad_norm": 1.4408897161483765,
      "learning_rate": 2.170770683518559e-05,
      "loss": 2.4458,
      "step": 15580
    },
    {
      "epoch": 1.6985605534447306,
      "grad_norm": 1.5369389057159424,
      "learning_rate": 2.16895474685843e-05,
      "loss": 2.5497,
      "step": 15590
    },
    {
      "epoch": 1.6996500115754927,
      "grad_norm": 1.5230268239974976,
      "learning_rate": 2.1671388101983002e-05,
      "loss": 2.3622,
      "step": 15600
    },
    {
      "epoch": 1.7007394697062548,
      "grad_norm": 1.4596954584121704,
      "learning_rate": 2.165322873538171e-05,
      "loss": 2.4455,
      "step": 15610
    },
    {
      "epoch": 1.7018289278370171,
      "grad_norm": 1.4387305974960327,
      "learning_rate": 2.1635069368780416e-05,
      "loss": 2.4721,
      "step": 15620
    },
    {
      "epoch": 1.7029183859677792,
      "grad_norm": 1.4693440198898315,
      "learning_rate": 2.1616910002179125e-05,
      "loss": 2.4061,
      "step": 15630
    },
    {
      "epoch": 1.7040078440985416,
      "grad_norm": 1.4150532484054565,
      "learning_rate": 2.1598750635577834e-05,
      "loss": 2.4378,
      "step": 15640
    },
    {
      "epoch": 1.7050973022293037,
      "grad_norm": 1.447229027748108,
      "learning_rate": 2.158059126897654e-05,
      "loss": 2.4147,
      "step": 15650
    },
    {
      "epoch": 1.7061867603600658,
      "grad_norm": 1.4718544483184814,
      "learning_rate": 2.1562431902375248e-05,
      "loss": 2.5206,
      "step": 15660
    },
    {
      "epoch": 1.7072762184908281,
      "grad_norm": 1.3900233507156372,
      "learning_rate": 2.1544272535773953e-05,
      "loss": 2.5073,
      "step": 15670
    },
    {
      "epoch": 1.7083656766215904,
      "grad_norm": 1.4706453084945679,
      "learning_rate": 2.152611316917266e-05,
      "loss": 2.415,
      "step": 15680
    },
    {
      "epoch": 1.7094551347523526,
      "grad_norm": 1.4946651458740234,
      "learning_rate": 2.1507953802571367e-05,
      "loss": 2.4321,
      "step": 15690
    },
    {
      "epoch": 1.7105445928831147,
      "grad_norm": 1.3836549520492554,
      "learning_rate": 2.1489794435970073e-05,
      "loss": 2.4502,
      "step": 15700
    },
    {
      "epoch": 1.711634051013877,
      "grad_norm": 1.4547001123428345,
      "learning_rate": 2.147163506936878e-05,
      "loss": 2.4753,
      "step": 15710
    },
    {
      "epoch": 1.7127235091446393,
      "grad_norm": 1.4325523376464844,
      "learning_rate": 2.145347570276749e-05,
      "loss": 2.4094,
      "step": 15720
    },
    {
      "epoch": 1.7138129672754014,
      "grad_norm": 1.385410189628601,
      "learning_rate": 2.1435316336166196e-05,
      "loss": 2.4469,
      "step": 15730
    },
    {
      "epoch": 1.7149024254061636,
      "grad_norm": 1.4480890035629272,
      "learning_rate": 2.14171569695649e-05,
      "loss": 2.5114,
      "step": 15740
    },
    {
      "epoch": 1.7159918835369257,
      "grad_norm": 1.3831712007522583,
      "learning_rate": 2.139899760296361e-05,
      "loss": 2.5125,
      "step": 15750
    },
    {
      "epoch": 1.717081341667688,
      "grad_norm": 1.6477986574172974,
      "learning_rate": 2.1380838236362316e-05,
      "loss": 2.5296,
      "step": 15760
    },
    {
      "epoch": 1.7181707997984503,
      "grad_norm": 1.4455331563949585,
      "learning_rate": 2.1362678869761024e-05,
      "loss": 2.4254,
      "step": 15770
    },
    {
      "epoch": 1.7192602579292124,
      "grad_norm": 1.3233546018600464,
      "learning_rate": 2.134451950315973e-05,
      "loss": 2.5183,
      "step": 15780
    },
    {
      "epoch": 1.7203497160599746,
      "grad_norm": 1.3881880044937134,
      "learning_rate": 2.132636013655844e-05,
      "loss": 2.5008,
      "step": 15790
    },
    {
      "epoch": 1.7214391741907369,
      "grad_norm": 1.6114048957824707,
      "learning_rate": 2.1308200769957144e-05,
      "loss": 2.5131,
      "step": 15800
    },
    {
      "epoch": 1.7225286323214992,
      "grad_norm": 1.4107840061187744,
      "learning_rate": 2.129004140335585e-05,
      "loss": 2.5562,
      "step": 15810
    },
    {
      "epoch": 1.7236180904522613,
      "grad_norm": 1.4241191148757935,
      "learning_rate": 2.1271882036754558e-05,
      "loss": 2.4954,
      "step": 15820
    },
    {
      "epoch": 1.7247075485830234,
      "grad_norm": 1.4646469354629517,
      "learning_rate": 2.1253722670153267e-05,
      "loss": 2.4924,
      "step": 15830
    },
    {
      "epoch": 1.7257970067137858,
      "grad_norm": 1.4830315113067627,
      "learning_rate": 2.1235563303551972e-05,
      "loss": 2.5028,
      "step": 15840
    },
    {
      "epoch": 1.726886464844548,
      "grad_norm": 1.4628108739852905,
      "learning_rate": 2.121740393695068e-05,
      "loss": 2.4539,
      "step": 15850
    },
    {
      "epoch": 1.7279759229753102,
      "grad_norm": 1.4138102531433105,
      "learning_rate": 2.119924457034939e-05,
      "loss": 2.4969,
      "step": 15860
    },
    {
      "epoch": 1.7290653811060723,
      "grad_norm": 1.443552017211914,
      "learning_rate": 2.1181085203748095e-05,
      "loss": 2.4577,
      "step": 15870
    },
    {
      "epoch": 1.7301548392368344,
      "grad_norm": 1.4741835594177246,
      "learning_rate": 2.11629258371468e-05,
      "loss": 2.5132,
      "step": 15880
    },
    {
      "epoch": 1.7312442973675968,
      "grad_norm": 1.4954602718353271,
      "learning_rate": 2.1144766470545506e-05,
      "loss": 2.463,
      "step": 15890
    },
    {
      "epoch": 1.732333755498359,
      "grad_norm": 1.3948861360549927,
      "learning_rate": 2.1126607103944215e-05,
      "loss": 2.4749,
      "step": 15900
    },
    {
      "epoch": 1.7334232136291212,
      "grad_norm": 1.5203865766525269,
      "learning_rate": 2.1108447737342924e-05,
      "loss": 2.4766,
      "step": 15910
    },
    {
      "epoch": 1.7345126717598833,
      "grad_norm": 1.4463468790054321,
      "learning_rate": 2.109028837074163e-05,
      "loss": 2.4156,
      "step": 15920
    },
    {
      "epoch": 1.7356021298906457,
      "grad_norm": 1.4260752201080322,
      "learning_rate": 2.1072129004140338e-05,
      "loss": 2.4239,
      "step": 15930
    },
    {
      "epoch": 1.736691588021408,
      "grad_norm": 1.4683045148849487,
      "learning_rate": 2.1053969637539044e-05,
      "loss": 2.4419,
      "step": 15940
    },
    {
      "epoch": 1.73778104615217,
      "grad_norm": 1.3896273374557495,
      "learning_rate": 2.103581027093775e-05,
      "loss": 2.4002,
      "step": 15950
    },
    {
      "epoch": 1.7388705042829322,
      "grad_norm": 1.4738463163375854,
      "learning_rate": 2.1017650904336458e-05,
      "loss": 2.5465,
      "step": 15960
    },
    {
      "epoch": 1.7399599624136943,
      "grad_norm": 1.409029245376587,
      "learning_rate": 2.0999491537735163e-05,
      "loss": 2.5346,
      "step": 15970
    },
    {
      "epoch": 1.7410494205444567,
      "grad_norm": 1.4722797870635986,
      "learning_rate": 2.0981332171133872e-05,
      "loss": 2.5148,
      "step": 15980
    },
    {
      "epoch": 1.742138878675219,
      "grad_norm": 1.470607042312622,
      "learning_rate": 2.096317280453258e-05,
      "loss": 2.5276,
      "step": 15990
    },
    {
      "epoch": 1.743228336805981,
      "grad_norm": 1.4394863843917847,
      "learning_rate": 2.0945013437931286e-05,
      "loss": 2.3981,
      "step": 16000
    },
    {
      "epoch": 1.7443177949367432,
      "grad_norm": 1.4858601093292236,
      "learning_rate": 2.092685407132999e-05,
      "loss": 2.5076,
      "step": 16010
    },
    {
      "epoch": 1.7454072530675055,
      "grad_norm": 1.4923979043960571,
      "learning_rate": 2.09086947047287e-05,
      "loss": 2.3781,
      "step": 16020
    },
    {
      "epoch": 1.7464967111982679,
      "grad_norm": 1.482846736907959,
      "learning_rate": 2.0890535338127406e-05,
      "loss": 2.4718,
      "step": 16030
    },
    {
      "epoch": 1.74758616932903,
      "grad_norm": 1.448784589767456,
      "learning_rate": 2.0872375971526115e-05,
      "loss": 2.5554,
      "step": 16040
    },
    {
      "epoch": 1.748675627459792,
      "grad_norm": 1.4487069845199585,
      "learning_rate": 2.085421660492482e-05,
      "loss": 2.4248,
      "step": 16050
    },
    {
      "epoch": 1.7497650855905544,
      "grad_norm": 1.391392469406128,
      "learning_rate": 2.083605723832353e-05,
      "loss": 2.4087,
      "step": 16060
    },
    {
      "epoch": 1.7508545437213168,
      "grad_norm": 1.5425434112548828,
      "learning_rate": 2.0817897871722238e-05,
      "loss": 2.4813,
      "step": 16070
    },
    {
      "epoch": 1.7519440018520789,
      "grad_norm": 1.4248135089874268,
      "learning_rate": 2.0799738505120943e-05,
      "loss": 2.4053,
      "step": 16080
    },
    {
      "epoch": 1.753033459982841,
      "grad_norm": 1.4294236898422241,
      "learning_rate": 2.078157913851965e-05,
      "loss": 2.484,
      "step": 16090
    },
    {
      "epoch": 1.754122918113603,
      "grad_norm": 1.4901771545410156,
      "learning_rate": 2.0763419771918357e-05,
      "loss": 2.4351,
      "step": 16100
    },
    {
      "epoch": 1.7552123762443654,
      "grad_norm": 1.4698468446731567,
      "learning_rate": 2.0745260405317063e-05,
      "loss": 2.4367,
      "step": 16110
    },
    {
      "epoch": 1.7563018343751278,
      "grad_norm": 1.3965940475463867,
      "learning_rate": 2.072710103871577e-05,
      "loss": 2.4447,
      "step": 16120
    },
    {
      "epoch": 1.7573912925058899,
      "grad_norm": 1.4797885417938232,
      "learning_rate": 2.0708941672114477e-05,
      "loss": 2.5145,
      "step": 16130
    },
    {
      "epoch": 1.758480750636652,
      "grad_norm": 1.3834031820297241,
      "learning_rate": 2.0690782305513186e-05,
      "loss": 2.459,
      "step": 16140
    },
    {
      "epoch": 1.7595702087674143,
      "grad_norm": 1.5175565481185913,
      "learning_rate": 2.067262293891189e-05,
      "loss": 2.4553,
      "step": 16150
    },
    {
      "epoch": 1.7606596668981767,
      "grad_norm": 1.480168104171753,
      "learning_rate": 2.0654463572310597e-05,
      "loss": 2.4547,
      "step": 16160
    },
    {
      "epoch": 1.7617491250289388,
      "grad_norm": 1.580399990081787,
      "learning_rate": 2.0636304205709305e-05,
      "loss": 2.5741,
      "step": 16170
    },
    {
      "epoch": 1.7628385831597009,
      "grad_norm": 1.4696285724639893,
      "learning_rate": 2.0618144839108014e-05,
      "loss": 2.3672,
      "step": 16180
    },
    {
      "epoch": 1.7639280412904632,
      "grad_norm": 1.4886987209320068,
      "learning_rate": 2.059998547250672e-05,
      "loss": 2.4643,
      "step": 16190
    },
    {
      "epoch": 1.7650174994212253,
      "grad_norm": 1.532755970954895,
      "learning_rate": 2.058182610590543e-05,
      "loss": 2.4178,
      "step": 16200
    },
    {
      "epoch": 1.7661069575519877,
      "grad_norm": 1.5023033618927002,
      "learning_rate": 2.0563666739304134e-05,
      "loss": 2.523,
      "step": 16210
    },
    {
      "epoch": 1.7671964156827498,
      "grad_norm": 1.3797465562820435,
      "learning_rate": 2.054550737270284e-05,
      "loss": 2.3725,
      "step": 16220
    },
    {
      "epoch": 1.7682858738135119,
      "grad_norm": 1.423112392425537,
      "learning_rate": 2.0527348006101548e-05,
      "loss": 2.497,
      "step": 16230
    },
    {
      "epoch": 1.7693753319442742,
      "grad_norm": 1.4380464553833008,
      "learning_rate": 2.0509188639500253e-05,
      "loss": 2.4553,
      "step": 16240
    },
    {
      "epoch": 1.7704647900750365,
      "grad_norm": 1.4119020700454712,
      "learning_rate": 2.0491029272898962e-05,
      "loss": 2.5203,
      "step": 16250
    },
    {
      "epoch": 1.7715542482057987,
      "grad_norm": 1.474138617515564,
      "learning_rate": 2.047286990629767e-05,
      "loss": 2.4867,
      "step": 16260
    },
    {
      "epoch": 1.7726437063365608,
      "grad_norm": 1.4986720085144043,
      "learning_rate": 2.0454710539696377e-05,
      "loss": 2.4857,
      "step": 16270
    },
    {
      "epoch": 1.773733164467323,
      "grad_norm": 1.4203014373779297,
      "learning_rate": 2.0436551173095085e-05,
      "loss": 2.4658,
      "step": 16280
    },
    {
      "epoch": 1.7748226225980854,
      "grad_norm": 1.472880482673645,
      "learning_rate": 2.041839180649379e-05,
      "loss": 2.4116,
      "step": 16290
    },
    {
      "epoch": 1.7759120807288475,
      "grad_norm": 1.4187456369400024,
      "learning_rate": 2.0400232439892496e-05,
      "loss": 2.3485,
      "step": 16300
    },
    {
      "epoch": 1.7770015388596097,
      "grad_norm": 1.4385474920272827,
      "learning_rate": 2.0382073073291205e-05,
      "loss": 2.5024,
      "step": 16310
    },
    {
      "epoch": 1.7780909969903718,
      "grad_norm": 1.4302958250045776,
      "learning_rate": 2.036391370668991e-05,
      "loss": 2.5082,
      "step": 16320
    },
    {
      "epoch": 1.779180455121134,
      "grad_norm": 1.3321795463562012,
      "learning_rate": 2.034575434008862e-05,
      "loss": 2.6077,
      "step": 16330
    },
    {
      "epoch": 1.7802699132518964,
      "grad_norm": 1.3616305589675903,
      "learning_rate": 2.0327594973487328e-05,
      "loss": 2.4359,
      "step": 16340
    },
    {
      "epoch": 1.7813593713826585,
      "grad_norm": 1.4525617361068726,
      "learning_rate": 2.0309435606886033e-05,
      "loss": 2.5314,
      "step": 16350
    },
    {
      "epoch": 1.7824488295134207,
      "grad_norm": 1.4094128608703613,
      "learning_rate": 2.029127624028474e-05,
      "loss": 2.4493,
      "step": 16360
    },
    {
      "epoch": 1.783538287644183,
      "grad_norm": 1.4784516096115112,
      "learning_rate": 2.0273116873683448e-05,
      "loss": 2.4523,
      "step": 16370
    },
    {
      "epoch": 1.7846277457749453,
      "grad_norm": 1.4744305610656738,
      "learning_rate": 2.0254957507082153e-05,
      "loss": 2.4808,
      "step": 16380
    },
    {
      "epoch": 1.7857172039057074,
      "grad_norm": 1.4938377141952515,
      "learning_rate": 2.0236798140480862e-05,
      "loss": 2.4753,
      "step": 16390
    },
    {
      "epoch": 1.7868066620364695,
      "grad_norm": 1.4633618593215942,
      "learning_rate": 2.0218638773879567e-05,
      "loss": 2.4773,
      "step": 16400
    },
    {
      "epoch": 1.7878961201672319,
      "grad_norm": 1.5409724712371826,
      "learning_rate": 2.0200479407278276e-05,
      "loss": 2.5326,
      "step": 16410
    },
    {
      "epoch": 1.788985578297994,
      "grad_norm": 1.5424524545669556,
      "learning_rate": 2.018232004067698e-05,
      "loss": 2.609,
      "step": 16420
    },
    {
      "epoch": 1.7900750364287563,
      "grad_norm": 1.442238688468933,
      "learning_rate": 2.0164160674075687e-05,
      "loss": 2.4418,
      "step": 16430
    },
    {
      "epoch": 1.7911644945595184,
      "grad_norm": 1.3830323219299316,
      "learning_rate": 2.0146001307474396e-05,
      "loss": 2.4268,
      "step": 16440
    },
    {
      "epoch": 1.7922539526902805,
      "grad_norm": 1.4583014249801636,
      "learning_rate": 2.0127841940873105e-05,
      "loss": 2.4977,
      "step": 16450
    },
    {
      "epoch": 1.7933434108210429,
      "grad_norm": 1.4243980646133423,
      "learning_rate": 2.010968257427181e-05,
      "loss": 2.4807,
      "step": 16460
    },
    {
      "epoch": 1.7944328689518052,
      "grad_norm": 1.4987094402313232,
      "learning_rate": 2.009152320767052e-05,
      "loss": 2.3678,
      "step": 16470
    },
    {
      "epoch": 1.7955223270825673,
      "grad_norm": 1.408904790878296,
      "learning_rate": 2.0073363841069224e-05,
      "loss": 2.4983,
      "step": 16480
    },
    {
      "epoch": 1.7966117852133294,
      "grad_norm": 1.4868966341018677,
      "learning_rate": 2.005520447446793e-05,
      "loss": 2.5428,
      "step": 16490
    },
    {
      "epoch": 1.7977012433440918,
      "grad_norm": 1.4062600135803223,
      "learning_rate": 2.003704510786664e-05,
      "loss": 2.4909,
      "step": 16500
    },
    {
      "epoch": 1.798790701474854,
      "grad_norm": 1.4751510620117188,
      "learning_rate": 2.0018885741265344e-05,
      "loss": 2.4991,
      "step": 16510
    },
    {
      "epoch": 1.7998801596056162,
      "grad_norm": 1.4870226383209229,
      "learning_rate": 2.0000726374664053e-05,
      "loss": 2.455,
      "step": 16520
    },
    {
      "epoch": 1.8009696177363783,
      "grad_norm": 1.4198453426361084,
      "learning_rate": 1.998256700806276e-05,
      "loss": 2.4367,
      "step": 16530
    },
    {
      "epoch": 1.8020590758671404,
      "grad_norm": 1.5901437997817993,
      "learning_rate": 1.9964407641461467e-05,
      "loss": 2.4928,
      "step": 16540
    },
    {
      "epoch": 1.8031485339979028,
      "grad_norm": 1.435058355331421,
      "learning_rate": 1.9946248274860176e-05,
      "loss": 2.4395,
      "step": 16550
    },
    {
      "epoch": 1.804237992128665,
      "grad_norm": 1.4505102634429932,
      "learning_rate": 1.992808890825888e-05,
      "loss": 2.4802,
      "step": 16560
    },
    {
      "epoch": 1.8053274502594272,
      "grad_norm": 1.4424335956573486,
      "learning_rate": 1.9909929541657586e-05,
      "loss": 2.3721,
      "step": 16570
    },
    {
      "epoch": 1.8064169083901893,
      "grad_norm": 1.497732162475586,
      "learning_rate": 1.9891770175056295e-05,
      "loss": 2.3632,
      "step": 16580
    },
    {
      "epoch": 1.8075063665209516,
      "grad_norm": 1.47055983543396,
      "learning_rate": 1.9873610808455e-05,
      "loss": 2.3835,
      "step": 16590
    },
    {
      "epoch": 1.808595824651714,
      "grad_norm": 1.497532606124878,
      "learning_rate": 1.985545144185371e-05,
      "loss": 2.3884,
      "step": 16600
    },
    {
      "epoch": 1.809685282782476,
      "grad_norm": 1.4565407037734985,
      "learning_rate": 1.9837292075252418e-05,
      "loss": 2.4154,
      "step": 16610
    },
    {
      "epoch": 1.8107747409132382,
      "grad_norm": 1.4205752611160278,
      "learning_rate": 1.9819132708651124e-05,
      "loss": 2.3937,
      "step": 16620
    },
    {
      "epoch": 1.8118641990440005,
      "grad_norm": 1.4761608839035034,
      "learning_rate": 1.980097334204983e-05,
      "loss": 2.4653,
      "step": 16630
    },
    {
      "epoch": 1.8129536571747629,
      "grad_norm": 1.433409333229065,
      "learning_rate": 1.9782813975448538e-05,
      "loss": 2.3582,
      "step": 16640
    },
    {
      "epoch": 1.814043115305525,
      "grad_norm": 1.5145615339279175,
      "learning_rate": 1.9764654608847243e-05,
      "loss": 2.3986,
      "step": 16650
    },
    {
      "epoch": 1.815132573436287,
      "grad_norm": 1.4986066818237305,
      "learning_rate": 1.9746495242245952e-05,
      "loss": 2.4417,
      "step": 16660
    },
    {
      "epoch": 1.8162220315670492,
      "grad_norm": 1.4150876998901367,
      "learning_rate": 1.9728335875644658e-05,
      "loss": 2.393,
      "step": 16670
    },
    {
      "epoch": 1.8173114896978115,
      "grad_norm": 1.438321828842163,
      "learning_rate": 1.9710176509043366e-05,
      "loss": 2.4017,
      "step": 16680
    },
    {
      "epoch": 1.8184009478285739,
      "grad_norm": 1.4627798795700073,
      "learning_rate": 1.9692017142442075e-05,
      "loss": 2.451,
      "step": 16690
    },
    {
      "epoch": 1.819490405959336,
      "grad_norm": 1.367960810661316,
      "learning_rate": 1.9673857775840777e-05,
      "loss": 2.3609,
      "step": 16700
    },
    {
      "epoch": 1.820579864090098,
      "grad_norm": 1.4828828573226929,
      "learning_rate": 1.9655698409239486e-05,
      "loss": 2.4226,
      "step": 16710
    },
    {
      "epoch": 1.8216693222208604,
      "grad_norm": 1.3846813440322876,
      "learning_rate": 1.9637539042638195e-05,
      "loss": 2.4897,
      "step": 16720
    },
    {
      "epoch": 1.8227587803516228,
      "grad_norm": 1.48598313331604,
      "learning_rate": 1.96193796760369e-05,
      "loss": 2.5631,
      "step": 16730
    },
    {
      "epoch": 1.8238482384823849,
      "grad_norm": 1.472452163696289,
      "learning_rate": 1.960122030943561e-05,
      "loss": 2.4938,
      "step": 16740
    },
    {
      "epoch": 1.824937696613147,
      "grad_norm": 1.4888091087341309,
      "learning_rate": 1.9583060942834314e-05,
      "loss": 2.4896,
      "step": 16750
    },
    {
      "epoch": 1.826027154743909,
      "grad_norm": 1.4317512512207031,
      "learning_rate": 1.9564901576233023e-05,
      "loss": 2.447,
      "step": 16760
    },
    {
      "epoch": 1.8271166128746714,
      "grad_norm": 1.4504588842391968,
      "learning_rate": 1.954674220963173e-05,
      "loss": 2.5221,
      "step": 16770
    },
    {
      "epoch": 1.8282060710054338,
      "grad_norm": 1.3916476964950562,
      "learning_rate": 1.9528582843030434e-05,
      "loss": 2.4593,
      "step": 16780
    },
    {
      "epoch": 1.8292955291361959,
      "grad_norm": 1.550659418106079,
      "learning_rate": 1.9510423476429143e-05,
      "loss": 2.3977,
      "step": 16790
    },
    {
      "epoch": 1.830384987266958,
      "grad_norm": 1.4950060844421387,
      "learning_rate": 1.9492264109827852e-05,
      "loss": 2.5271,
      "step": 16800
    },
    {
      "epoch": 1.8314744453977203,
      "grad_norm": 1.4505120515823364,
      "learning_rate": 1.9474104743226557e-05,
      "loss": 2.4478,
      "step": 16810
    },
    {
      "epoch": 1.8325639035284826,
      "grad_norm": 1.469112753868103,
      "learning_rate": 1.9455945376625266e-05,
      "loss": 2.4036,
      "step": 16820
    },
    {
      "epoch": 1.8336533616592448,
      "grad_norm": 1.4618347883224487,
      "learning_rate": 1.943778601002397e-05,
      "loss": 2.4308,
      "step": 16830
    },
    {
      "epoch": 1.8347428197900069,
      "grad_norm": 1.493875503540039,
      "learning_rate": 1.9419626643422677e-05,
      "loss": 2.4126,
      "step": 16840
    },
    {
      "epoch": 1.8358322779207692,
      "grad_norm": 1.4787360429763794,
      "learning_rate": 1.9401467276821386e-05,
      "loss": 2.5086,
      "step": 16850
    },
    {
      "epoch": 1.8369217360515315,
      "grad_norm": 1.4180431365966797,
      "learning_rate": 1.938330791022009e-05,
      "loss": 2.4006,
      "step": 16860
    },
    {
      "epoch": 1.8380111941822936,
      "grad_norm": 1.4785717725753784,
      "learning_rate": 1.93651485436188e-05,
      "loss": 2.4605,
      "step": 16870
    },
    {
      "epoch": 1.8391006523130558,
      "grad_norm": 1.435330867767334,
      "learning_rate": 1.934698917701751e-05,
      "loss": 2.5704,
      "step": 16880
    },
    {
      "epoch": 1.8401901104438179,
      "grad_norm": 1.5249065160751343,
      "learning_rate": 1.9328829810416214e-05,
      "loss": 2.4065,
      "step": 16890
    },
    {
      "epoch": 1.8412795685745802,
      "grad_norm": 1.462999701499939,
      "learning_rate": 1.931067044381492e-05,
      "loss": 2.4599,
      "step": 16900
    },
    {
      "epoch": 1.8423690267053425,
      "grad_norm": 1.4192168712615967,
      "learning_rate": 1.9292511077213625e-05,
      "loss": 2.514,
      "step": 16910
    },
    {
      "epoch": 1.8434584848361046,
      "grad_norm": 1.4916698932647705,
      "learning_rate": 1.9274351710612334e-05,
      "loss": 2.5642,
      "step": 16920
    },
    {
      "epoch": 1.8445479429668667,
      "grad_norm": 1.4264583587646484,
      "learning_rate": 1.9256192344011042e-05,
      "loss": 2.4603,
      "step": 16930
    },
    {
      "epoch": 1.845637401097629,
      "grad_norm": 1.4282453060150146,
      "learning_rate": 1.9238032977409748e-05,
      "loss": 2.4817,
      "step": 16940
    },
    {
      "epoch": 1.8467268592283914,
      "grad_norm": 1.5089887380599976,
      "learning_rate": 1.9219873610808457e-05,
      "loss": 2.4988,
      "step": 16950
    },
    {
      "epoch": 1.8478163173591535,
      "grad_norm": 1.5247008800506592,
      "learning_rate": 1.9201714244207166e-05,
      "loss": 2.4636,
      "step": 16960
    },
    {
      "epoch": 1.8489057754899156,
      "grad_norm": 1.4517980813980103,
      "learning_rate": 1.918355487760587e-05,
      "loss": 2.5353,
      "step": 16970
    },
    {
      "epoch": 1.849995233620678,
      "grad_norm": 1.392024040222168,
      "learning_rate": 1.9165395511004576e-05,
      "loss": 2.393,
      "step": 16980
    },
    {
      "epoch": 1.85108469175144,
      "grad_norm": 1.5088673830032349,
      "learning_rate": 1.9147236144403285e-05,
      "loss": 2.4812,
      "step": 16990
    },
    {
      "epoch": 1.8521741498822024,
      "grad_norm": 1.5671989917755127,
      "learning_rate": 1.912907677780199e-05,
      "loss": 2.4491,
      "step": 17000
    },
    {
      "epoch": 1.8532636080129645,
      "grad_norm": 1.4958893060684204,
      "learning_rate": 1.91109174112007e-05,
      "loss": 2.4383,
      "step": 17010
    },
    {
      "epoch": 1.8543530661437266,
      "grad_norm": 1.354201316833496,
      "learning_rate": 1.9092758044599405e-05,
      "loss": 2.3597,
      "step": 17020
    },
    {
      "epoch": 1.855442524274489,
      "grad_norm": 1.4879432916641235,
      "learning_rate": 1.9074598677998114e-05,
      "loss": 2.4463,
      "step": 17030
    },
    {
      "epoch": 1.8565319824052513,
      "grad_norm": 1.4183964729309082,
      "learning_rate": 1.905643931139682e-05,
      "loss": 2.4258,
      "step": 17040
    },
    {
      "epoch": 1.8576214405360134,
      "grad_norm": 1.3923358917236328,
      "learning_rate": 1.9038279944795524e-05,
      "loss": 2.4275,
      "step": 17050
    },
    {
      "epoch": 1.8587108986667755,
      "grad_norm": 1.4592365026474,
      "learning_rate": 1.9020120578194233e-05,
      "loss": 2.466,
      "step": 17060
    },
    {
      "epoch": 1.8598003567975379,
      "grad_norm": 1.4058781862258911,
      "learning_rate": 1.9001961211592942e-05,
      "loss": 2.4668,
      "step": 17070
    },
    {
      "epoch": 1.8608898149283002,
      "grad_norm": 1.443773627281189,
      "learning_rate": 1.8983801844991647e-05,
      "loss": 2.4887,
      "step": 17080
    },
    {
      "epoch": 1.8619792730590623,
      "grad_norm": 1.4821923971176147,
      "learning_rate": 1.8965642478390356e-05,
      "loss": 2.4651,
      "step": 17090
    },
    {
      "epoch": 1.8630687311898244,
      "grad_norm": 1.4375742673873901,
      "learning_rate": 1.8947483111789062e-05,
      "loss": 2.4148,
      "step": 17100
    },
    {
      "epoch": 1.8641581893205865,
      "grad_norm": 1.4457154273986816,
      "learning_rate": 1.8929323745187767e-05,
      "loss": 2.397,
      "step": 17110
    },
    {
      "epoch": 1.8652476474513489,
      "grad_norm": 1.426946997642517,
      "learning_rate": 1.8911164378586476e-05,
      "loss": 2.4272,
      "step": 17120
    },
    {
      "epoch": 1.8663371055821112,
      "grad_norm": 1.4975701570510864,
      "learning_rate": 1.889300501198518e-05,
      "loss": 2.4986,
      "step": 17130
    },
    {
      "epoch": 1.8674265637128733,
      "grad_norm": 1.4836195707321167,
      "learning_rate": 1.887484564538389e-05,
      "loss": 2.4504,
      "step": 17140
    },
    {
      "epoch": 1.8685160218436354,
      "grad_norm": 1.5054914951324463,
      "learning_rate": 1.88566862787826e-05,
      "loss": 2.5034,
      "step": 17150
    },
    {
      "epoch": 1.8696054799743977,
      "grad_norm": 1.3883516788482666,
      "learning_rate": 1.8838526912181304e-05,
      "loss": 2.4522,
      "step": 17160
    },
    {
      "epoch": 1.87069493810516,
      "grad_norm": 1.5148069858551025,
      "learning_rate": 1.8820367545580013e-05,
      "loss": 2.548,
      "step": 17170
    },
    {
      "epoch": 1.8717843962359222,
      "grad_norm": 1.4854400157928467,
      "learning_rate": 1.8802208178978715e-05,
      "loss": 2.4658,
      "step": 17180
    },
    {
      "epoch": 1.8728738543666843,
      "grad_norm": 1.4058164358139038,
      "learning_rate": 1.8784048812377424e-05,
      "loss": 2.4444,
      "step": 17190
    },
    {
      "epoch": 1.8739633124974466,
      "grad_norm": 1.5240633487701416,
      "learning_rate": 1.8765889445776133e-05,
      "loss": 2.4943,
      "step": 17200
    },
    {
      "epoch": 1.8750527706282087,
      "grad_norm": 1.468544363975525,
      "learning_rate": 1.8747730079174838e-05,
      "loss": 2.3926,
      "step": 17210
    },
    {
      "epoch": 1.876142228758971,
      "grad_norm": 1.4305636882781982,
      "learning_rate": 1.8729570712573547e-05,
      "loss": 2.4642,
      "step": 17220
    },
    {
      "epoch": 1.8772316868897332,
      "grad_norm": 1.3968247175216675,
      "learning_rate": 1.8711411345972256e-05,
      "loss": 2.582,
      "step": 17230
    },
    {
      "epoch": 1.8783211450204953,
      "grad_norm": 1.4926843643188477,
      "learning_rate": 1.869325197937096e-05,
      "loss": 2.5,
      "step": 17240
    },
    {
      "epoch": 1.8794106031512576,
      "grad_norm": 1.47026526927948,
      "learning_rate": 1.8675092612769667e-05,
      "loss": 2.4543,
      "step": 17250
    },
    {
      "epoch": 1.88050006128202,
      "grad_norm": 1.4438867568969727,
      "learning_rate": 1.8656933246168372e-05,
      "loss": 2.389,
      "step": 17260
    },
    {
      "epoch": 1.881589519412782,
      "grad_norm": 1.599938988685608,
      "learning_rate": 1.863877387956708e-05,
      "loss": 2.4202,
      "step": 17270
    },
    {
      "epoch": 1.8826789775435442,
      "grad_norm": 1.4730324745178223,
      "learning_rate": 1.862061451296579e-05,
      "loss": 2.4252,
      "step": 17280
    },
    {
      "epoch": 1.8837684356743065,
      "grad_norm": 1.451994776725769,
      "learning_rate": 1.8602455146364495e-05,
      "loss": 2.5134,
      "step": 17290
    },
    {
      "epoch": 1.8848578938050689,
      "grad_norm": 1.5447437763214111,
      "learning_rate": 1.8584295779763204e-05,
      "loss": 2.4133,
      "step": 17300
    },
    {
      "epoch": 1.885947351935831,
      "grad_norm": 1.426297903060913,
      "learning_rate": 1.856613641316191e-05,
      "loss": 2.4777,
      "step": 17310
    },
    {
      "epoch": 1.887036810066593,
      "grad_norm": 1.438359260559082,
      "learning_rate": 1.8547977046560615e-05,
      "loss": 2.3842,
      "step": 17320
    },
    {
      "epoch": 1.8881262681973552,
      "grad_norm": 1.4985554218292236,
      "learning_rate": 1.8529817679959324e-05,
      "loss": 2.4964,
      "step": 17330
    },
    {
      "epoch": 1.8892157263281175,
      "grad_norm": 1.4596790075302124,
      "learning_rate": 1.851165831335803e-05,
      "loss": 2.5008,
      "step": 17340
    },
    {
      "epoch": 1.8903051844588799,
      "grad_norm": 1.556438684463501,
      "learning_rate": 1.8493498946756738e-05,
      "loss": 2.4887,
      "step": 17350
    },
    {
      "epoch": 1.891394642589642,
      "grad_norm": 1.434031367301941,
      "learning_rate": 1.8475339580155447e-05,
      "loss": 2.4528,
      "step": 17360
    },
    {
      "epoch": 1.892484100720404,
      "grad_norm": 1.527809977531433,
      "learning_rate": 1.8457180213554152e-05,
      "loss": 2.5223,
      "step": 17370
    },
    {
      "epoch": 1.8935735588511664,
      "grad_norm": 1.4289880990982056,
      "learning_rate": 1.843902084695286e-05,
      "loss": 2.4261,
      "step": 17380
    },
    {
      "epoch": 1.8946630169819287,
      "grad_norm": 1.4504812955856323,
      "learning_rate": 1.8420861480351566e-05,
      "loss": 2.4373,
      "step": 17390
    },
    {
      "epoch": 1.8957524751126908,
      "grad_norm": 1.4965096712112427,
      "learning_rate": 1.840270211375027e-05,
      "loss": 2.3712,
      "step": 17400
    },
    {
      "epoch": 1.896841933243453,
      "grad_norm": 1.4920644760131836,
      "learning_rate": 1.838454274714898e-05,
      "loss": 2.4557,
      "step": 17410
    },
    {
      "epoch": 1.8979313913742153,
      "grad_norm": 1.4447546005249023,
      "learning_rate": 1.836638338054769e-05,
      "loss": 2.5098,
      "step": 17420
    },
    {
      "epoch": 1.8990208495049776,
      "grad_norm": 1.4414156675338745,
      "learning_rate": 1.8348224013946395e-05,
      "loss": 2.421,
      "step": 17430
    },
    {
      "epoch": 1.9001103076357397,
      "grad_norm": 1.4987297058105469,
      "learning_rate": 1.8330064647345103e-05,
      "loss": 2.3997,
      "step": 17440
    },
    {
      "epoch": 1.9011997657665018,
      "grad_norm": 1.5271574258804321,
      "learning_rate": 1.831190528074381e-05,
      "loss": 2.3819,
      "step": 17450
    },
    {
      "epoch": 1.902289223897264,
      "grad_norm": 1.3794350624084473,
      "learning_rate": 1.8293745914142514e-05,
      "loss": 2.4024,
      "step": 17460
    },
    {
      "epoch": 1.9033786820280263,
      "grad_norm": 1.4777414798736572,
      "learning_rate": 1.8275586547541223e-05,
      "loss": 2.3979,
      "step": 17470
    },
    {
      "epoch": 1.9044681401587886,
      "grad_norm": 1.5072603225708008,
      "learning_rate": 1.825742718093993e-05,
      "loss": 2.3563,
      "step": 17480
    },
    {
      "epoch": 1.9055575982895507,
      "grad_norm": 1.4437572956085205,
      "learning_rate": 1.8239267814338637e-05,
      "loss": 2.4719,
      "step": 17490
    },
    {
      "epoch": 1.9066470564203128,
      "grad_norm": 1.4728797674179077,
      "learning_rate": 1.8221108447737346e-05,
      "loss": 2.3993,
      "step": 17500
    },
    {
      "epoch": 1.9077365145510752,
      "grad_norm": 1.460519552230835,
      "learning_rate": 1.820294908113605e-05,
      "loss": 2.3261,
      "step": 17510
    },
    {
      "epoch": 1.9088259726818375,
      "grad_norm": 1.542036771774292,
      "learning_rate": 1.8184789714534757e-05,
      "loss": 2.4322,
      "step": 17520
    },
    {
      "epoch": 1.9099154308125996,
      "grad_norm": 1.5582005977630615,
      "learning_rate": 1.8166630347933462e-05,
      "loss": 2.4711,
      "step": 17530
    },
    {
      "epoch": 1.9110048889433617,
      "grad_norm": 1.4406898021697998,
      "learning_rate": 1.814847098133217e-05,
      "loss": 2.4697,
      "step": 17540
    },
    {
      "epoch": 1.9120943470741238,
      "grad_norm": 1.4636156558990479,
      "learning_rate": 1.813031161473088e-05,
      "loss": 2.4094,
      "step": 17550
    },
    {
      "epoch": 1.9131838052048862,
      "grad_norm": 1.50491201877594,
      "learning_rate": 1.8112152248129585e-05,
      "loss": 2.4254,
      "step": 17560
    },
    {
      "epoch": 1.9142732633356485,
      "grad_norm": 1.4901509284973145,
      "learning_rate": 1.8093992881528294e-05,
      "loss": 2.5197,
      "step": 17570
    },
    {
      "epoch": 1.9153627214664106,
      "grad_norm": 1.3900036811828613,
      "learning_rate": 1.8075833514927003e-05,
      "loss": 2.3476,
      "step": 17580
    },
    {
      "epoch": 1.9164521795971727,
      "grad_norm": 1.45760178565979,
      "learning_rate": 1.8057674148325705e-05,
      "loss": 2.4829,
      "step": 17590
    },
    {
      "epoch": 1.917541637727935,
      "grad_norm": 1.4955134391784668,
      "learning_rate": 1.8039514781724414e-05,
      "loss": 2.3875,
      "step": 17600
    },
    {
      "epoch": 1.9186310958586974,
      "grad_norm": 1.4443436861038208,
      "learning_rate": 1.802135541512312e-05,
      "loss": 2.3727,
      "step": 17610
    },
    {
      "epoch": 1.9197205539894595,
      "grad_norm": 1.5000404119491577,
      "learning_rate": 1.8003196048521828e-05,
      "loss": 2.3869,
      "step": 17620
    },
    {
      "epoch": 1.9208100121202216,
      "grad_norm": 1.5046011209487915,
      "learning_rate": 1.7985036681920537e-05,
      "loss": 2.4081,
      "step": 17630
    },
    {
      "epoch": 1.921899470250984,
      "grad_norm": 1.5170124769210815,
      "learning_rate": 1.7966877315319242e-05,
      "loss": 2.4968,
      "step": 17640
    },
    {
      "epoch": 1.9229889283817463,
      "grad_norm": 1.5542526245117188,
      "learning_rate": 1.794871794871795e-05,
      "loss": 2.4719,
      "step": 17650
    },
    {
      "epoch": 1.9240783865125084,
      "grad_norm": 1.4143600463867188,
      "learning_rate": 1.7930558582116657e-05,
      "loss": 2.3967,
      "step": 17660
    },
    {
      "epoch": 1.9251678446432705,
      "grad_norm": 1.383198618888855,
      "learning_rate": 1.7912399215515362e-05,
      "loss": 2.5052,
      "step": 17670
    },
    {
      "epoch": 1.9262573027740326,
      "grad_norm": 1.42786705493927,
      "learning_rate": 1.789423984891407e-05,
      "loss": 2.4025,
      "step": 17680
    },
    {
      "epoch": 1.927346760904795,
      "grad_norm": 1.4648566246032715,
      "learning_rate": 1.7876080482312776e-05,
      "loss": 2.4513,
      "step": 17690
    },
    {
      "epoch": 1.9284362190355573,
      "grad_norm": 1.500445008277893,
      "learning_rate": 1.7857921115711485e-05,
      "loss": 2.5357,
      "step": 17700
    },
    {
      "epoch": 1.9295256771663194,
      "grad_norm": 1.3945990800857544,
      "learning_rate": 1.7839761749110194e-05,
      "loss": 2.4333,
      "step": 17710
    },
    {
      "epoch": 1.9306151352970815,
      "grad_norm": 1.4302362203598022,
      "learning_rate": 1.78216023825089e-05,
      "loss": 2.4122,
      "step": 17720
    },
    {
      "epoch": 1.9317045934278438,
      "grad_norm": 1.503653645515442,
      "learning_rate": 1.7803443015907605e-05,
      "loss": 2.4366,
      "step": 17730
    },
    {
      "epoch": 1.9327940515586062,
      "grad_norm": 1.3999102115631104,
      "learning_rate": 1.7785283649306313e-05,
      "loss": 2.4047,
      "step": 17740
    },
    {
      "epoch": 1.9338835096893683,
      "grad_norm": 1.4925163984298706,
      "learning_rate": 1.776712428270502e-05,
      "loss": 2.4777,
      "step": 17750
    },
    {
      "epoch": 1.9349729678201304,
      "grad_norm": 1.6717942953109741,
      "learning_rate": 1.7748964916103728e-05,
      "loss": 2.5019,
      "step": 17760
    },
    {
      "epoch": 1.9360624259508927,
      "grad_norm": 1.3988608121871948,
      "learning_rate": 1.7730805549502436e-05,
      "loss": 2.4013,
      "step": 17770
    },
    {
      "epoch": 1.9371518840816548,
      "grad_norm": 1.418831467628479,
      "learning_rate": 1.7712646182901142e-05,
      "loss": 2.452,
      "step": 17780
    },
    {
      "epoch": 1.9382413422124172,
      "grad_norm": 1.4904457330703735,
      "learning_rate": 1.7694486816299847e-05,
      "loss": 2.4391,
      "step": 17790
    },
    {
      "epoch": 1.9393308003431793,
      "grad_norm": 1.5055797100067139,
      "learning_rate": 1.7676327449698553e-05,
      "loss": 2.4066,
      "step": 17800
    },
    {
      "epoch": 1.9404202584739414,
      "grad_norm": 1.51618492603302,
      "learning_rate": 1.765816808309726e-05,
      "loss": 2.5218,
      "step": 17810
    },
    {
      "epoch": 1.9415097166047037,
      "grad_norm": 1.5430024862289429,
      "learning_rate": 1.764000871649597e-05,
      "loss": 2.4576,
      "step": 17820
    },
    {
      "epoch": 1.942599174735466,
      "grad_norm": 1.3841907978057861,
      "learning_rate": 1.7621849349894676e-05,
      "loss": 2.5006,
      "step": 17830
    },
    {
      "epoch": 1.9436886328662282,
      "grad_norm": 1.5215346813201904,
      "learning_rate": 1.7603689983293385e-05,
      "loss": 2.4666,
      "step": 17840
    },
    {
      "epoch": 1.9447780909969903,
      "grad_norm": 1.5056535005569458,
      "learning_rate": 1.7585530616692093e-05,
      "loss": 2.452,
      "step": 17850
    },
    {
      "epoch": 1.9458675491277526,
      "grad_norm": 1.4581310749053955,
      "learning_rate": 1.75673712500908e-05,
      "loss": 2.4293,
      "step": 17860
    },
    {
      "epoch": 1.946957007258515,
      "grad_norm": 1.625110149383545,
      "learning_rate": 1.7549211883489504e-05,
      "loss": 2.5079,
      "step": 17870
    },
    {
      "epoch": 1.948046465389277,
      "grad_norm": 1.4895063638687134,
      "learning_rate": 1.753105251688821e-05,
      "loss": 2.4013,
      "step": 17880
    },
    {
      "epoch": 1.9491359235200392,
      "grad_norm": 1.384500503540039,
      "learning_rate": 1.751289315028692e-05,
      "loss": 2.4453,
      "step": 17890
    },
    {
      "epoch": 1.9502253816508013,
      "grad_norm": 1.6574143171310425,
      "learning_rate": 1.7494733783685627e-05,
      "loss": 2.5643,
      "step": 17900
    },
    {
      "epoch": 1.9513148397815636,
      "grad_norm": 1.4655359983444214,
      "learning_rate": 1.7476574417084333e-05,
      "loss": 2.4377,
      "step": 17910
    },
    {
      "epoch": 1.952404297912326,
      "grad_norm": 1.5794730186462402,
      "learning_rate": 1.745841505048304e-05,
      "loss": 2.4888,
      "step": 17920
    },
    {
      "epoch": 1.953493756043088,
      "grad_norm": 1.5230356454849243,
      "learning_rate": 1.7440255683881747e-05,
      "loss": 2.4167,
      "step": 17930
    },
    {
      "epoch": 1.9545832141738502,
      "grad_norm": 1.4132874011993408,
      "learning_rate": 1.7422096317280452e-05,
      "loss": 2.4545,
      "step": 17940
    },
    {
      "epoch": 1.9556726723046125,
      "grad_norm": 1.5508960485458374,
      "learning_rate": 1.740393695067916e-05,
      "loss": 2.5281,
      "step": 17950
    },
    {
      "epoch": 1.9567621304353748,
      "grad_norm": 1.4609812498092651,
      "learning_rate": 1.7385777584077867e-05,
      "loss": 2.5061,
      "step": 17960
    },
    {
      "epoch": 1.957851588566137,
      "grad_norm": 1.5346665382385254,
      "learning_rate": 1.7367618217476575e-05,
      "loss": 2.3943,
      "step": 17970
    },
    {
      "epoch": 1.958941046696899,
      "grad_norm": 1.4524791240692139,
      "learning_rate": 1.7349458850875284e-05,
      "loss": 2.4558,
      "step": 17980
    },
    {
      "epoch": 1.9600305048276614,
      "grad_norm": 1.4426056146621704,
      "learning_rate": 1.733129948427399e-05,
      "loss": 2.4741,
      "step": 17990
    },
    {
      "epoch": 1.9611199629584235,
      "grad_norm": 1.5238152742385864,
      "learning_rate": 1.7313140117672695e-05,
      "loss": 2.3799,
      "step": 18000
    },
    {
      "epoch": 1.9622094210891858,
      "grad_norm": 1.4784942865371704,
      "learning_rate": 1.7294980751071404e-05,
      "loss": 2.3432,
      "step": 18010
    },
    {
      "epoch": 1.963298879219948,
      "grad_norm": 1.502233862876892,
      "learning_rate": 1.727682138447011e-05,
      "loss": 2.4247,
      "step": 18020
    },
    {
      "epoch": 1.96438833735071,
      "grad_norm": 1.4511805772781372,
      "learning_rate": 1.7258662017868818e-05,
      "loss": 2.4119,
      "step": 18030
    },
    {
      "epoch": 1.9654777954814724,
      "grad_norm": 1.474446415901184,
      "learning_rate": 1.7240502651267523e-05,
      "loss": 2.4195,
      "step": 18040
    },
    {
      "epoch": 1.9665672536122347,
      "grad_norm": 1.453603744506836,
      "learning_rate": 1.7222343284666232e-05,
      "loss": 2.5223,
      "step": 18050
    },
    {
      "epoch": 1.9676567117429968,
      "grad_norm": 1.5597366094589233,
      "learning_rate": 1.720418391806494e-05,
      "loss": 2.4823,
      "step": 18060
    },
    {
      "epoch": 1.968746169873759,
      "grad_norm": 1.3359277248382568,
      "learning_rate": 1.7186024551463646e-05,
      "loss": 2.4503,
      "step": 18070
    },
    {
      "epoch": 1.9698356280045213,
      "grad_norm": 1.4452842473983765,
      "learning_rate": 1.7167865184862352e-05,
      "loss": 2.4585,
      "step": 18080
    },
    {
      "epoch": 1.9709250861352836,
      "grad_norm": 1.4467729330062866,
      "learning_rate": 1.714970581826106e-05,
      "loss": 2.4789,
      "step": 18090
    },
    {
      "epoch": 1.9720145442660457,
      "grad_norm": 1.6056058406829834,
      "learning_rate": 1.7131546451659766e-05,
      "loss": 2.3812,
      "step": 18100
    },
    {
      "epoch": 1.9731040023968078,
      "grad_norm": 1.4537580013275146,
      "learning_rate": 1.7113387085058475e-05,
      "loss": 2.4902,
      "step": 18110
    },
    {
      "epoch": 1.97419346052757,
      "grad_norm": 1.41109299659729,
      "learning_rate": 1.7095227718457184e-05,
      "loss": 2.4368,
      "step": 18120
    },
    {
      "epoch": 1.9752829186583323,
      "grad_norm": 1.4241540431976318,
      "learning_rate": 1.707706835185589e-05,
      "loss": 2.5717,
      "step": 18130
    },
    {
      "epoch": 1.9763723767890946,
      "grad_norm": 1.5369584560394287,
      "learning_rate": 1.7058908985254595e-05,
      "loss": 2.5479,
      "step": 18140
    },
    {
      "epoch": 1.9774618349198567,
      "grad_norm": 1.5268784761428833,
      "learning_rate": 1.70407496186533e-05,
      "loss": 2.3799,
      "step": 18150
    },
    {
      "epoch": 1.9785512930506188,
      "grad_norm": 1.47007417678833,
      "learning_rate": 1.702259025205201e-05,
      "loss": 2.5789,
      "step": 18160
    },
    {
      "epoch": 1.9796407511813812,
      "grad_norm": 1.508123755455017,
      "learning_rate": 1.7004430885450718e-05,
      "loss": 2.4726,
      "step": 18170
    },
    {
      "epoch": 1.9807302093121435,
      "grad_norm": 1.3903752565383911,
      "learning_rate": 1.6986271518849423e-05,
      "loss": 2.4664,
      "step": 18180
    },
    {
      "epoch": 1.9818196674429056,
      "grad_norm": 1.4216015338897705,
      "learning_rate": 1.6968112152248132e-05,
      "loss": 2.4297,
      "step": 18190
    },
    {
      "epoch": 1.9829091255736677,
      "grad_norm": 1.4947752952575684,
      "learning_rate": 1.6949952785646837e-05,
      "loss": 2.4242,
      "step": 18200
    },
    {
      "epoch": 1.98399858370443,
      "grad_norm": 1.5177427530288696,
      "learning_rate": 1.6931793419045543e-05,
      "loss": 2.4814,
      "step": 18210
    },
    {
      "epoch": 1.9850880418351924,
      "grad_norm": 1.579757571220398,
      "learning_rate": 1.691363405244425e-05,
      "loss": 2.4693,
      "step": 18220
    },
    {
      "epoch": 1.9861774999659545,
      "grad_norm": 1.5287350416183472,
      "learning_rate": 1.6895474685842957e-05,
      "loss": 2.5385,
      "step": 18230
    },
    {
      "epoch": 1.9872669580967166,
      "grad_norm": 1.4693660736083984,
      "learning_rate": 1.6877315319241666e-05,
      "loss": 2.4767,
      "step": 18240
    },
    {
      "epoch": 1.9883564162274787,
      "grad_norm": 1.4830461740493774,
      "learning_rate": 1.6859155952640374e-05,
      "loss": 2.4285,
      "step": 18250
    },
    {
      "epoch": 1.989445874358241,
      "grad_norm": 1.463416337966919,
      "learning_rate": 1.684099658603908e-05,
      "loss": 2.4285,
      "step": 18260
    },
    {
      "epoch": 1.9905353324890034,
      "grad_norm": 1.432917594909668,
      "learning_rate": 1.682283721943779e-05,
      "loss": 2.4945,
      "step": 18270
    },
    {
      "epoch": 1.9916247906197655,
      "grad_norm": 1.5431946516036987,
      "learning_rate": 1.6804677852836494e-05,
      "loss": 2.4714,
      "step": 18280
    },
    {
      "epoch": 1.9927142487505276,
      "grad_norm": 1.4957914352416992,
      "learning_rate": 1.67865184862352e-05,
      "loss": 2.474,
      "step": 18290
    },
    {
      "epoch": 1.99380370688129,
      "grad_norm": 1.4633785486221313,
      "learning_rate": 1.676835911963391e-05,
      "loss": 2.4695,
      "step": 18300
    },
    {
      "epoch": 1.9948931650120523,
      "grad_norm": 1.5041176080703735,
      "learning_rate": 1.6750199753032614e-05,
      "loss": 2.4151,
      "step": 18310
    },
    {
      "epoch": 1.9959826231428144,
      "grad_norm": 1.4215714931488037,
      "learning_rate": 1.6732040386431323e-05,
      "loss": 2.4281,
      "step": 18320
    },
    {
      "epoch": 1.9970720812735765,
      "grad_norm": 1.5038046836853027,
      "learning_rate": 1.671388101983003e-05,
      "loss": 2.4312,
      "step": 18330
    },
    {
      "epoch": 1.9981615394043386,
      "grad_norm": 1.494828462600708,
      "learning_rate": 1.6695721653228737e-05,
      "loss": 2.4356,
      "step": 18340
    },
    {
      "epoch": 1.999250997535101,
      "grad_norm": 1.4441865682601929,
      "learning_rate": 1.6677562286627442e-05,
      "loss": 2.4059,
      "step": 18350
    },
    {
      "epoch": 2.0003404556658633,
      "grad_norm": 1.457434058189392,
      "learning_rate": 1.665940292002615e-05,
      "loss": 2.5137,
      "step": 18360
    },
    {
      "epoch": 2.0014299137966254,
      "grad_norm": 1.5337945222854614,
      "learning_rate": 1.6641243553424856e-05,
      "loss": 2.3826,
      "step": 18370
    },
    {
      "epoch": 2.0025193719273875,
      "grad_norm": 1.466744065284729,
      "learning_rate": 1.6623084186823565e-05,
      "loss": 2.4151,
      "step": 18380
    },
    {
      "epoch": 2.00360883005815,
      "grad_norm": 1.4753808975219727,
      "learning_rate": 1.660492482022227e-05,
      "loss": 2.4657,
      "step": 18390
    },
    {
      "epoch": 2.004698288188912,
      "grad_norm": 1.4131313562393188,
      "learning_rate": 1.658676545362098e-05,
      "loss": 2.4126,
      "step": 18400
    },
    {
      "epoch": 2.0057877463196743,
      "grad_norm": 1.5125964879989624,
      "learning_rate": 1.6568606087019685e-05,
      "loss": 2.466,
      "step": 18410
    },
    {
      "epoch": 2.0068772044504364,
      "grad_norm": 1.5634756088256836,
      "learning_rate": 1.655044672041839e-05,
      "loss": 2.3613,
      "step": 18420
    },
    {
      "epoch": 2.0079666625811985,
      "grad_norm": 1.435622215270996,
      "learning_rate": 1.65322873538171e-05,
      "loss": 2.4416,
      "step": 18430
    },
    {
      "epoch": 2.009056120711961,
      "grad_norm": 1.5282841920852661,
      "learning_rate": 1.6514127987215808e-05,
      "loss": 2.5045,
      "step": 18440
    },
    {
      "epoch": 2.010145578842723,
      "grad_norm": 1.4619536399841309,
      "learning_rate": 1.6495968620614513e-05,
      "loss": 2.4186,
      "step": 18450
    },
    {
      "epoch": 2.0112350369734853,
      "grad_norm": 1.5271971225738525,
      "learning_rate": 1.6477809254013222e-05,
      "loss": 2.5426,
      "step": 18460
    },
    {
      "epoch": 2.0123244951042474,
      "grad_norm": 1.4674339294433594,
      "learning_rate": 1.6459649887411928e-05,
      "loss": 2.479,
      "step": 18470
    },
    {
      "epoch": 2.01341395323501,
      "grad_norm": 1.516021966934204,
      "learning_rate": 1.6441490520810633e-05,
      "loss": 2.4516,
      "step": 18480
    },
    {
      "epoch": 2.014503411365772,
      "grad_norm": 1.463405728340149,
      "learning_rate": 1.6423331154209342e-05,
      "loss": 2.4298,
      "step": 18490
    },
    {
      "epoch": 2.015592869496534,
      "grad_norm": 1.4355239868164062,
      "learning_rate": 1.6405171787608047e-05,
      "loss": 2.4176,
      "step": 18500
    },
    {
      "epoch": 2.0166823276272963,
      "grad_norm": 1.416215419769287,
      "learning_rate": 1.6387012421006756e-05,
      "loss": 2.414,
      "step": 18510
    },
    {
      "epoch": 2.0177717857580584,
      "grad_norm": 1.481403112411499,
      "learning_rate": 1.6368853054405465e-05,
      "loss": 2.4232,
      "step": 18520
    },
    {
      "epoch": 2.018861243888821,
      "grad_norm": 1.479982614517212,
      "learning_rate": 1.635069368780417e-05,
      "loss": 2.4452,
      "step": 18530
    },
    {
      "epoch": 2.019950702019583,
      "grad_norm": 1.4293774366378784,
      "learning_rate": 1.633253432120288e-05,
      "loss": 2.4192,
      "step": 18540
    },
    {
      "epoch": 2.021040160150345,
      "grad_norm": 1.6473979949951172,
      "learning_rate": 1.6314374954601584e-05,
      "loss": 2.5678,
      "step": 18550
    },
    {
      "epoch": 2.0221296182811073,
      "grad_norm": 1.3784996271133423,
      "learning_rate": 1.629621558800029e-05,
      "loss": 2.4212,
      "step": 18560
    },
    {
      "epoch": 2.02321907641187,
      "grad_norm": 1.4470536708831787,
      "learning_rate": 1.6278056221399e-05,
      "loss": 2.3248,
      "step": 18570
    },
    {
      "epoch": 2.024308534542632,
      "grad_norm": 1.5034255981445312,
      "learning_rate": 1.6259896854797704e-05,
      "loss": 2.4812,
      "step": 18580
    },
    {
      "epoch": 2.025397992673394,
      "grad_norm": 1.428780198097229,
      "learning_rate": 1.6241737488196413e-05,
      "loss": 2.4231,
      "step": 18590
    },
    {
      "epoch": 2.026487450804156,
      "grad_norm": 1.4205082654953003,
      "learning_rate": 1.622357812159512e-05,
      "loss": 2.4521,
      "step": 18600
    },
    {
      "epoch": 2.0275769089349187,
      "grad_norm": 1.5195361375808716,
      "learning_rate": 1.6205418754993827e-05,
      "loss": 2.4622,
      "step": 18610
    },
    {
      "epoch": 2.028666367065681,
      "grad_norm": 1.524510145187378,
      "learning_rate": 1.6187259388392533e-05,
      "loss": 2.4426,
      "step": 18620
    },
    {
      "epoch": 2.029755825196443,
      "grad_norm": 1.496424913406372,
      "learning_rate": 1.616910002179124e-05,
      "loss": 2.5262,
      "step": 18630
    },
    {
      "epoch": 2.030845283327205,
      "grad_norm": 1.5071609020233154,
      "learning_rate": 1.6150940655189947e-05,
      "loss": 2.4176,
      "step": 18640
    },
    {
      "epoch": 2.031934741457967,
      "grad_norm": 1.5115848779678345,
      "learning_rate": 1.6132781288588656e-05,
      "loss": 2.4972,
      "step": 18650
    },
    {
      "epoch": 2.0330241995887297,
      "grad_norm": 1.5389244556427002,
      "learning_rate": 1.611462192198736e-05,
      "loss": 2.3569,
      "step": 18660
    },
    {
      "epoch": 2.034113657719492,
      "grad_norm": 1.382161259651184,
      "learning_rate": 1.609646255538607e-05,
      "loss": 2.2933,
      "step": 18670
    },
    {
      "epoch": 2.035203115850254,
      "grad_norm": 1.538906455039978,
      "learning_rate": 1.6078303188784775e-05,
      "loss": 2.4479,
      "step": 18680
    },
    {
      "epoch": 2.036292573981016,
      "grad_norm": 1.4962786436080933,
      "learning_rate": 1.606014382218348e-05,
      "loss": 2.4019,
      "step": 18690
    },
    {
      "epoch": 2.0373820321117786,
      "grad_norm": 1.5067027807235718,
      "learning_rate": 1.604198445558219e-05,
      "loss": 2.5123,
      "step": 18700
    },
    {
      "epoch": 2.0384714902425407,
      "grad_norm": 1.5170080661773682,
      "learning_rate": 1.6023825088980898e-05,
      "loss": 2.3465,
      "step": 18710
    },
    {
      "epoch": 2.039560948373303,
      "grad_norm": 1.4645215272903442,
      "learning_rate": 1.6005665722379604e-05,
      "loss": 2.384,
      "step": 18720
    },
    {
      "epoch": 2.040650406504065,
      "grad_norm": 1.4738458395004272,
      "learning_rate": 1.5987506355778312e-05,
      "loss": 2.5139,
      "step": 18730
    },
    {
      "epoch": 2.041739864634827,
      "grad_norm": 1.5130096673965454,
      "learning_rate": 1.5969346989177018e-05,
      "loss": 2.443,
      "step": 18740
    },
    {
      "epoch": 2.0428293227655896,
      "grad_norm": 1.492945671081543,
      "learning_rate": 1.5951187622575727e-05,
      "loss": 2.4348,
      "step": 18750
    },
    {
      "epoch": 2.0439187808963517,
      "grad_norm": 1.5375622510910034,
      "learning_rate": 1.5933028255974432e-05,
      "loss": 2.4472,
      "step": 18760
    },
    {
      "epoch": 2.045008239027114,
      "grad_norm": 1.4552277326583862,
      "learning_rate": 1.5914868889373137e-05,
      "loss": 2.369,
      "step": 18770
    },
    {
      "epoch": 2.046097697157876,
      "grad_norm": 1.4601857662200928,
      "learning_rate": 1.5896709522771846e-05,
      "loss": 2.3619,
      "step": 18780
    },
    {
      "epoch": 2.0471871552886385,
      "grad_norm": 1.5195164680480957,
      "learning_rate": 1.5878550156170555e-05,
      "loss": 2.4782,
      "step": 18790
    },
    {
      "epoch": 2.0482766134194006,
      "grad_norm": 1.4620275497436523,
      "learning_rate": 1.586039078956926e-05,
      "loss": 2.5,
      "step": 18800
    },
    {
      "epoch": 2.0493660715501627,
      "grad_norm": 1.5180033445358276,
      "learning_rate": 1.584223142296797e-05,
      "loss": 2.4603,
      "step": 18810
    },
    {
      "epoch": 2.050455529680925,
      "grad_norm": 1.5089306831359863,
      "learning_rate": 1.5824072056366675e-05,
      "loss": 2.4227,
      "step": 18820
    },
    {
      "epoch": 2.0515449878116874,
      "grad_norm": 1.4805842638015747,
      "learning_rate": 1.580591268976538e-05,
      "loss": 2.3722,
      "step": 18830
    },
    {
      "epoch": 2.0526344459424495,
      "grad_norm": 1.4835950136184692,
      "learning_rate": 1.578775332316409e-05,
      "loss": 2.3965,
      "step": 18840
    },
    {
      "epoch": 2.0537239040732116,
      "grad_norm": 1.5510390996932983,
      "learning_rate": 1.5769593956562794e-05,
      "loss": 2.4352,
      "step": 18850
    },
    {
      "epoch": 2.0548133622039737,
      "grad_norm": 1.5293850898742676,
      "learning_rate": 1.5751434589961503e-05,
      "loss": 2.4311,
      "step": 18860
    },
    {
      "epoch": 2.055902820334736,
      "grad_norm": 1.5231202840805054,
      "learning_rate": 1.5733275223360212e-05,
      "loss": 2.4569,
      "step": 18870
    },
    {
      "epoch": 2.0569922784654984,
      "grad_norm": 1.4409974813461304,
      "learning_rate": 1.5715115856758917e-05,
      "loss": 2.4465,
      "step": 18880
    },
    {
      "epoch": 2.0580817365962605,
      "grad_norm": 1.5793665647506714,
      "learning_rate": 1.5696956490157623e-05,
      "loss": 2.4864,
      "step": 18890
    },
    {
      "epoch": 2.0591711947270226,
      "grad_norm": 1.460248351097107,
      "learning_rate": 1.567879712355633e-05,
      "loss": 2.4707,
      "step": 18900
    },
    {
      "epoch": 2.0602606528577847,
      "grad_norm": 1.5825352668762207,
      "learning_rate": 1.5660637756955037e-05,
      "loss": 2.4486,
      "step": 18910
    },
    {
      "epoch": 2.0613501109885473,
      "grad_norm": 1.4409667253494263,
      "learning_rate": 1.5642478390353746e-05,
      "loss": 2.3051,
      "step": 18920
    },
    {
      "epoch": 2.0624395691193094,
      "grad_norm": 1.5524300336837769,
      "learning_rate": 1.562431902375245e-05,
      "loss": 2.4637,
      "step": 18930
    },
    {
      "epoch": 2.0635290272500715,
      "grad_norm": 1.4542431831359863,
      "learning_rate": 1.560615965715116e-05,
      "loss": 2.4348,
      "step": 18940
    },
    {
      "epoch": 2.0646184853808336,
      "grad_norm": 1.4467180967330933,
      "learning_rate": 1.558800029054987e-05,
      "loss": 2.3999,
      "step": 18950
    },
    {
      "epoch": 2.0657079435115957,
      "grad_norm": 1.4296127557754517,
      "learning_rate": 1.5569840923948574e-05,
      "loss": 2.4042,
      "step": 18960
    },
    {
      "epoch": 2.0667974016423583,
      "grad_norm": 1.4603426456451416,
      "learning_rate": 1.555168155734728e-05,
      "loss": 2.4392,
      "step": 18970
    },
    {
      "epoch": 2.0678868597731204,
      "grad_norm": 1.4610151052474976,
      "learning_rate": 1.553352219074599e-05,
      "loss": 2.4111,
      "step": 18980
    },
    {
      "epoch": 2.0689763179038825,
      "grad_norm": 1.5752527713775635,
      "learning_rate": 1.5515362824144694e-05,
      "loss": 2.4424,
      "step": 18990
    },
    {
      "epoch": 2.0700657760346446,
      "grad_norm": 1.4612364768981934,
      "learning_rate": 1.5497203457543403e-05,
      "loss": 2.4352,
      "step": 19000
    },
    {
      "epoch": 2.071155234165407,
      "grad_norm": 1.6381580829620361,
      "learning_rate": 1.5479044090942108e-05,
      "loss": 2.3873,
      "step": 19010
    },
    {
      "epoch": 2.0722446922961693,
      "grad_norm": 1.5130032300949097,
      "learning_rate": 1.5460884724340817e-05,
      "loss": 2.4695,
      "step": 19020
    },
    {
      "epoch": 2.0733341504269314,
      "grad_norm": 1.4114930629730225,
      "learning_rate": 1.5442725357739522e-05,
      "loss": 2.567,
      "step": 19030
    },
    {
      "epoch": 2.0744236085576935,
      "grad_norm": 1.5985051393508911,
      "learning_rate": 1.5424565991138228e-05,
      "loss": 2.4412,
      "step": 19040
    },
    {
      "epoch": 2.075513066688456,
      "grad_norm": 1.5285500288009644,
      "learning_rate": 1.5406406624536937e-05,
      "loss": 2.3857,
      "step": 19050
    },
    {
      "epoch": 2.076602524819218,
      "grad_norm": 1.6281931400299072,
      "learning_rate": 1.5388247257935645e-05,
      "loss": 2.4745,
      "step": 19060
    },
    {
      "epoch": 2.0776919829499803,
      "grad_norm": 1.5193641185760498,
      "learning_rate": 1.537008789133435e-05,
      "loss": 2.457,
      "step": 19070
    },
    {
      "epoch": 2.0787814410807424,
      "grad_norm": 1.56688392162323,
      "learning_rate": 1.535192852473306e-05,
      "loss": 2.4564,
      "step": 19080
    },
    {
      "epoch": 2.0798708992115045,
      "grad_norm": 1.4078646898269653,
      "learning_rate": 1.5333769158131765e-05,
      "loss": 2.3997,
      "step": 19090
    },
    {
      "epoch": 2.080960357342267,
      "grad_norm": 1.5043991804122925,
      "learning_rate": 1.531560979153047e-05,
      "loss": 2.4239,
      "step": 19100
    },
    {
      "epoch": 2.082049815473029,
      "grad_norm": 1.5228114128112793,
      "learning_rate": 1.529745042492918e-05,
      "loss": 2.4814,
      "step": 19110
    },
    {
      "epoch": 2.0831392736037913,
      "grad_norm": 1.446073055267334,
      "learning_rate": 1.5279291058327885e-05,
      "loss": 2.3954,
      "step": 19120
    },
    {
      "epoch": 2.0842287317345534,
      "grad_norm": 1.586349606513977,
      "learning_rate": 1.5261131691726594e-05,
      "loss": 2.4338,
      "step": 19130
    },
    {
      "epoch": 2.085318189865316,
      "grad_norm": 1.549552321434021,
      "learning_rate": 1.5242972325125302e-05,
      "loss": 2.4824,
      "step": 19140
    },
    {
      "epoch": 2.086407647996078,
      "grad_norm": 1.628705382347107,
      "learning_rate": 1.5224812958524006e-05,
      "loss": 2.3536,
      "step": 19150
    },
    {
      "epoch": 2.08749710612684,
      "grad_norm": 1.412482500076294,
      "learning_rate": 1.5206653591922715e-05,
      "loss": 2.3696,
      "step": 19160
    },
    {
      "epoch": 2.0885865642576023,
      "grad_norm": 1.5010946989059448,
      "learning_rate": 1.518849422532142e-05,
      "loss": 2.4032,
      "step": 19170
    },
    {
      "epoch": 2.089676022388365,
      "grad_norm": 1.5070496797561646,
      "learning_rate": 1.5170334858720129e-05,
      "loss": 2.5778,
      "step": 19180
    },
    {
      "epoch": 2.090765480519127,
      "grad_norm": 1.5830202102661133,
      "learning_rate": 1.5152175492118836e-05,
      "loss": 2.5132,
      "step": 19190
    },
    {
      "epoch": 2.091854938649889,
      "grad_norm": 1.5589747428894043,
      "learning_rate": 1.5134016125517542e-05,
      "loss": 2.5269,
      "step": 19200
    },
    {
      "epoch": 2.092944396780651,
      "grad_norm": 1.5360493659973145,
      "learning_rate": 1.511585675891625e-05,
      "loss": 2.3871,
      "step": 19210
    },
    {
      "epoch": 2.0940338549114133,
      "grad_norm": 1.4512217044830322,
      "learning_rate": 1.5097697392314958e-05,
      "loss": 2.4083,
      "step": 19220
    },
    {
      "epoch": 2.095123313042176,
      "grad_norm": 1.4948475360870361,
      "learning_rate": 1.5079538025713663e-05,
      "loss": 2.5107,
      "step": 19230
    },
    {
      "epoch": 2.096212771172938,
      "grad_norm": 1.4782992601394653,
      "learning_rate": 1.5061378659112372e-05,
      "loss": 2.4307,
      "step": 19240
    },
    {
      "epoch": 2.0973022293037,
      "grad_norm": 1.484844446182251,
      "learning_rate": 1.5043219292511079e-05,
      "loss": 2.4072,
      "step": 19250
    },
    {
      "epoch": 2.098391687434462,
      "grad_norm": 1.4639525413513184,
      "learning_rate": 1.5025059925909784e-05,
      "loss": 2.4259,
      "step": 19260
    },
    {
      "epoch": 2.0994811455652247,
      "grad_norm": 1.5042543411254883,
      "learning_rate": 1.5006900559308493e-05,
      "loss": 2.3599,
      "step": 19270
    },
    {
      "epoch": 2.100570603695987,
      "grad_norm": 1.4622734785079956,
      "learning_rate": 1.4988741192707198e-05,
      "loss": 2.3945,
      "step": 19280
    },
    {
      "epoch": 2.101660061826749,
      "grad_norm": 1.4568313360214233,
      "learning_rate": 1.4970581826105906e-05,
      "loss": 2.4416,
      "step": 19290
    },
    {
      "epoch": 2.102749519957511,
      "grad_norm": 1.4734596014022827,
      "learning_rate": 1.4952422459504614e-05,
      "loss": 2.4218,
      "step": 19300
    },
    {
      "epoch": 2.103838978088273,
      "grad_norm": 1.4782627820968628,
      "learning_rate": 1.493426309290332e-05,
      "loss": 2.4265,
      "step": 19310
    },
    {
      "epoch": 2.1049284362190357,
      "grad_norm": 1.463536024093628,
      "learning_rate": 1.4916103726302027e-05,
      "loss": 2.3181,
      "step": 19320
    },
    {
      "epoch": 2.106017894349798,
      "grad_norm": 1.512281060218811,
      "learning_rate": 1.4897944359700736e-05,
      "loss": 2.4156,
      "step": 19330
    },
    {
      "epoch": 2.10710735248056,
      "grad_norm": 1.6408145427703857,
      "learning_rate": 1.4879784993099441e-05,
      "loss": 2.4452,
      "step": 19340
    },
    {
      "epoch": 2.108196810611322,
      "grad_norm": 1.524539828300476,
      "learning_rate": 1.4861625626498148e-05,
      "loss": 2.3661,
      "step": 19350
    },
    {
      "epoch": 2.1092862687420846,
      "grad_norm": 1.5190051794052124,
      "learning_rate": 1.4843466259896854e-05,
      "loss": 2.5189,
      "step": 19360
    },
    {
      "epoch": 2.1103757268728467,
      "grad_norm": 1.5185060501098633,
      "learning_rate": 1.4825306893295562e-05,
      "loss": 2.356,
      "step": 19370
    },
    {
      "epoch": 2.111465185003609,
      "grad_norm": 1.4749258756637573,
      "learning_rate": 1.4807147526694271e-05,
      "loss": 2.418,
      "step": 19380
    },
    {
      "epoch": 2.112554643134371,
      "grad_norm": 1.464815616607666,
      "learning_rate": 1.4788988160092975e-05,
      "loss": 2.4124,
      "step": 19390
    },
    {
      "epoch": 2.1136441012651335,
      "grad_norm": 1.4144775867462158,
      "learning_rate": 1.4770828793491684e-05,
      "loss": 2.5081,
      "step": 19400
    },
    {
      "epoch": 2.1147335593958956,
      "grad_norm": 1.4885995388031006,
      "learning_rate": 1.4752669426890393e-05,
      "loss": 2.4948,
      "step": 19410
    },
    {
      "epoch": 2.1158230175266577,
      "grad_norm": 1.4360463619232178,
      "learning_rate": 1.4734510060289098e-05,
      "loss": 2.4485,
      "step": 19420
    },
    {
      "epoch": 2.11691247565742,
      "grad_norm": 1.5679303407669067,
      "learning_rate": 1.4716350693687805e-05,
      "loss": 2.4222,
      "step": 19430
    },
    {
      "epoch": 2.118001933788182,
      "grad_norm": 1.5326744318008423,
      "learning_rate": 1.469819132708651e-05,
      "loss": 2.455,
      "step": 19440
    },
    {
      "epoch": 2.1190913919189445,
      "grad_norm": 1.5093529224395752,
      "learning_rate": 1.468003196048522e-05,
      "loss": 2.4906,
      "step": 19450
    },
    {
      "epoch": 2.1201808500497066,
      "grad_norm": 1.4173035621643066,
      "learning_rate": 1.4661872593883926e-05,
      "loss": 2.4615,
      "step": 19460
    },
    {
      "epoch": 2.1212703081804687,
      "grad_norm": 1.4549003839492798,
      "learning_rate": 1.4643713227282632e-05,
      "loss": 2.3624,
      "step": 19470
    },
    {
      "epoch": 2.122359766311231,
      "grad_norm": 1.4324613809585571,
      "learning_rate": 1.462555386068134e-05,
      "loss": 2.3851,
      "step": 19480
    },
    {
      "epoch": 2.1234492244419934,
      "grad_norm": 1.5017622709274292,
      "learning_rate": 1.4607394494080048e-05,
      "loss": 2.5584,
      "step": 19490
    },
    {
      "epoch": 2.1245386825727555,
      "grad_norm": 1.451872706413269,
      "learning_rate": 1.4589235127478753e-05,
      "loss": 2.3667,
      "step": 19500
    },
    {
      "epoch": 2.1256281407035176,
      "grad_norm": 1.5117638111114502,
      "learning_rate": 1.4571075760877462e-05,
      "loss": 2.4099,
      "step": 19510
    },
    {
      "epoch": 2.1267175988342797,
      "grad_norm": 1.4536089897155762,
      "learning_rate": 1.4552916394276167e-05,
      "loss": 2.4259,
      "step": 19520
    },
    {
      "epoch": 2.1278070569650422,
      "grad_norm": 1.4646795988082886,
      "learning_rate": 1.4534757027674875e-05,
      "loss": 2.4398,
      "step": 19530
    },
    {
      "epoch": 2.1288965150958044,
      "grad_norm": 1.5041245222091675,
      "learning_rate": 1.4516597661073583e-05,
      "loss": 2.4309,
      "step": 19540
    },
    {
      "epoch": 2.1299859732265665,
      "grad_norm": 1.4697059392929077,
      "learning_rate": 1.4498438294472289e-05,
      "loss": 2.4446,
      "step": 19550
    },
    {
      "epoch": 2.1310754313573286,
      "grad_norm": 1.4246695041656494,
      "learning_rate": 1.4480278927870996e-05,
      "loss": 2.4504,
      "step": 19560
    },
    {
      "epoch": 2.1321648894880907,
      "grad_norm": 1.4787955284118652,
      "learning_rate": 1.4462119561269705e-05,
      "loss": 2.3602,
      "step": 19570
    },
    {
      "epoch": 2.1332543476188532,
      "grad_norm": 1.5811715126037598,
      "learning_rate": 1.444396019466841e-05,
      "loss": 2.4063,
      "step": 19580
    },
    {
      "epoch": 2.1343438057496154,
      "grad_norm": 1.5234601497650146,
      "learning_rate": 1.4425800828067117e-05,
      "loss": 2.4382,
      "step": 19590
    },
    {
      "epoch": 2.1354332638803775,
      "grad_norm": 1.4273282289505005,
      "learning_rate": 1.4407641461465823e-05,
      "loss": 2.4484,
      "step": 19600
    },
    {
      "epoch": 2.1365227220111396,
      "grad_norm": 1.501364827156067,
      "learning_rate": 1.4389482094864531e-05,
      "loss": 2.3763,
      "step": 19610
    },
    {
      "epoch": 2.137612180141902,
      "grad_norm": 1.5313385725021362,
      "learning_rate": 1.437132272826324e-05,
      "loss": 2.409,
      "step": 19620
    },
    {
      "epoch": 2.1387016382726642,
      "grad_norm": 1.548310399055481,
      "learning_rate": 1.4353163361661946e-05,
      "loss": 2.4365,
      "step": 19630
    },
    {
      "epoch": 2.1397910964034264,
      "grad_norm": 1.4888033866882324,
      "learning_rate": 1.4335003995060653e-05,
      "loss": 2.4264,
      "step": 19640
    },
    {
      "epoch": 2.1408805545341885,
      "grad_norm": 1.6098380088806152,
      "learning_rate": 1.4316844628459362e-05,
      "loss": 2.4701,
      "step": 19650
    },
    {
      "epoch": 2.1419700126649506,
      "grad_norm": 1.5163071155548096,
      "learning_rate": 1.4298685261858067e-05,
      "loss": 2.3396,
      "step": 19660
    },
    {
      "epoch": 2.143059470795713,
      "grad_norm": 1.5050938129425049,
      "learning_rate": 1.4280525895256774e-05,
      "loss": 2.4271,
      "step": 19670
    },
    {
      "epoch": 2.1441489289264752,
      "grad_norm": 1.4791804552078247,
      "learning_rate": 1.4262366528655483e-05,
      "loss": 2.4745,
      "step": 19680
    },
    {
      "epoch": 2.1452383870572374,
      "grad_norm": 1.4896388053894043,
      "learning_rate": 1.4244207162054188e-05,
      "loss": 2.4581,
      "step": 19690
    },
    {
      "epoch": 2.1463278451879995,
      "grad_norm": 1.4850143194198608,
      "learning_rate": 1.4226047795452895e-05,
      "loss": 2.4747,
      "step": 19700
    },
    {
      "epoch": 2.147417303318762,
      "grad_norm": 1.4623310565948486,
      "learning_rate": 1.4207888428851601e-05,
      "loss": 2.3783,
      "step": 19710
    },
    {
      "epoch": 2.148506761449524,
      "grad_norm": 1.5374583005905151,
      "learning_rate": 1.418972906225031e-05,
      "loss": 2.4529,
      "step": 19720
    },
    {
      "epoch": 2.1495962195802862,
      "grad_norm": 1.5480334758758545,
      "learning_rate": 1.4171569695649017e-05,
      "loss": 2.3178,
      "step": 19730
    },
    {
      "epoch": 2.1506856777110483,
      "grad_norm": 1.5095629692077637,
      "learning_rate": 1.4153410329047722e-05,
      "loss": 2.464,
      "step": 19740
    },
    {
      "epoch": 2.1517751358418105,
      "grad_norm": 1.5118767023086548,
      "learning_rate": 1.4135250962446431e-05,
      "loss": 2.5077,
      "step": 19750
    },
    {
      "epoch": 2.152864593972573,
      "grad_norm": 1.5854125022888184,
      "learning_rate": 1.4117091595845138e-05,
      "loss": 2.5033,
      "step": 19760
    },
    {
      "epoch": 2.153954052103335,
      "grad_norm": 1.6797844171524048,
      "learning_rate": 1.4098932229243844e-05,
      "loss": 2.471,
      "step": 19770
    },
    {
      "epoch": 2.1550435102340972,
      "grad_norm": 1.4521085023880005,
      "learning_rate": 1.4080772862642552e-05,
      "loss": 2.3207,
      "step": 19780
    },
    {
      "epoch": 2.1561329683648593,
      "grad_norm": 1.5694326162338257,
      "learning_rate": 1.4062613496041258e-05,
      "loss": 2.4375,
      "step": 19790
    },
    {
      "epoch": 2.157222426495622,
      "grad_norm": 1.5497071743011475,
      "learning_rate": 1.4044454129439965e-05,
      "loss": 2.4723,
      "step": 19800
    },
    {
      "epoch": 2.158311884626384,
      "grad_norm": 1.4745514392852783,
      "learning_rate": 1.4026294762838674e-05,
      "loss": 2.4037,
      "step": 19810
    },
    {
      "epoch": 2.159401342757146,
      "grad_norm": 1.5290268659591675,
      "learning_rate": 1.4008135396237379e-05,
      "loss": 2.5016,
      "step": 19820
    },
    {
      "epoch": 2.1604908008879082,
      "grad_norm": 1.508823037147522,
      "learning_rate": 1.3989976029636088e-05,
      "loss": 2.3153,
      "step": 19830
    },
    {
      "epoch": 2.161580259018671,
      "grad_norm": 1.493571400642395,
      "learning_rate": 1.3971816663034795e-05,
      "loss": 2.4493,
      "step": 19840
    },
    {
      "epoch": 2.162669717149433,
      "grad_norm": 1.4935575723648071,
      "learning_rate": 1.39536572964335e-05,
      "loss": 2.4707,
      "step": 19850
    },
    {
      "epoch": 2.163759175280195,
      "grad_norm": 1.4973939657211304,
      "learning_rate": 1.393549792983221e-05,
      "loss": 2.321,
      "step": 19860
    },
    {
      "epoch": 2.164848633410957,
      "grad_norm": 1.5021039247512817,
      "learning_rate": 1.3917338563230915e-05,
      "loss": 2.4252,
      "step": 19870
    },
    {
      "epoch": 2.1659380915417192,
      "grad_norm": 1.5099657773971558,
      "learning_rate": 1.3899179196629622e-05,
      "loss": 2.4662,
      "step": 19880
    },
    {
      "epoch": 2.167027549672482,
      "grad_norm": 1.4268643856048584,
      "learning_rate": 1.388101983002833e-05,
      "loss": 2.3995,
      "step": 19890
    },
    {
      "epoch": 2.168117007803244,
      "grad_norm": 1.4777685403823853,
      "learning_rate": 1.3862860463427036e-05,
      "loss": 2.3532,
      "step": 19900
    },
    {
      "epoch": 2.169206465934006,
      "grad_norm": 1.5254592895507812,
      "learning_rate": 1.3844701096825743e-05,
      "loss": 2.3625,
      "step": 19910
    },
    {
      "epoch": 2.170295924064768,
      "grad_norm": 1.51339590549469,
      "learning_rate": 1.3826541730224452e-05,
      "loss": 2.4983,
      "step": 19920
    },
    {
      "epoch": 2.1713853821955307,
      "grad_norm": 1.6722606420516968,
      "learning_rate": 1.3808382363623157e-05,
      "loss": 2.4658,
      "step": 19930
    },
    {
      "epoch": 2.172474840326293,
      "grad_norm": 1.4786649942398071,
      "learning_rate": 1.3790222997021864e-05,
      "loss": 2.3924,
      "step": 19940
    },
    {
      "epoch": 2.173564298457055,
      "grad_norm": 1.556257724761963,
      "learning_rate": 1.377206363042057e-05,
      "loss": 2.451,
      "step": 19950
    },
    {
      "epoch": 2.174653756587817,
      "grad_norm": 1.4605894088745117,
      "learning_rate": 1.3753904263819279e-05,
      "loss": 2.4263,
      "step": 19960
    },
    {
      "epoch": 2.1757432147185796,
      "grad_norm": 1.5633467435836792,
      "learning_rate": 1.3735744897217986e-05,
      "loss": 2.47,
      "step": 19970
    },
    {
      "epoch": 2.1768326728493417,
      "grad_norm": 1.4965012073516846,
      "learning_rate": 1.3717585530616691e-05,
      "loss": 2.4376,
      "step": 19980
    },
    {
      "epoch": 2.177922130980104,
      "grad_norm": 1.528712272644043,
      "learning_rate": 1.36994261640154e-05,
      "loss": 2.5053,
      "step": 19990
    },
    {
      "epoch": 2.179011589110866,
      "grad_norm": 1.5027283430099487,
      "learning_rate": 1.3681266797414107e-05,
      "loss": 2.3837,
      "step": 20000
    },
    {
      "epoch": 2.180101047241628,
      "grad_norm": 1.479286551475525,
      "learning_rate": 1.3663107430812813e-05,
      "loss": 2.4632,
      "step": 20010
    },
    {
      "epoch": 2.1811905053723906,
      "grad_norm": 1.5585445165634155,
      "learning_rate": 1.3644948064211521e-05,
      "loss": 2.4441,
      "step": 20020
    },
    {
      "epoch": 2.1822799635031527,
      "grad_norm": 1.4257761240005493,
      "learning_rate": 1.362678869761023e-05,
      "loss": 2.3991,
      "step": 20030
    },
    {
      "epoch": 2.183369421633915,
      "grad_norm": 1.505924105644226,
      "learning_rate": 1.3608629331008934e-05,
      "loss": 2.3404,
      "step": 20040
    },
    {
      "epoch": 2.184458879764677,
      "grad_norm": 1.6607667207717896,
      "learning_rate": 1.3590469964407643e-05,
      "loss": 2.4539,
      "step": 20050
    },
    {
      "epoch": 2.1855483378954395,
      "grad_norm": 1.5110702514648438,
      "learning_rate": 1.3572310597806348e-05,
      "loss": 2.4994,
      "step": 20060
    },
    {
      "epoch": 2.1866377960262016,
      "grad_norm": 1.4636013507843018,
      "learning_rate": 1.3554151231205057e-05,
      "loss": 2.4186,
      "step": 20070
    },
    {
      "epoch": 2.1877272541569637,
      "grad_norm": 1.5418578386306763,
      "learning_rate": 1.3535991864603764e-05,
      "loss": 2.4119,
      "step": 20080
    },
    {
      "epoch": 2.188816712287726,
      "grad_norm": 1.501936435699463,
      "learning_rate": 1.351783249800247e-05,
      "loss": 2.3533,
      "step": 20090
    },
    {
      "epoch": 2.189906170418488,
      "grad_norm": 1.4743950366973877,
      "learning_rate": 1.3499673131401178e-05,
      "loss": 2.3618,
      "step": 20100
    },
    {
      "epoch": 2.1909956285492505,
      "grad_norm": 1.5793417692184448,
      "learning_rate": 1.3481513764799885e-05,
      "loss": 2.3514,
      "step": 20110
    },
    {
      "epoch": 2.1920850866800126,
      "grad_norm": 1.4929672479629517,
      "learning_rate": 1.346335439819859e-05,
      "loss": 2.4987,
      "step": 20120
    },
    {
      "epoch": 2.1931745448107747,
      "grad_norm": 1.4339600801467896,
      "learning_rate": 1.34451950315973e-05,
      "loss": 2.4697,
      "step": 20130
    },
    {
      "epoch": 2.194264002941537,
      "grad_norm": 1.5115364789962769,
      "learning_rate": 1.3427035664996005e-05,
      "loss": 2.4954,
      "step": 20140
    },
    {
      "epoch": 2.1953534610722993,
      "grad_norm": 1.5360095500946045,
      "learning_rate": 1.3408876298394712e-05,
      "loss": 2.3958,
      "step": 20150
    },
    {
      "epoch": 2.1964429192030615,
      "grad_norm": 1.5053468942642212,
      "learning_rate": 1.3390716931793421e-05,
      "loss": 2.5013,
      "step": 20160
    },
    {
      "epoch": 2.1975323773338236,
      "grad_norm": 1.6172341108322144,
      "learning_rate": 1.3372557565192126e-05,
      "loss": 2.4474,
      "step": 20170
    },
    {
      "epoch": 2.1986218354645857,
      "grad_norm": 1.5131621360778809,
      "learning_rate": 1.3354398198590833e-05,
      "loss": 2.3866,
      "step": 20180
    },
    {
      "epoch": 2.199711293595348,
      "grad_norm": 1.5082978010177612,
      "learning_rate": 1.3336238831989542e-05,
      "loss": 2.4733,
      "step": 20190
    },
    {
      "epoch": 2.2008007517261103,
      "grad_norm": 1.488058090209961,
      "learning_rate": 1.3318079465388248e-05,
      "loss": 2.4045,
      "step": 20200
    },
    {
      "epoch": 2.2018902098568724,
      "grad_norm": 1.6363098621368408,
      "learning_rate": 1.3299920098786955e-05,
      "loss": 2.4837,
      "step": 20210
    },
    {
      "epoch": 2.2029796679876346,
      "grad_norm": 1.6005419492721558,
      "learning_rate": 1.328176073218566e-05,
      "loss": 2.4523,
      "step": 20220
    },
    {
      "epoch": 2.2040691261183967,
      "grad_norm": 1.4689209461212158,
      "learning_rate": 1.3263601365584369e-05,
      "loss": 2.4246,
      "step": 20230
    },
    {
      "epoch": 2.2051585842491592,
      "grad_norm": 1.624984860420227,
      "learning_rate": 1.3245441998983076e-05,
      "loss": 2.4274,
      "step": 20240
    },
    {
      "epoch": 2.2062480423799213,
      "grad_norm": 1.42740797996521,
      "learning_rate": 1.3227282632381782e-05,
      "loss": 2.4368,
      "step": 20250
    },
    {
      "epoch": 2.2073375005106834,
      "grad_norm": 1.465186357498169,
      "learning_rate": 1.320912326578049e-05,
      "loss": 2.4484,
      "step": 20260
    },
    {
      "epoch": 2.2084269586414456,
      "grad_norm": 1.4987459182739258,
      "learning_rate": 1.3190963899179199e-05,
      "loss": 2.4781,
      "step": 20270
    },
    {
      "epoch": 2.209516416772208,
      "grad_norm": 1.5653982162475586,
      "learning_rate": 1.3172804532577903e-05,
      "loss": 2.4602,
      "step": 20280
    },
    {
      "epoch": 2.2106058749029702,
      "grad_norm": 1.489185094833374,
      "learning_rate": 1.3154645165976612e-05,
      "loss": 2.4361,
      "step": 20290
    },
    {
      "epoch": 2.2116953330337323,
      "grad_norm": 1.4973738193511963,
      "learning_rate": 1.3136485799375317e-05,
      "loss": 2.4031,
      "step": 20300
    },
    {
      "epoch": 2.2127847911644944,
      "grad_norm": 1.549208402633667,
      "learning_rate": 1.3118326432774026e-05,
      "loss": 2.4122,
      "step": 20310
    },
    {
      "epoch": 2.213874249295257,
      "grad_norm": 1.7404862642288208,
      "learning_rate": 1.3100167066172733e-05,
      "loss": 2.3774,
      "step": 20320
    },
    {
      "epoch": 2.214963707426019,
      "grad_norm": 1.5839074850082397,
      "learning_rate": 1.3082007699571438e-05,
      "loss": 2.4177,
      "step": 20330
    },
    {
      "epoch": 2.2160531655567812,
      "grad_norm": 1.5023335218429565,
      "learning_rate": 1.3063848332970147e-05,
      "loss": 2.4279,
      "step": 20340
    },
    {
      "epoch": 2.2171426236875433,
      "grad_norm": 1.5074244737625122,
      "learning_rate": 1.3045688966368854e-05,
      "loss": 2.4298,
      "step": 20350
    },
    {
      "epoch": 2.2182320818183054,
      "grad_norm": 1.5700397491455078,
      "learning_rate": 1.302752959976756e-05,
      "loss": 2.3972,
      "step": 20360
    },
    {
      "epoch": 2.219321539949068,
      "grad_norm": 1.493980050086975,
      "learning_rate": 1.3009370233166269e-05,
      "loss": 2.3443,
      "step": 20370
    },
    {
      "epoch": 2.22041099807983,
      "grad_norm": 1.4529054164886475,
      "learning_rate": 1.2991210866564976e-05,
      "loss": 2.3764,
      "step": 20380
    },
    {
      "epoch": 2.2215004562105922,
      "grad_norm": 1.5412306785583496,
      "learning_rate": 1.2973051499963681e-05,
      "loss": 2.4111,
      "step": 20390
    },
    {
      "epoch": 2.2225899143413543,
      "grad_norm": 1.4695943593978882,
      "learning_rate": 1.295489213336239e-05,
      "loss": 2.3948,
      "step": 20400
    },
    {
      "epoch": 2.223679372472117,
      "grad_norm": 1.5366686582565308,
      "learning_rate": 1.2936732766761095e-05,
      "loss": 2.5596,
      "step": 20410
    },
    {
      "epoch": 2.224768830602879,
      "grad_norm": 1.6205989122390747,
      "learning_rate": 1.2918573400159802e-05,
      "loss": 2.447,
      "step": 20420
    },
    {
      "epoch": 2.225858288733641,
      "grad_norm": 1.5164209604263306,
      "learning_rate": 1.2900414033558511e-05,
      "loss": 2.4998,
      "step": 20430
    },
    {
      "epoch": 2.226947746864403,
      "grad_norm": 1.5356216430664062,
      "learning_rate": 1.2882254666957217e-05,
      "loss": 2.4649,
      "step": 20440
    },
    {
      "epoch": 2.2280372049951653,
      "grad_norm": 1.5251684188842773,
      "learning_rate": 1.2864095300355924e-05,
      "loss": 2.3312,
      "step": 20450
    },
    {
      "epoch": 2.229126663125928,
      "grad_norm": 1.4750652313232422,
      "learning_rate": 1.2845935933754633e-05,
      "loss": 2.3924,
      "step": 20460
    },
    {
      "epoch": 2.23021612125669,
      "grad_norm": 1.5355284214019775,
      "learning_rate": 1.2827776567153338e-05,
      "loss": 2.3926,
      "step": 20470
    },
    {
      "epoch": 2.231305579387452,
      "grad_norm": 1.5465000867843628,
      "learning_rate": 1.2809617200552045e-05,
      "loss": 2.3335,
      "step": 20480
    },
    {
      "epoch": 2.232395037518214,
      "grad_norm": 1.4426259994506836,
      "learning_rate": 1.279145783395075e-05,
      "loss": 2.4184,
      "step": 20490
    },
    {
      "epoch": 2.2334844956489768,
      "grad_norm": 1.5305603742599487,
      "learning_rate": 1.277329846734946e-05,
      "loss": 2.4435,
      "step": 20500
    },
    {
      "epoch": 2.234573953779739,
      "grad_norm": 1.5134305953979492,
      "learning_rate": 1.2755139100748168e-05,
      "loss": 2.3462,
      "step": 20510
    },
    {
      "epoch": 2.235663411910501,
      "grad_norm": 1.615153431892395,
      "learning_rate": 1.2736979734146874e-05,
      "loss": 2.3376,
      "step": 20520
    },
    {
      "epoch": 2.236752870041263,
      "grad_norm": 1.505794882774353,
      "learning_rate": 1.271882036754558e-05,
      "loss": 2.4295,
      "step": 20530
    },
    {
      "epoch": 2.237842328172025,
      "grad_norm": 1.521838665008545,
      "learning_rate": 1.270066100094429e-05,
      "loss": 2.3949,
      "step": 20540
    },
    {
      "epoch": 2.2389317863027878,
      "grad_norm": 1.4553673267364502,
      "learning_rate": 1.2682501634342995e-05,
      "loss": 2.5512,
      "step": 20550
    },
    {
      "epoch": 2.24002124443355,
      "grad_norm": 1.4940035343170166,
      "learning_rate": 1.2664342267741702e-05,
      "loss": 2.444,
      "step": 20560
    },
    {
      "epoch": 2.241110702564312,
      "grad_norm": 1.6010773181915283,
      "learning_rate": 1.2646182901140407e-05,
      "loss": 2.3987,
      "step": 20570
    },
    {
      "epoch": 2.242200160695074,
      "grad_norm": 1.449966549873352,
      "learning_rate": 1.2628023534539116e-05,
      "loss": 2.4418,
      "step": 20580
    },
    {
      "epoch": 2.2432896188258367,
      "grad_norm": 1.5023424625396729,
      "learning_rate": 1.2609864167937823e-05,
      "loss": 2.4858,
      "step": 20590
    },
    {
      "epoch": 2.2443790769565988,
      "grad_norm": 1.5310029983520508,
      "learning_rate": 1.2591704801336529e-05,
      "loss": 2.4506,
      "step": 20600
    },
    {
      "epoch": 2.245468535087361,
      "grad_norm": 1.553809642791748,
      "learning_rate": 1.2573545434735238e-05,
      "loss": 2.3362,
      "step": 20610
    },
    {
      "epoch": 2.246557993218123,
      "grad_norm": 1.560581088066101,
      "learning_rate": 1.2555386068133945e-05,
      "loss": 2.3303,
      "step": 20620
    },
    {
      "epoch": 2.2476474513488856,
      "grad_norm": 1.5138945579528809,
      "learning_rate": 1.253722670153265e-05,
      "loss": 2.4079,
      "step": 20630
    },
    {
      "epoch": 2.2487369094796477,
      "grad_norm": 1.5176209211349487,
      "learning_rate": 1.2519067334931359e-05,
      "loss": 2.3789,
      "step": 20640
    },
    {
      "epoch": 2.2498263676104098,
      "grad_norm": 1.4938021898269653,
      "learning_rate": 1.2500907968330064e-05,
      "loss": 2.3907,
      "step": 20650
    },
    {
      "epoch": 2.250915825741172,
      "grad_norm": 1.5463190078735352,
      "learning_rate": 1.2482748601728771e-05,
      "loss": 2.3172,
      "step": 20660
    },
    {
      "epoch": 2.2520052838719344,
      "grad_norm": 1.5939602851867676,
      "learning_rate": 1.2464589235127479e-05,
      "loss": 2.4538,
      "step": 20670
    },
    {
      "epoch": 2.2530947420026965,
      "grad_norm": 1.6088906526565552,
      "learning_rate": 1.2446429868526187e-05,
      "loss": 2.4267,
      "step": 20680
    },
    {
      "epoch": 2.2541842001334587,
      "grad_norm": 1.5079398155212402,
      "learning_rate": 1.2428270501924893e-05,
      "loss": 2.4714,
      "step": 20690
    },
    {
      "epoch": 2.2552736582642208,
      "grad_norm": 1.5675643682479858,
      "learning_rate": 1.24101111353236e-05,
      "loss": 2.3543,
      "step": 20700
    },
    {
      "epoch": 2.256363116394983,
      "grad_norm": 1.4888485670089722,
      "learning_rate": 1.2391951768722307e-05,
      "loss": 2.4458,
      "step": 20710
    },
    {
      "epoch": 2.2574525745257454,
      "grad_norm": 1.5955066680908203,
      "learning_rate": 1.2373792402121016e-05,
      "loss": 2.4053,
      "step": 20720
    },
    {
      "epoch": 2.2585420326565075,
      "grad_norm": 1.4536089897155762,
      "learning_rate": 1.2355633035519721e-05,
      "loss": 2.5095,
      "step": 20730
    },
    {
      "epoch": 2.2596314907872697,
      "grad_norm": 1.5456078052520752,
      "learning_rate": 1.2337473668918428e-05,
      "loss": 2.4098,
      "step": 20740
    },
    {
      "epoch": 2.2607209489180318,
      "grad_norm": 1.528537631034851,
      "learning_rate": 1.2319314302317137e-05,
      "loss": 2.4564,
      "step": 20750
    },
    {
      "epoch": 2.2618104070487943,
      "grad_norm": 1.4936909675598145,
      "learning_rate": 1.2301154935715843e-05,
      "loss": 2.4673,
      "step": 20760
    },
    {
      "epoch": 2.2628998651795564,
      "grad_norm": 1.4956063032150269,
      "learning_rate": 1.228299556911455e-05,
      "loss": 2.3785,
      "step": 20770
    },
    {
      "epoch": 2.2639893233103185,
      "grad_norm": 1.5686959028244019,
      "learning_rate": 1.2264836202513257e-05,
      "loss": 2.4728,
      "step": 20780
    },
    {
      "epoch": 2.2650787814410807,
      "grad_norm": 1.5024996995925903,
      "learning_rate": 1.2246676835911964e-05,
      "loss": 2.4619,
      "step": 20790
    },
    {
      "epoch": 2.2661682395718428,
      "grad_norm": 1.500322937965393,
      "learning_rate": 1.2228517469310671e-05,
      "loss": 2.4119,
      "step": 20800
    },
    {
      "epoch": 2.2672576977026053,
      "grad_norm": 1.421108365058899,
      "learning_rate": 1.2210358102709378e-05,
      "loss": 2.4578,
      "step": 20810
    },
    {
      "epoch": 2.2683471558333674,
      "grad_norm": 1.4503357410430908,
      "learning_rate": 1.2192198736108085e-05,
      "loss": 2.4317,
      "step": 20820
    },
    {
      "epoch": 2.2694366139641295,
      "grad_norm": 1.5975862741470337,
      "learning_rate": 1.2174039369506792e-05,
      "loss": 2.3497,
      "step": 20830
    },
    {
      "epoch": 2.2705260720948917,
      "grad_norm": 1.5554391145706177,
      "learning_rate": 1.21558800029055e-05,
      "loss": 2.4204,
      "step": 20840
    },
    {
      "epoch": 2.271615530225654,
      "grad_norm": 1.457959771156311,
      "learning_rate": 1.2137720636304207e-05,
      "loss": 2.3392,
      "step": 20850
    },
    {
      "epoch": 2.2727049883564163,
      "grad_norm": 1.472147822380066,
      "learning_rate": 1.2119561269702914e-05,
      "loss": 2.3432,
      "step": 20860
    },
    {
      "epoch": 2.2737944464871784,
      "grad_norm": 1.5669306516647339,
      "learning_rate": 1.210140190310162e-05,
      "loss": 2.4563,
      "step": 20870
    },
    {
      "epoch": 2.2748839046179405,
      "grad_norm": 1.51967453956604,
      "learning_rate": 1.2083242536500328e-05,
      "loss": 2.4392,
      "step": 20880
    },
    {
      "epoch": 2.2759733627487027,
      "grad_norm": 1.5053112506866455,
      "learning_rate": 1.2065083169899035e-05,
      "loss": 2.4323,
      "step": 20890
    },
    {
      "epoch": 2.277062820879465,
      "grad_norm": 1.4563409090042114,
      "learning_rate": 1.204692380329774e-05,
      "loss": 2.3567,
      "step": 20900
    },
    {
      "epoch": 2.2781522790102273,
      "grad_norm": 1.5418154001235962,
      "learning_rate": 1.202876443669645e-05,
      "loss": 2.4092,
      "step": 20910
    },
    {
      "epoch": 2.2792417371409894,
      "grad_norm": 1.5998045206069946,
      "learning_rate": 1.2010605070095156e-05,
      "loss": 2.374,
      "step": 20920
    },
    {
      "epoch": 2.2803311952717515,
      "grad_norm": 1.560652732849121,
      "learning_rate": 1.1992445703493862e-05,
      "loss": 2.4187,
      "step": 20930
    },
    {
      "epoch": 2.281420653402514,
      "grad_norm": 1.510494351387024,
      "learning_rate": 1.1974286336892569e-05,
      "loss": 2.4404,
      "step": 20940
    },
    {
      "epoch": 2.282510111533276,
      "grad_norm": 1.568089485168457,
      "learning_rate": 1.1956126970291278e-05,
      "loss": 2.372,
      "step": 20950
    },
    {
      "epoch": 2.2835995696640383,
      "grad_norm": 1.509604811668396,
      "learning_rate": 1.1937967603689985e-05,
      "loss": 2.4278,
      "step": 20960
    },
    {
      "epoch": 2.2846890277948004,
      "grad_norm": 1.5013118982315063,
      "learning_rate": 1.191980823708869e-05,
      "loss": 2.4997,
      "step": 20970
    },
    {
      "epoch": 2.2857784859255625,
      "grad_norm": 1.4799319505691528,
      "learning_rate": 1.1901648870487397e-05,
      "loss": 2.3577,
      "step": 20980
    },
    {
      "epoch": 2.286867944056325,
      "grad_norm": 1.511810541152954,
      "learning_rate": 1.1883489503886106e-05,
      "loss": 2.4298,
      "step": 20990
    },
    {
      "epoch": 2.287957402187087,
      "grad_norm": 1.4784756898880005,
      "learning_rate": 1.1865330137284812e-05,
      "loss": 2.4963,
      "step": 21000
    },
    {
      "epoch": 2.2890468603178493,
      "grad_norm": 1.4791682958602905,
      "learning_rate": 1.1847170770683519e-05,
      "loss": 2.4817,
      "step": 21010
    },
    {
      "epoch": 2.2901363184486114,
      "grad_norm": 1.5457338094711304,
      "learning_rate": 1.1829011404082226e-05,
      "loss": 2.4673,
      "step": 21020
    },
    {
      "epoch": 2.291225776579374,
      "grad_norm": 1.5528992414474487,
      "learning_rate": 1.1810852037480933e-05,
      "loss": 2.4099,
      "step": 21030
    },
    {
      "epoch": 2.292315234710136,
      "grad_norm": 1.5352227687835693,
      "learning_rate": 1.179269267087964e-05,
      "loss": 2.4388,
      "step": 21040
    },
    {
      "epoch": 2.293404692840898,
      "grad_norm": 1.5473144054412842,
      "learning_rate": 1.1774533304278347e-05,
      "loss": 2.4531,
      "step": 21050
    },
    {
      "epoch": 2.2944941509716603,
      "grad_norm": 1.4934977293014526,
      "learning_rate": 1.1756373937677054e-05,
      "loss": 2.377,
      "step": 21060
    },
    {
      "epoch": 2.295583609102423,
      "grad_norm": 1.4726066589355469,
      "learning_rate": 1.1738214571075761e-05,
      "loss": 2.4188,
      "step": 21070
    },
    {
      "epoch": 2.296673067233185,
      "grad_norm": 1.5332928895950317,
      "learning_rate": 1.1720055204474468e-05,
      "loss": 2.4667,
      "step": 21080
    },
    {
      "epoch": 2.297762525363947,
      "grad_norm": 1.5758782625198364,
      "learning_rate": 1.1701895837873176e-05,
      "loss": 2.3974,
      "step": 21090
    },
    {
      "epoch": 2.298851983494709,
      "grad_norm": 1.4577642679214478,
      "learning_rate": 1.1683736471271883e-05,
      "loss": 2.4711,
      "step": 21100
    },
    {
      "epoch": 2.2999414416254718,
      "grad_norm": 1.4840134382247925,
      "learning_rate": 1.166557710467059e-05,
      "loss": 2.3931,
      "step": 21110
    },
    {
      "epoch": 2.301030899756234,
      "grad_norm": 1.539228081703186,
      "learning_rate": 1.1647417738069297e-05,
      "loss": 2.4465,
      "step": 21120
    },
    {
      "epoch": 2.302120357886996,
      "grad_norm": 1.5473439693450928,
      "learning_rate": 1.1629258371468004e-05,
      "loss": 2.4819,
      "step": 21130
    },
    {
      "epoch": 2.303209816017758,
      "grad_norm": 1.5829745531082153,
      "learning_rate": 1.1611099004866711e-05,
      "loss": 2.5438,
      "step": 21140
    },
    {
      "epoch": 2.30429927414852,
      "grad_norm": 1.5206496715545654,
      "learning_rate": 1.1592939638265418e-05,
      "loss": 2.4406,
      "step": 21150
    },
    {
      "epoch": 2.3053887322792828,
      "grad_norm": 1.4961901903152466,
      "learning_rate": 1.1574780271664125e-05,
      "loss": 2.4696,
      "step": 21160
    },
    {
      "epoch": 2.306478190410045,
      "grad_norm": 1.5411243438720703,
      "learning_rate": 1.155662090506283e-05,
      "loss": 2.5163,
      "step": 21170
    },
    {
      "epoch": 2.307567648540807,
      "grad_norm": 1.5332233905792236,
      "learning_rate": 1.153846153846154e-05,
      "loss": 2.3992,
      "step": 21180
    },
    {
      "epoch": 2.308657106671569,
      "grad_norm": 1.4791038036346436,
      "learning_rate": 1.1520302171860247e-05,
      "loss": 2.4132,
      "step": 21190
    },
    {
      "epoch": 2.3097465648023316,
      "grad_norm": 1.5232291221618652,
      "learning_rate": 1.1502142805258954e-05,
      "loss": 2.4167,
      "step": 21200
    },
    {
      "epoch": 2.3108360229330938,
      "grad_norm": 1.5208022594451904,
      "learning_rate": 1.148398343865766e-05,
      "loss": 2.4443,
      "step": 21210
    },
    {
      "epoch": 2.311925481063856,
      "grad_norm": 1.5665103197097778,
      "learning_rate": 1.1465824072056368e-05,
      "loss": 2.3691,
      "step": 21220
    },
    {
      "epoch": 2.313014939194618,
      "grad_norm": 1.456357717514038,
      "learning_rate": 1.1447664705455075e-05,
      "loss": 2.4267,
      "step": 21230
    },
    {
      "epoch": 2.31410439732538,
      "grad_norm": 1.5313032865524292,
      "learning_rate": 1.142950533885378e-05,
      "loss": 2.4662,
      "step": 21240
    },
    {
      "epoch": 2.3151938554561426,
      "grad_norm": 1.574054479598999,
      "learning_rate": 1.1411345972252488e-05,
      "loss": 2.4661,
      "step": 21250
    },
    {
      "epoch": 2.3162833135869048,
      "grad_norm": 1.5547484159469604,
      "learning_rate": 1.1393186605651196e-05,
      "loss": 2.403,
      "step": 21260
    },
    {
      "epoch": 2.317372771717667,
      "grad_norm": 1.4779413938522339,
      "learning_rate": 1.1375027239049904e-05,
      "loss": 2.3821,
      "step": 21270
    },
    {
      "epoch": 2.318462229848429,
      "grad_norm": 1.6862614154815674,
      "learning_rate": 1.1356867872448609e-05,
      "loss": 2.4505,
      "step": 21280
    },
    {
      "epoch": 2.3195516879791915,
      "grad_norm": 1.4394160509109497,
      "learning_rate": 1.1338708505847316e-05,
      "loss": 2.4081,
      "step": 21290
    },
    {
      "epoch": 2.3206411461099536,
      "grad_norm": 1.5108859539031982,
      "learning_rate": 1.1320549139246025e-05,
      "loss": 2.4693,
      "step": 21300
    },
    {
      "epoch": 2.3217306042407158,
      "grad_norm": 1.571833848953247,
      "learning_rate": 1.130238977264473e-05,
      "loss": 2.4161,
      "step": 21310
    },
    {
      "epoch": 2.322820062371478,
      "grad_norm": 1.5155043601989746,
      "learning_rate": 1.1284230406043437e-05,
      "loss": 2.3711,
      "step": 21320
    },
    {
      "epoch": 2.32390952050224,
      "grad_norm": 1.5190768241882324,
      "learning_rate": 1.1266071039442145e-05,
      "loss": 2.3767,
      "step": 21330
    },
    {
      "epoch": 2.3249989786330025,
      "grad_norm": 1.505706548690796,
      "learning_rate": 1.1247911672840852e-05,
      "loss": 2.4846,
      "step": 21340
    },
    {
      "epoch": 2.3260884367637646,
      "grad_norm": 1.607583999633789,
      "learning_rate": 1.1229752306239559e-05,
      "loss": 2.406,
      "step": 21350
    },
    {
      "epoch": 2.3271778948945268,
      "grad_norm": 1.5125200748443604,
      "learning_rate": 1.1211592939638266e-05,
      "loss": 2.3802,
      "step": 21360
    },
    {
      "epoch": 2.328267353025289,
      "grad_norm": 1.5056082010269165,
      "learning_rate": 1.1193433573036973e-05,
      "loss": 2.4076,
      "step": 21370
    },
    {
      "epoch": 2.3293568111560514,
      "grad_norm": 1.5903091430664062,
      "learning_rate": 1.117527420643568e-05,
      "loss": 2.4269,
      "step": 21380
    },
    {
      "epoch": 2.3304462692868135,
      "grad_norm": 1.58906090259552,
      "learning_rate": 1.1157114839834387e-05,
      "loss": 2.4871,
      "step": 21390
    },
    {
      "epoch": 2.3315357274175756,
      "grad_norm": 1.5165644884109497,
      "learning_rate": 1.1138955473233094e-05,
      "loss": 2.4397,
      "step": 21400
    },
    {
      "epoch": 2.3326251855483378,
      "grad_norm": 1.565317153930664,
      "learning_rate": 1.1120796106631801e-05,
      "loss": 2.461,
      "step": 21410
    },
    {
      "epoch": 2.3337146436791,
      "grad_norm": 1.482066035270691,
      "learning_rate": 1.1102636740030509e-05,
      "loss": 2.4871,
      "step": 21420
    },
    {
      "epoch": 2.3348041018098624,
      "grad_norm": 1.588301658630371,
      "learning_rate": 1.1084477373429216e-05,
      "loss": 2.4059,
      "step": 21430
    },
    {
      "epoch": 2.3358935599406245,
      "grad_norm": 1.4510681629180908,
      "learning_rate": 1.1066318006827923e-05,
      "loss": 2.4434,
      "step": 21440
    },
    {
      "epoch": 2.3369830180713866,
      "grad_norm": 1.4956930875778198,
      "learning_rate": 1.1048158640226628e-05,
      "loss": 2.3371,
      "step": 21450
    },
    {
      "epoch": 2.338072476202149,
      "grad_norm": 1.4416580200195312,
      "learning_rate": 1.1029999273625337e-05,
      "loss": 2.3883,
      "step": 21460
    },
    {
      "epoch": 2.3391619343329113,
      "grad_norm": 1.4842067956924438,
      "learning_rate": 1.1011839907024044e-05,
      "loss": 2.3918,
      "step": 21470
    },
    {
      "epoch": 2.3402513924636734,
      "grad_norm": 1.5347931385040283,
      "learning_rate": 1.099368054042275e-05,
      "loss": 2.4052,
      "step": 21480
    },
    {
      "epoch": 2.3413408505944355,
      "grad_norm": 1.5737725496292114,
      "learning_rate": 1.0975521173821457e-05,
      "loss": 2.4359,
      "step": 21490
    },
    {
      "epoch": 2.3424303087251976,
      "grad_norm": 1.5605719089508057,
      "learning_rate": 1.0957361807220165e-05,
      "loss": 2.4438,
      "step": 21500
    },
    {
      "epoch": 2.34351976685596,
      "grad_norm": 1.4494166374206543,
      "learning_rate": 1.0939202440618873e-05,
      "loss": 2.3784,
      "step": 21510
    },
    {
      "epoch": 2.3446092249867223,
      "grad_norm": 1.464421272277832,
      "learning_rate": 1.0921043074017578e-05,
      "loss": 2.4311,
      "step": 21520
    },
    {
      "epoch": 2.3456986831174844,
      "grad_norm": 1.5503517389297485,
      "learning_rate": 1.0902883707416287e-05,
      "loss": 2.4301,
      "step": 21530
    },
    {
      "epoch": 2.3467881412482465,
      "grad_norm": 1.546910285949707,
      "learning_rate": 1.0884724340814994e-05,
      "loss": 2.3679,
      "step": 21540
    },
    {
      "epoch": 2.347877599379009,
      "grad_norm": 1.5631146430969238,
      "learning_rate": 1.08665649742137e-05,
      "loss": 2.4293,
      "step": 21550
    },
    {
      "epoch": 2.348967057509771,
      "grad_norm": 1.496783971786499,
      "learning_rate": 1.0848405607612406e-05,
      "loss": 2.3608,
      "step": 21560
    },
    {
      "epoch": 2.3500565156405333,
      "grad_norm": 1.5418964624404907,
      "learning_rate": 1.0830246241011115e-05,
      "loss": 2.3329,
      "step": 21570
    },
    {
      "epoch": 2.3511459737712954,
      "grad_norm": 1.5848127603530884,
      "learning_rate": 1.081208687440982e-05,
      "loss": 2.4869,
      "step": 21580
    },
    {
      "epoch": 2.3522354319020575,
      "grad_norm": 1.5069713592529297,
      "learning_rate": 1.0793927507808528e-05,
      "loss": 2.3837,
      "step": 21590
    },
    {
      "epoch": 2.35332489003282,
      "grad_norm": 1.5383559465408325,
      "learning_rate": 1.0775768141207235e-05,
      "loss": 2.3968,
      "step": 21600
    },
    {
      "epoch": 2.354414348163582,
      "grad_norm": 1.6219969987869263,
      "learning_rate": 1.0757608774605944e-05,
      "loss": 2.3896,
      "step": 21610
    },
    {
      "epoch": 2.3555038062943443,
      "grad_norm": 1.4446920156478882,
      "learning_rate": 1.0739449408004649e-05,
      "loss": 2.3435,
      "step": 21620
    },
    {
      "epoch": 2.3565932644251064,
      "grad_norm": 1.5006359815597534,
      "learning_rate": 1.0721290041403356e-05,
      "loss": 2.3952,
      "step": 21630
    },
    {
      "epoch": 2.357682722555869,
      "grad_norm": 1.5327216386795044,
      "learning_rate": 1.0703130674802063e-05,
      "loss": 2.5174,
      "step": 21640
    },
    {
      "epoch": 2.358772180686631,
      "grad_norm": 1.4905954599380493,
      "learning_rate": 1.068497130820077e-05,
      "loss": 2.4579,
      "step": 21650
    },
    {
      "epoch": 2.359861638817393,
      "grad_norm": 1.6546378135681152,
      "learning_rate": 1.0666811941599478e-05,
      "loss": 2.4573,
      "step": 21660
    },
    {
      "epoch": 2.3609510969481553,
      "grad_norm": 1.5902810096740723,
      "learning_rate": 1.0648652574998185e-05,
      "loss": 2.4027,
      "step": 21670
    },
    {
      "epoch": 2.3620405550789174,
      "grad_norm": 1.5056251287460327,
      "learning_rate": 1.0630493208396892e-05,
      "loss": 2.451,
      "step": 21680
    },
    {
      "epoch": 2.36313001320968,
      "grad_norm": 1.5052199363708496,
      "learning_rate": 1.0612333841795599e-05,
      "loss": 2.4803,
      "step": 21690
    },
    {
      "epoch": 2.364219471340442,
      "grad_norm": 1.549061894416809,
      "learning_rate": 1.0594174475194306e-05,
      "loss": 2.4574,
      "step": 21700
    },
    {
      "epoch": 2.365308929471204,
      "grad_norm": 1.4773766994476318,
      "learning_rate": 1.0576015108593013e-05,
      "loss": 2.3268,
      "step": 21710
    },
    {
      "epoch": 2.3663983876019663,
      "grad_norm": 1.5908498764038086,
      "learning_rate": 1.0557855741991718e-05,
      "loss": 2.4166,
      "step": 21720
    },
    {
      "epoch": 2.367487845732729,
      "grad_norm": 1.5243654251098633,
      "learning_rate": 1.0539696375390427e-05,
      "loss": 2.3807,
      "step": 21730
    },
    {
      "epoch": 2.368577303863491,
      "grad_norm": 1.5342435836791992,
      "learning_rate": 1.0521537008789134e-05,
      "loss": 2.3587,
      "step": 21740
    },
    {
      "epoch": 2.369666761994253,
      "grad_norm": 1.5040379762649536,
      "learning_rate": 1.0503377642187842e-05,
      "loss": 2.4001,
      "step": 21750
    },
    {
      "epoch": 2.370756220125015,
      "grad_norm": 1.4688416719436646,
      "learning_rate": 1.0485218275586547e-05,
      "loss": 2.4354,
      "step": 21760
    },
    {
      "epoch": 2.3718456782557773,
      "grad_norm": 1.4917117357254028,
      "learning_rate": 1.0467058908985256e-05,
      "loss": 2.4988,
      "step": 21770
    },
    {
      "epoch": 2.37293513638654,
      "grad_norm": 1.4961442947387695,
      "learning_rate": 1.0448899542383963e-05,
      "loss": 2.5672,
      "step": 21780
    },
    {
      "epoch": 2.374024594517302,
      "grad_norm": 1.5072121620178223,
      "learning_rate": 1.0430740175782668e-05,
      "loss": 2.4394,
      "step": 21790
    },
    {
      "epoch": 2.375114052648064,
      "grad_norm": 1.5738880634307861,
      "learning_rate": 1.0412580809181375e-05,
      "loss": 2.5274,
      "step": 21800
    },
    {
      "epoch": 2.376203510778826,
      "grad_norm": 1.5140609741210938,
      "learning_rate": 1.0394421442580084e-05,
      "loss": 2.4227,
      "step": 21810
    },
    {
      "epoch": 2.3772929689095887,
      "grad_norm": 1.5327094793319702,
      "learning_rate": 1.037626207597879e-05,
      "loss": 2.3783,
      "step": 21820
    },
    {
      "epoch": 2.378382427040351,
      "grad_norm": 1.4749633073806763,
      "learning_rate": 1.0358102709377497e-05,
      "loss": 2.453,
      "step": 21830
    },
    {
      "epoch": 2.379471885171113,
      "grad_norm": 1.5254473686218262,
      "learning_rate": 1.0339943342776204e-05,
      "loss": 2.3996,
      "step": 21840
    },
    {
      "epoch": 2.380561343301875,
      "grad_norm": 1.545724868774414,
      "learning_rate": 1.0321783976174913e-05,
      "loss": 2.438,
      "step": 21850
    },
    {
      "epoch": 2.3816508014326376,
      "grad_norm": 1.5423842668533325,
      "learning_rate": 1.0303624609573618e-05,
      "loss": 2.4694,
      "step": 21860
    },
    {
      "epoch": 2.3827402595633997,
      "grad_norm": 1.509261965751648,
      "learning_rate": 1.0285465242972325e-05,
      "loss": 2.4982,
      "step": 21870
    },
    {
      "epoch": 2.383829717694162,
      "grad_norm": 1.5229676961898804,
      "learning_rate": 1.0267305876371034e-05,
      "loss": 2.4924,
      "step": 21880
    },
    {
      "epoch": 2.384919175824924,
      "grad_norm": 1.4631155729293823,
      "learning_rate": 1.024914650976974e-05,
      "loss": 2.3293,
      "step": 21890
    },
    {
      "epoch": 2.3860086339556865,
      "grad_norm": 1.6120169162750244,
      "learning_rate": 1.0230987143168446e-05,
      "loss": 2.3678,
      "step": 21900
    },
    {
      "epoch": 2.3870980920864486,
      "grad_norm": 1.5358235836029053,
      "learning_rate": 1.0212827776567154e-05,
      "loss": 2.4936,
      "step": 21910
    },
    {
      "epoch": 2.3881875502172107,
      "grad_norm": 1.5543437004089355,
      "learning_rate": 1.019466840996586e-05,
      "loss": 2.4321,
      "step": 21920
    },
    {
      "epoch": 2.389277008347973,
      "grad_norm": 1.5322142839431763,
      "learning_rate": 1.0176509043364568e-05,
      "loss": 2.4485,
      "step": 21930
    },
    {
      "epoch": 2.390366466478735,
      "grad_norm": 1.433245301246643,
      "learning_rate": 1.0158349676763275e-05,
      "loss": 2.4084,
      "step": 21940
    },
    {
      "epoch": 2.3914559246094975,
      "grad_norm": 1.4435192346572876,
      "learning_rate": 1.0140190310161982e-05,
      "loss": 2.4231,
      "step": 21950
    },
    {
      "epoch": 2.3925453827402596,
      "grad_norm": 1.531704068183899,
      "learning_rate": 1.012203094356069e-05,
      "loss": 2.4025,
      "step": 21960
    },
    {
      "epoch": 2.3936348408710217,
      "grad_norm": 1.501051425933838,
      "learning_rate": 1.0103871576959396e-05,
      "loss": 2.4636,
      "step": 21970
    },
    {
      "epoch": 2.394724299001784,
      "grad_norm": 1.5200461149215698,
      "learning_rate": 1.0085712210358103e-05,
      "loss": 2.4774,
      "step": 21980
    },
    {
      "epoch": 2.3958137571325464,
      "grad_norm": 1.5171509981155396,
      "learning_rate": 1.006755284375681e-05,
      "loss": 2.5193,
      "step": 21990
    },
    {
      "epoch": 2.3969032152633085,
      "grad_norm": 1.5230791568756104,
      "learning_rate": 1.0049393477155518e-05,
      "loss": 2.4265,
      "step": 22000
    },
    {
      "epoch": 2.3979926733940706,
      "grad_norm": 1.502747654914856,
      "learning_rate": 1.0031234110554225e-05,
      "loss": 2.4361,
      "step": 22010
    },
    {
      "epoch": 2.3990821315248327,
      "grad_norm": 1.5215938091278076,
      "learning_rate": 1.0013074743952932e-05,
      "loss": 2.5111,
      "step": 22020
    },
    {
      "epoch": 2.400171589655595,
      "grad_norm": 1.547037124633789,
      "learning_rate": 9.994915377351637e-06,
      "loss": 2.4088,
      "step": 22030
    },
    {
      "epoch": 2.4012610477863574,
      "grad_norm": 1.5924100875854492,
      "learning_rate": 9.976756010750346e-06,
      "loss": 2.3413,
      "step": 22040
    },
    {
      "epoch": 2.4023505059171195,
      "grad_norm": 1.5215981006622314,
      "learning_rate": 9.958596644149053e-06,
      "loss": 2.4664,
      "step": 22050
    },
    {
      "epoch": 2.4034399640478816,
      "grad_norm": 1.472849726676941,
      "learning_rate": 9.94043727754776e-06,
      "loss": 2.3901,
      "step": 22060
    },
    {
      "epoch": 2.4045294221786437,
      "grad_norm": 1.529248595237732,
      "learning_rate": 9.922277910946466e-06,
      "loss": 2.4144,
      "step": 22070
    },
    {
      "epoch": 2.4056188803094063,
      "grad_norm": 1.4904712438583374,
      "learning_rate": 9.904118544345175e-06,
      "loss": 2.4346,
      "step": 22080
    },
    {
      "epoch": 2.4067083384401684,
      "grad_norm": 1.4618911743164062,
      "learning_rate": 9.885959177743882e-06,
      "loss": 2.3706,
      "step": 22090
    },
    {
      "epoch": 2.4077977965709305,
      "grad_norm": 1.6033329963684082,
      "learning_rate": 9.867799811142587e-06,
      "loss": 2.452,
      "step": 22100
    },
    {
      "epoch": 2.4088872547016926,
      "grad_norm": 1.4563558101654053,
      "learning_rate": 9.849640444541294e-06,
      "loss": 2.4098,
      "step": 22110
    },
    {
      "epoch": 2.4099767128324547,
      "grad_norm": 1.5468909740447998,
      "learning_rate": 9.831481077940003e-06,
      "loss": 2.4454,
      "step": 22120
    },
    {
      "epoch": 2.4110661709632173,
      "grad_norm": 1.49873685836792,
      "learning_rate": 9.813321711338708e-06,
      "loss": 2.4236,
      "step": 22130
    },
    {
      "epoch": 2.4121556290939794,
      "grad_norm": 1.573152780532837,
      "learning_rate": 9.795162344737415e-06,
      "loss": 2.3677,
      "step": 22140
    },
    {
      "epoch": 2.4132450872247415,
      "grad_norm": 1.4986222982406616,
      "learning_rate": 9.778818914796252e-06,
      "loss": 2.4193,
      "step": 22150
    },
    {
      "epoch": 2.4143345453555036,
      "grad_norm": 1.5849064588546753,
      "learning_rate": 9.76065954819496e-06,
      "loss": 2.5842,
      "step": 22160
    },
    {
      "epoch": 2.415424003486266,
      "grad_norm": 1.5279312133789062,
      "learning_rate": 9.742500181593666e-06,
      "loss": 2.4121,
      "step": 22170
    },
    {
      "epoch": 2.4165134616170283,
      "grad_norm": 1.4366388320922852,
      "learning_rate": 9.724340814992373e-06,
      "loss": 2.3794,
      "step": 22180
    },
    {
      "epoch": 2.4176029197477904,
      "grad_norm": 1.5991486310958862,
      "learning_rate": 9.70618144839108e-06,
      "loss": 2.4544,
      "step": 22190
    },
    {
      "epoch": 2.4186923778785525,
      "grad_norm": 1.4946433305740356,
      "learning_rate": 9.688022081789789e-06,
      "loss": 2.5017,
      "step": 22200
    },
    {
      "epoch": 2.4197818360093146,
      "grad_norm": 1.519374966621399,
      "learning_rate": 9.669862715188494e-06,
      "loss": 2.3806,
      "step": 22210
    },
    {
      "epoch": 2.420871294140077,
      "grad_norm": 1.5710723400115967,
      "learning_rate": 9.651703348587201e-06,
      "loss": 2.3915,
      "step": 22220
    },
    {
      "epoch": 2.4219607522708393,
      "grad_norm": 1.5410906076431274,
      "learning_rate": 9.633543981985909e-06,
      "loss": 2.4196,
      "step": 22230
    },
    {
      "epoch": 2.4230502104016014,
      "grad_norm": 1.4729292392730713,
      "learning_rate": 9.615384615384616e-06,
      "loss": 2.4324,
      "step": 22240
    },
    {
      "epoch": 2.424139668532364,
      "grad_norm": 1.4569088220596313,
      "learning_rate": 9.597225248783323e-06,
      "loss": 2.3787,
      "step": 22250
    },
    {
      "epoch": 2.425229126663126,
      "grad_norm": 1.5746898651123047,
      "learning_rate": 9.57906588218203e-06,
      "loss": 2.5098,
      "step": 22260
    },
    {
      "epoch": 2.426318584793888,
      "grad_norm": 1.5886846780776978,
      "learning_rate": 9.560906515580737e-06,
      "loss": 2.3669,
      "step": 22270
    },
    {
      "epoch": 2.4274080429246503,
      "grad_norm": 1.5530494451522827,
      "learning_rate": 9.542747148979444e-06,
      "loss": 2.4301,
      "step": 22280
    },
    {
      "epoch": 2.4284975010554124,
      "grad_norm": 1.4697507619857788,
      "learning_rate": 9.524587782378151e-06,
      "loss": 2.3535,
      "step": 22290
    },
    {
      "epoch": 2.429586959186175,
      "grad_norm": 1.5363892316818237,
      "learning_rate": 9.506428415776858e-06,
      "loss": 2.4258,
      "step": 22300
    },
    {
      "epoch": 2.430676417316937,
      "grad_norm": 1.4992318153381348,
      "learning_rate": 9.488269049175564e-06,
      "loss": 2.4338,
      "step": 22310
    },
    {
      "epoch": 2.431765875447699,
      "grad_norm": 1.5759329795837402,
      "learning_rate": 9.470109682574273e-06,
      "loss": 2.3934,
      "step": 22320
    },
    {
      "epoch": 2.4328553335784613,
      "grad_norm": 1.5372257232666016,
      "learning_rate": 9.45195031597298e-06,
      "loss": 2.2631,
      "step": 22330
    },
    {
      "epoch": 2.433944791709224,
      "grad_norm": 1.6318734884262085,
      "learning_rate": 9.433790949371687e-06,
      "loss": 2.4371,
      "step": 22340
    },
    {
      "epoch": 2.435034249839986,
      "grad_norm": 1.6307584047317505,
      "learning_rate": 9.415631582770392e-06,
      "loss": 2.4011,
      "step": 22350
    },
    {
      "epoch": 2.436123707970748,
      "grad_norm": 1.481444239616394,
      "learning_rate": 9.397472216169101e-06,
      "loss": 2.4492,
      "step": 22360
    },
    {
      "epoch": 2.43721316610151,
      "grad_norm": 1.6153435707092285,
      "learning_rate": 9.379312849567808e-06,
      "loss": 2.4619,
      "step": 22370
    },
    {
      "epoch": 2.4383026242322723,
      "grad_norm": 1.5384844541549683,
      "learning_rate": 9.361153482966514e-06,
      "loss": 2.3827,
      "step": 22380
    },
    {
      "epoch": 2.439392082363035,
      "grad_norm": 1.4856072664260864,
      "learning_rate": 9.34299411636522e-06,
      "loss": 2.4842,
      "step": 22390
    },
    {
      "epoch": 2.440481540493797,
      "grad_norm": 1.603208065032959,
      "learning_rate": 9.32483474976393e-06,
      "loss": 2.4647,
      "step": 22400
    },
    {
      "epoch": 2.441570998624559,
      "grad_norm": 1.5022481679916382,
      "learning_rate": 9.306675383162635e-06,
      "loss": 2.4329,
      "step": 22410
    },
    {
      "epoch": 2.442660456755321,
      "grad_norm": 1.5063875913619995,
      "learning_rate": 9.288516016561342e-06,
      "loss": 2.3713,
      "step": 22420
    },
    {
      "epoch": 2.4437499148860837,
      "grad_norm": 1.5484460592269897,
      "learning_rate": 9.270356649960049e-06,
      "loss": 2.4019,
      "step": 22430
    },
    {
      "epoch": 2.444839373016846,
      "grad_norm": 1.6005979776382446,
      "learning_rate": 9.252197283358758e-06,
      "loss": 2.4878,
      "step": 22440
    },
    {
      "epoch": 2.445928831147608,
      "grad_norm": 1.5679371356964111,
      "learning_rate": 9.234037916757463e-06,
      "loss": 2.3603,
      "step": 22450
    },
    {
      "epoch": 2.44701828927837,
      "grad_norm": 1.5158823728561401,
      "learning_rate": 9.21587855015617e-06,
      "loss": 2.4322,
      "step": 22460
    },
    {
      "epoch": 2.448107747409132,
      "grad_norm": 1.5584237575531006,
      "learning_rate": 9.19771918355488e-06,
      "loss": 2.4282,
      "step": 22470
    },
    {
      "epoch": 2.4491972055398947,
      "grad_norm": 1.4606866836547852,
      "learning_rate": 9.179559816953585e-06,
      "loss": 2.4155,
      "step": 22480
    },
    {
      "epoch": 2.450286663670657,
      "grad_norm": 1.5392327308654785,
      "learning_rate": 9.161400450352292e-06,
      "loss": 2.4574,
      "step": 22490
    },
    {
      "epoch": 2.451376121801419,
      "grad_norm": 1.5486137866973877,
      "learning_rate": 9.143241083750999e-06,
      "loss": 2.4116,
      "step": 22500
    },
    {
      "epoch": 2.452465579932181,
      "grad_norm": 1.573250651359558,
      "learning_rate": 9.125081717149708e-06,
      "loss": 2.5052,
      "step": 22510
    },
    {
      "epoch": 2.4535550380629436,
      "grad_norm": 1.48513662815094,
      "learning_rate": 9.106922350548413e-06,
      "loss": 2.4406,
      "step": 22520
    },
    {
      "epoch": 2.4546444961937057,
      "grad_norm": 1.57499098777771,
      "learning_rate": 9.08876298394712e-06,
      "loss": 2.4516,
      "step": 22530
    },
    {
      "epoch": 2.455733954324468,
      "grad_norm": 1.5216529369354248,
      "learning_rate": 9.070603617345827e-06,
      "loss": 2.3894,
      "step": 22540
    },
    {
      "epoch": 2.45682341245523,
      "grad_norm": 1.5344196557998657,
      "learning_rate": 9.052444250744534e-06,
      "loss": 2.3805,
      "step": 22550
    },
    {
      "epoch": 2.457912870585992,
      "grad_norm": 1.4533566236495972,
      "learning_rate": 9.034284884143242e-06,
      "loss": 2.4211,
      "step": 22560
    },
    {
      "epoch": 2.4590023287167546,
      "grad_norm": 1.4840524196624756,
      "learning_rate": 9.016125517541949e-06,
      "loss": 2.4062,
      "step": 22570
    },
    {
      "epoch": 2.4600917868475167,
      "grad_norm": 1.5795091390609741,
      "learning_rate": 8.997966150940656e-06,
      "loss": 2.3902,
      "step": 22580
    },
    {
      "epoch": 2.461181244978279,
      "grad_norm": 1.554925560951233,
      "learning_rate": 8.979806784339363e-06,
      "loss": 2.3646,
      "step": 22590
    },
    {
      "epoch": 2.462270703109041,
      "grad_norm": 1.5138006210327148,
      "learning_rate": 8.96164741773807e-06,
      "loss": 2.4069,
      "step": 22600
    },
    {
      "epoch": 2.4633601612398035,
      "grad_norm": 1.645383596420288,
      "learning_rate": 8.943488051136777e-06,
      "loss": 2.4424,
      "step": 22610
    },
    {
      "epoch": 2.4644496193705656,
      "grad_norm": 1.484567642211914,
      "learning_rate": 8.925328684535483e-06,
      "loss": 2.3299,
      "step": 22620
    },
    {
      "epoch": 2.4655390775013277,
      "grad_norm": 1.537542700767517,
      "learning_rate": 8.907169317934191e-06,
      "loss": 2.4418,
      "step": 22630
    },
    {
      "epoch": 2.46662853563209,
      "grad_norm": 1.5750044584274292,
      "learning_rate": 8.889009951332898e-06,
      "loss": 2.4746,
      "step": 22640
    },
    {
      "epoch": 2.4677179937628524,
      "grad_norm": 1.6182581186294556,
      "learning_rate": 8.870850584731606e-06,
      "loss": 2.4182,
      "step": 22650
    },
    {
      "epoch": 2.4688074518936145,
      "grad_norm": 1.53285813331604,
      "learning_rate": 8.852691218130311e-06,
      "loss": 2.4395,
      "step": 22660
    },
    {
      "epoch": 2.4698969100243766,
      "grad_norm": 1.538846492767334,
      "learning_rate": 8.83453185152902e-06,
      "loss": 2.455,
      "step": 22670
    },
    {
      "epoch": 2.4709863681551387,
      "grad_norm": 1.60007905960083,
      "learning_rate": 8.816372484927727e-06,
      "loss": 2.4837,
      "step": 22680
    },
    {
      "epoch": 2.4720758262859013,
      "grad_norm": 1.481836199760437,
      "learning_rate": 8.798213118326432e-06,
      "loss": 2.3383,
      "step": 22690
    },
    {
      "epoch": 2.4731652844166634,
      "grad_norm": 1.6198980808258057,
      "learning_rate": 8.78005375172514e-06,
      "loss": 2.3371,
      "step": 22700
    },
    {
      "epoch": 2.4742547425474255,
      "grad_norm": 1.5242875814437866,
      "learning_rate": 8.761894385123848e-06,
      "loss": 2.4992,
      "step": 22710
    },
    {
      "epoch": 2.4753442006781876,
      "grad_norm": 1.5260133743286133,
      "learning_rate": 8.743735018522554e-06,
      "loss": 2.4197,
      "step": 22720
    },
    {
      "epoch": 2.4764336588089497,
      "grad_norm": 1.5656543970108032,
      "learning_rate": 8.72557565192126e-06,
      "loss": 2.3564,
      "step": 22730
    },
    {
      "epoch": 2.4775231169397123,
      "grad_norm": 1.443624496459961,
      "learning_rate": 8.707416285319968e-06,
      "loss": 2.4274,
      "step": 22740
    },
    {
      "epoch": 2.4786125750704744,
      "grad_norm": 1.5298550128936768,
      "learning_rate": 8.689256918718677e-06,
      "loss": 2.4812,
      "step": 22750
    },
    {
      "epoch": 2.4797020332012365,
      "grad_norm": 1.5321458578109741,
      "learning_rate": 8.671097552117382e-06,
      "loss": 2.3872,
      "step": 22760
    },
    {
      "epoch": 2.4807914913319986,
      "grad_norm": 1.6565901041030884,
      "learning_rate": 8.65293818551609e-06,
      "loss": 2.4494,
      "step": 22770
    },
    {
      "epoch": 2.481880949462761,
      "grad_norm": 1.5284149646759033,
      "learning_rate": 8.634778818914796e-06,
      "loss": 2.3523,
      "step": 22780
    },
    {
      "epoch": 2.4829704075935233,
      "grad_norm": 1.5550391674041748,
      "learning_rate": 8.616619452313503e-06,
      "loss": 2.3762,
      "step": 22790
    },
    {
      "epoch": 2.4840598657242854,
      "grad_norm": 1.545842170715332,
      "learning_rate": 8.59846008571221e-06,
      "loss": 2.4472,
      "step": 22800
    },
    {
      "epoch": 2.4851493238550475,
      "grad_norm": 1.6181128025054932,
      "learning_rate": 8.580300719110918e-06,
      "loss": 2.3657,
      "step": 22810
    },
    {
      "epoch": 2.4862387819858096,
      "grad_norm": 1.6292009353637695,
      "learning_rate": 8.562141352509625e-06,
      "loss": 2.5408,
      "step": 22820
    },
    {
      "epoch": 2.487328240116572,
      "grad_norm": 1.6480885744094849,
      "learning_rate": 8.543981985908332e-06,
      "loss": 2.552,
      "step": 22830
    },
    {
      "epoch": 2.4884176982473343,
      "grad_norm": 1.4644482135772705,
      "learning_rate": 8.525822619307039e-06,
      "loss": 2.3199,
      "step": 22840
    },
    {
      "epoch": 2.4895071563780964,
      "grad_norm": 1.516018271446228,
      "learning_rate": 8.507663252705746e-06,
      "loss": 2.3458,
      "step": 22850
    },
    {
      "epoch": 2.4905966145088585,
      "grad_norm": 1.5786218643188477,
      "learning_rate": 8.489503886104453e-06,
      "loss": 2.516,
      "step": 22860
    },
    {
      "epoch": 2.491686072639621,
      "grad_norm": 1.541576623916626,
      "learning_rate": 8.47134451950316e-06,
      "loss": 2.4818,
      "step": 22870
    },
    {
      "epoch": 2.492775530770383,
      "grad_norm": 1.563849925994873,
      "learning_rate": 8.453185152901867e-06,
      "loss": 2.3124,
      "step": 22880
    },
    {
      "epoch": 2.4938649889011453,
      "grad_norm": 1.5462729930877686,
      "learning_rate": 8.435025786300575e-06,
      "loss": 2.5298,
      "step": 22890
    },
    {
      "epoch": 2.4949544470319074,
      "grad_norm": 1.4766058921813965,
      "learning_rate": 8.416866419699282e-06,
      "loss": 2.4598,
      "step": 22900
    },
    {
      "epoch": 2.4960439051626695,
      "grad_norm": 1.5512654781341553,
      "learning_rate": 8.398707053097989e-06,
      "loss": 2.4268,
      "step": 22910
    },
    {
      "epoch": 2.497133363293432,
      "grad_norm": 1.5683116912841797,
      "learning_rate": 8.380547686496696e-06,
      "loss": 2.5142,
      "step": 22920
    },
    {
      "epoch": 2.498222821424194,
      "grad_norm": 1.4537410736083984,
      "learning_rate": 8.362388319895401e-06,
      "loss": 2.4084,
      "step": 22930
    },
    {
      "epoch": 2.4993122795549563,
      "grad_norm": 1.500466227531433,
      "learning_rate": 8.34422895329411e-06,
      "loss": 2.3778,
      "step": 22940
    },
    {
      "epoch": 2.500401737685719,
      "grad_norm": 1.530022382736206,
      "learning_rate": 8.326069586692817e-06,
      "loss": 2.4307,
      "step": 22950
    },
    {
      "epoch": 2.501491195816481,
      "grad_norm": 1.5220497846603394,
      "learning_rate": 8.307910220091523e-06,
      "loss": 2.3941,
      "step": 22960
    },
    {
      "epoch": 2.502580653947243,
      "grad_norm": 1.5459339618682861,
      "learning_rate": 8.28975085349023e-06,
      "loss": 2.3977,
      "step": 22970
    },
    {
      "epoch": 2.503670112078005,
      "grad_norm": 1.6495088338851929,
      "learning_rate": 8.271591486888939e-06,
      "loss": 2.5757,
      "step": 22980
    },
    {
      "epoch": 2.5047595702087673,
      "grad_norm": 1.5793768167495728,
      "learning_rate": 8.253432120287646e-06,
      "loss": 2.4513,
      "step": 22990
    },
    {
      "epoch": 2.5058490283395294,
      "grad_norm": 1.5000954866409302,
      "learning_rate": 8.235272753686351e-06,
      "loss": 2.4379,
      "step": 23000
    },
    {
      "epoch": 2.506938486470292,
      "grad_norm": 1.5415053367614746,
      "learning_rate": 8.217113387085058e-06,
      "loss": 2.3181,
      "step": 23010
    },
    {
      "epoch": 2.508027944601054,
      "grad_norm": 1.4615602493286133,
      "learning_rate": 8.198954020483767e-06,
      "loss": 2.4264,
      "step": 23020
    },
    {
      "epoch": 2.509117402731816,
      "grad_norm": 1.4763121604919434,
      "learning_rate": 8.180794653882472e-06,
      "loss": 2.405,
      "step": 23030
    },
    {
      "epoch": 2.5102068608625787,
      "grad_norm": 1.4712843894958496,
      "learning_rate": 8.16263528728118e-06,
      "loss": 2.4523,
      "step": 23040
    },
    {
      "epoch": 2.511296318993341,
      "grad_norm": 1.5554863214492798,
      "learning_rate": 8.144475920679887e-06,
      "loss": 2.4536,
      "step": 23050
    },
    {
      "epoch": 2.512385777124103,
      "grad_norm": 1.5022505521774292,
      "learning_rate": 8.126316554078594e-06,
      "loss": 2.398,
      "step": 23060
    },
    {
      "epoch": 2.513475235254865,
      "grad_norm": 1.561545729637146,
      "learning_rate": 8.108157187477301e-06,
      "loss": 2.3548,
      "step": 23070
    },
    {
      "epoch": 2.514564693385627,
      "grad_norm": 1.4904789924621582,
      "learning_rate": 8.089997820876008e-06,
      "loss": 2.3701,
      "step": 23080
    },
    {
      "epoch": 2.5156541515163893,
      "grad_norm": 1.5455609560012817,
      "learning_rate": 8.071838454274715e-06,
      "loss": 2.4434,
      "step": 23090
    },
    {
      "epoch": 2.516743609647152,
      "grad_norm": 1.5301190614700317,
      "learning_rate": 8.053679087673422e-06,
      "loss": 2.3829,
      "step": 23100
    },
    {
      "epoch": 2.517833067777914,
      "grad_norm": 1.5556825399398804,
      "learning_rate": 8.03551972107213e-06,
      "loss": 2.4414,
      "step": 23110
    },
    {
      "epoch": 2.518922525908676,
      "grad_norm": 1.582098364830017,
      "learning_rate": 8.017360354470836e-06,
      "loss": 2.4124,
      "step": 23120
    },
    {
      "epoch": 2.5200119840394386,
      "grad_norm": 1.5513403415679932,
      "learning_rate": 7.999200987869544e-06,
      "loss": 2.413,
      "step": 23130
    },
    {
      "epoch": 2.5211014421702007,
      "grad_norm": 1.6143828630447388,
      "learning_rate": 7.98104162126825e-06,
      "loss": 2.5295,
      "step": 23140
    },
    {
      "epoch": 2.522190900300963,
      "grad_norm": 1.4795482158660889,
      "learning_rate": 7.962882254666958e-06,
      "loss": 2.4789,
      "step": 23150
    },
    {
      "epoch": 2.523280358431725,
      "grad_norm": 1.434798240661621,
      "learning_rate": 7.944722888065665e-06,
      "loss": 2.3053,
      "step": 23160
    },
    {
      "epoch": 2.524369816562487,
      "grad_norm": 1.529524326324463,
      "learning_rate": 7.92656352146437e-06,
      "loss": 2.3593,
      "step": 23170
    },
    {
      "epoch": 2.5254592746932496,
      "grad_norm": 1.5658003091812134,
      "learning_rate": 7.908404154863079e-06,
      "loss": 2.4143,
      "step": 23180
    },
    {
      "epoch": 2.5265487328240117,
      "grad_norm": 1.6293716430664062,
      "learning_rate": 7.890244788261786e-06,
      "loss": 2.371,
      "step": 23190
    },
    {
      "epoch": 2.527638190954774,
      "grad_norm": 1.573987364768982,
      "learning_rate": 7.872085421660493e-06,
      "loss": 2.3741,
      "step": 23200
    },
    {
      "epoch": 2.528727649085536,
      "grad_norm": 1.5819119215011597,
      "learning_rate": 7.8539260550592e-06,
      "loss": 2.4916,
      "step": 23210
    },
    {
      "epoch": 2.5298171072162985,
      "grad_norm": 1.7163184881210327,
      "learning_rate": 7.835766688457908e-06,
      "loss": 2.4207,
      "step": 23220
    },
    {
      "epoch": 2.5309065653470606,
      "grad_norm": 1.4848823547363281,
      "learning_rate": 7.817607321856615e-06,
      "loss": 2.3806,
      "step": 23230
    },
    {
      "epoch": 2.5319960234778227,
      "grad_norm": 1.4961017370224,
      "learning_rate": 7.79944795525532e-06,
      "loss": 2.3729,
      "step": 23240
    },
    {
      "epoch": 2.533085481608585,
      "grad_norm": 1.494682788848877,
      "learning_rate": 7.781288588654029e-06,
      "loss": 2.3645,
      "step": 23250
    },
    {
      "epoch": 2.534174939739347,
      "grad_norm": 1.5449436902999878,
      "learning_rate": 7.763129222052736e-06,
      "loss": 2.4293,
      "step": 23260
    },
    {
      "epoch": 2.5352643978701095,
      "grad_norm": 1.5051735639572144,
      "learning_rate": 7.744969855451441e-06,
      "loss": 2.3918,
      "step": 23270
    },
    {
      "epoch": 2.5363538560008716,
      "grad_norm": 1.5672938823699951,
      "learning_rate": 7.726810488850149e-06,
      "loss": 2.4152,
      "step": 23280
    },
    {
      "epoch": 2.5374433141316337,
      "grad_norm": 1.6379656791687012,
      "learning_rate": 7.708651122248857e-06,
      "loss": 2.4308,
      "step": 23290
    },
    {
      "epoch": 2.5385327722623963,
      "grad_norm": 1.4617900848388672,
      "learning_rate": 7.690491755647564e-06,
      "loss": 2.4758,
      "step": 23300
    },
    {
      "epoch": 2.5396222303931584,
      "grad_norm": 1.6335058212280273,
      "learning_rate": 7.67233238904627e-06,
      "loss": 2.4104,
      "step": 23310
    },
    {
      "epoch": 2.5407116885239205,
      "grad_norm": 1.5492311716079712,
      "learning_rate": 7.654173022444977e-06,
      "loss": 2.3893,
      "step": 23320
    },
    {
      "epoch": 2.5418011466546826,
      "grad_norm": 1.4921501874923706,
      "learning_rate": 7.636013655843686e-06,
      "loss": 2.4423,
      "step": 23330
    },
    {
      "epoch": 2.5428906047854447,
      "grad_norm": 1.500614047050476,
      "learning_rate": 7.617854289242392e-06,
      "loss": 2.3555,
      "step": 23340
    },
    {
      "epoch": 2.543980062916207,
      "grad_norm": 1.6251055002212524,
      "learning_rate": 7.599694922641098e-06,
      "loss": 2.424,
      "step": 23350
    },
    {
      "epoch": 2.5450695210469694,
      "grad_norm": 1.6027779579162598,
      "learning_rate": 7.5815355560398054e-06,
      "loss": 2.4101,
      "step": 23360
    },
    {
      "epoch": 2.5461589791777315,
      "grad_norm": 1.5323528051376343,
      "learning_rate": 7.563376189438513e-06,
      "loss": 2.3905,
      "step": 23370
    },
    {
      "epoch": 2.5472484373084936,
      "grad_norm": 1.5502920150756836,
      "learning_rate": 7.54521682283722e-06,
      "loss": 2.3864,
      "step": 23380
    },
    {
      "epoch": 2.548337895439256,
      "grad_norm": 1.4903005361557007,
      "learning_rate": 7.527057456235927e-06,
      "loss": 2.3736,
      "step": 23390
    },
    {
      "epoch": 2.5494273535700183,
      "grad_norm": 1.5354816913604736,
      "learning_rate": 7.508898089634633e-06,
      "loss": 2.3714,
      "step": 23400
    },
    {
      "epoch": 2.5505168117007804,
      "grad_norm": 1.5077934265136719,
      "learning_rate": 7.490738723033342e-06,
      "loss": 2.3413,
      "step": 23410
    },
    {
      "epoch": 2.5516062698315425,
      "grad_norm": 1.5562057495117188,
      "learning_rate": 7.472579356432048e-06,
      "loss": 2.4048,
      "step": 23420
    },
    {
      "epoch": 2.5526957279623046,
      "grad_norm": 1.5871628522872925,
      "learning_rate": 7.454419989830755e-06,
      "loss": 2.5198,
      "step": 23430
    },
    {
      "epoch": 2.5537851860930667,
      "grad_norm": 1.691644549369812,
      "learning_rate": 7.4362606232294615e-06,
      "loss": 2.4277,
      "step": 23440
    },
    {
      "epoch": 2.5548746442238293,
      "grad_norm": 1.5073399543762207,
      "learning_rate": 7.4181012566281694e-06,
      "loss": 2.4879,
      "step": 23450
    },
    {
      "epoch": 2.5559641023545914,
      "grad_norm": 1.5810790061950684,
      "learning_rate": 7.3999418900268766e-06,
      "loss": 2.5083,
      "step": 23460
    },
    {
      "epoch": 2.5570535604853535,
      "grad_norm": 1.5521082878112793,
      "learning_rate": 7.381782523425583e-06,
      "loss": 2.4342,
      "step": 23470
    },
    {
      "epoch": 2.558143018616116,
      "grad_norm": 1.541200876235962,
      "learning_rate": 7.36362315682429e-06,
      "loss": 2.4362,
      "step": 23480
    },
    {
      "epoch": 2.559232476746878,
      "grad_norm": 1.5182970762252808,
      "learning_rate": 7.345463790222998e-06,
      "loss": 2.4667,
      "step": 23490
    },
    {
      "epoch": 2.5603219348776403,
      "grad_norm": 1.499230980873108,
      "learning_rate": 7.327304423621704e-06,
      "loss": 2.3861,
      "step": 23500
    },
    {
      "epoch": 2.5614113930084024,
      "grad_norm": 1.5565277338027954,
      "learning_rate": 7.309145057020411e-06,
      "loss": 2.2932,
      "step": 23510
    },
    {
      "epoch": 2.5625008511391645,
      "grad_norm": 1.555445909500122,
      "learning_rate": 7.2909856904191175e-06,
      "loss": 2.4037,
      "step": 23520
    },
    {
      "epoch": 2.5635903092699266,
      "grad_norm": 1.6211766004562378,
      "learning_rate": 7.272826323817826e-06,
      "loss": 2.4251,
      "step": 23530
    },
    {
      "epoch": 2.564679767400689,
      "grad_norm": 1.4989039897918701,
      "learning_rate": 7.254666957216533e-06,
      "loss": 2.45,
      "step": 23540
    },
    {
      "epoch": 2.5657692255314513,
      "grad_norm": 1.5405588150024414,
      "learning_rate": 7.23650759061524e-06,
      "loss": 2.3692,
      "step": 23550
    },
    {
      "epoch": 2.5668586836622134,
      "grad_norm": 1.6930623054504395,
      "learning_rate": 7.218348224013946e-06,
      "loss": 2.4823,
      "step": 23560
    },
    {
      "epoch": 2.567948141792976,
      "grad_norm": 1.5572792291641235,
      "learning_rate": 7.200188857412654e-06,
      "loss": 2.3438,
      "step": 23570
    },
    {
      "epoch": 2.569037599923738,
      "grad_norm": 1.624523401260376,
      "learning_rate": 7.182029490811361e-06,
      "loss": 2.4533,
      "step": 23580
    },
    {
      "epoch": 2.5701270580545,
      "grad_norm": 1.520668864250183,
      "learning_rate": 7.163870124210067e-06,
      "loss": 2.379,
      "step": 23590
    },
    {
      "epoch": 2.5712165161852623,
      "grad_norm": 1.595690369606018,
      "learning_rate": 7.145710757608775e-06,
      "loss": 2.3917,
      "step": 23600
    },
    {
      "epoch": 2.5723059743160244,
      "grad_norm": 1.598052978515625,
      "learning_rate": 7.127551391007482e-06,
      "loss": 2.4559,
      "step": 23610
    },
    {
      "epoch": 2.573395432446787,
      "grad_norm": 1.549106240272522,
      "learning_rate": 7.109392024406189e-06,
      "loss": 2.5173,
      "step": 23620
    },
    {
      "epoch": 2.574484890577549,
      "grad_norm": 1.5355671644210815,
      "learning_rate": 7.091232657804896e-06,
      "loss": 2.5183,
      "step": 23630
    },
    {
      "epoch": 2.575574348708311,
      "grad_norm": 1.5273642539978027,
      "learning_rate": 7.073073291203604e-06,
      "loss": 2.3973,
      "step": 23640
    },
    {
      "epoch": 2.5766638068390733,
      "grad_norm": 1.5852793455123901,
      "learning_rate": 7.054913924602311e-06,
      "loss": 2.3935,
      "step": 23650
    },
    {
      "epoch": 2.577753264969836,
      "grad_norm": 1.622728705406189,
      "learning_rate": 7.036754558001017e-06,
      "loss": 2.4514,
      "step": 23660
    },
    {
      "epoch": 2.578842723100598,
      "grad_norm": 1.5612868070602417,
      "learning_rate": 7.018595191399724e-06,
      "loss": 2.4465,
      "step": 23670
    },
    {
      "epoch": 2.57993218123136,
      "grad_norm": 1.5503475666046143,
      "learning_rate": 7.000435824798432e-06,
      "loss": 2.3662,
      "step": 23680
    },
    {
      "epoch": 2.581021639362122,
      "grad_norm": 1.5132172107696533,
      "learning_rate": 6.9822764581971384e-06,
      "loss": 2.4024,
      "step": 23690
    },
    {
      "epoch": 2.5821110974928843,
      "grad_norm": 1.5671298503875732,
      "learning_rate": 6.9641170915958455e-06,
      "loss": 2.4205,
      "step": 23700
    },
    {
      "epoch": 2.583200555623647,
      "grad_norm": 1.5702300071716309,
      "learning_rate": 6.945957724994552e-06,
      "loss": 2.3721,
      "step": 23710
    },
    {
      "epoch": 2.584290013754409,
      "grad_norm": 1.5349293947219849,
      "learning_rate": 6.92779835839326e-06,
      "loss": 2.3971,
      "step": 23720
    },
    {
      "epoch": 2.585379471885171,
      "grad_norm": 1.577956199645996,
      "learning_rate": 6.909638991791967e-06,
      "loss": 2.4211,
      "step": 23730
    },
    {
      "epoch": 2.5864689300159336,
      "grad_norm": 1.6123027801513672,
      "learning_rate": 6.891479625190673e-06,
      "loss": 2.3808,
      "step": 23740
    },
    {
      "epoch": 2.5875583881466957,
      "grad_norm": 1.6157516241073608,
      "learning_rate": 6.87332025858938e-06,
      "loss": 2.42,
      "step": 23750
    },
    {
      "epoch": 2.588647846277458,
      "grad_norm": 1.5411089658737183,
      "learning_rate": 6.855160891988088e-06,
      "loss": 2.3333,
      "step": 23760
    },
    {
      "epoch": 2.58973730440822,
      "grad_norm": 1.525534987449646,
      "learning_rate": 6.837001525386795e-06,
      "loss": 2.538,
      "step": 23770
    },
    {
      "epoch": 2.590826762538982,
      "grad_norm": 1.5509840250015259,
      "learning_rate": 6.818842158785502e-06,
      "loss": 2.3951,
      "step": 23780
    },
    {
      "epoch": 2.591916220669744,
      "grad_norm": 1.5530269145965576,
      "learning_rate": 6.800682792184209e-06,
      "loss": 2.4134,
      "step": 23790
    },
    {
      "epoch": 2.5930056788005067,
      "grad_norm": 1.6682209968566895,
      "learning_rate": 6.782523425582917e-06,
      "loss": 2.4463,
      "step": 23800
    },
    {
      "epoch": 2.594095136931269,
      "grad_norm": 1.6179324388504028,
      "learning_rate": 6.764364058981623e-06,
      "loss": 2.3966,
      "step": 23810
    },
    {
      "epoch": 2.595184595062031,
      "grad_norm": 1.582598328590393,
      "learning_rate": 6.74620469238033e-06,
      "loss": 2.379,
      "step": 23820
    },
    {
      "epoch": 2.5962740531927935,
      "grad_norm": 1.50137197971344,
      "learning_rate": 6.728045325779036e-06,
      "loss": 2.3403,
      "step": 23830
    },
    {
      "epoch": 2.5973635113235556,
      "grad_norm": 1.5114978551864624,
      "learning_rate": 6.709885959177744e-06,
      "loss": 2.392,
      "step": 23840
    },
    {
      "epoch": 2.5984529694543177,
      "grad_norm": 1.6685683727264404,
      "learning_rate": 6.691726592576451e-06,
      "loss": 2.3864,
      "step": 23850
    },
    {
      "epoch": 2.59954242758508,
      "grad_norm": 1.5519784688949585,
      "learning_rate": 6.673567225975158e-06,
      "loss": 2.4092,
      "step": 23860
    },
    {
      "epoch": 2.600631885715842,
      "grad_norm": 1.5343645811080933,
      "learning_rate": 6.655407859373865e-06,
      "loss": 2.3518,
      "step": 23870
    },
    {
      "epoch": 2.601721343846604,
      "grad_norm": 1.5510021448135376,
      "learning_rate": 6.637248492772573e-06,
      "loss": 2.3969,
      "step": 23880
    },
    {
      "epoch": 2.6028108019773666,
      "grad_norm": 1.5402107238769531,
      "learning_rate": 6.61908912617128e-06,
      "loss": 2.4094,
      "step": 23890
    },
    {
      "epoch": 2.6039002601081287,
      "grad_norm": 1.5225422382354736,
      "learning_rate": 6.600929759569986e-06,
      "loss": 2.3638,
      "step": 23900
    },
    {
      "epoch": 2.604989718238891,
      "grad_norm": 1.5719932317733765,
      "learning_rate": 6.582770392968693e-06,
      "loss": 2.4211,
      "step": 23910
    },
    {
      "epoch": 2.6060791763696534,
      "grad_norm": 1.5177565813064575,
      "learning_rate": 6.564611026367401e-06,
      "loss": 2.4415,
      "step": 23920
    },
    {
      "epoch": 2.6071686345004155,
      "grad_norm": 1.654258370399475,
      "learning_rate": 6.546451659766107e-06,
      "loss": 2.415,
      "step": 23930
    },
    {
      "epoch": 2.6082580926311776,
      "grad_norm": 1.5393223762512207,
      "learning_rate": 6.5282922931648145e-06,
      "loss": 2.3478,
      "step": 23940
    },
    {
      "epoch": 2.6093475507619397,
      "grad_norm": 1.5866661071777344,
      "learning_rate": 6.510132926563521e-06,
      "loss": 2.5159,
      "step": 23950
    },
    {
      "epoch": 2.610437008892702,
      "grad_norm": 1.481484293937683,
      "learning_rate": 6.491973559962229e-06,
      "loss": 2.2999,
      "step": 23960
    },
    {
      "epoch": 2.6115264670234644,
      "grad_norm": 1.5146321058273315,
      "learning_rate": 6.473814193360936e-06,
      "loss": 2.4081,
      "step": 23970
    },
    {
      "epoch": 2.6126159251542265,
      "grad_norm": 1.5088930130004883,
      "learning_rate": 6.455654826759643e-06,
      "loss": 2.4233,
      "step": 23980
    },
    {
      "epoch": 2.6137053832849886,
      "grad_norm": 1.5478949546813965,
      "learning_rate": 6.437495460158351e-06,
      "loss": 2.4147,
      "step": 23990
    },
    {
      "epoch": 2.6147948414157507,
      "grad_norm": 1.6203930377960205,
      "learning_rate": 6.419336093557057e-06,
      "loss": 2.5109,
      "step": 24000
    },
    {
      "epoch": 2.6158842995465132,
      "grad_norm": 1.5652077198028564,
      "learning_rate": 6.401176726955764e-06,
      "loss": 2.4038,
      "step": 24010
    },
    {
      "epoch": 2.6169737576772754,
      "grad_norm": 1.6047625541687012,
      "learning_rate": 6.383017360354471e-06,
      "loss": 2.4383,
      "step": 24020
    },
    {
      "epoch": 2.6180632158080375,
      "grad_norm": 1.4674829244613647,
      "learning_rate": 6.3648579937531785e-06,
      "loss": 2.392,
      "step": 24030
    },
    {
      "epoch": 2.6191526739387996,
      "grad_norm": 1.6150555610656738,
      "learning_rate": 6.346698627151886e-06,
      "loss": 2.481,
      "step": 24040
    },
    {
      "epoch": 2.6202421320695617,
      "grad_norm": 1.551034688949585,
      "learning_rate": 6.328539260550592e-06,
      "loss": 2.3786,
      "step": 24050
    },
    {
      "epoch": 2.6213315902003242,
      "grad_norm": 1.5904242992401123,
      "learning_rate": 6.310379893949299e-06,
      "loss": 2.4319,
      "step": 24060
    },
    {
      "epoch": 2.6224210483310864,
      "grad_norm": 1.5611331462860107,
      "learning_rate": 6.292220527348007e-06,
      "loss": 2.3633,
      "step": 24070
    },
    {
      "epoch": 2.6235105064618485,
      "grad_norm": 1.401452660560608,
      "learning_rate": 6.274061160746714e-06,
      "loss": 2.3736,
      "step": 24080
    },
    {
      "epoch": 2.624599964592611,
      "grad_norm": 1.6419581174850464,
      "learning_rate": 6.25590179414542e-06,
      "loss": 2.4638,
      "step": 24090
    },
    {
      "epoch": 2.625689422723373,
      "grad_norm": 1.4309405088424683,
      "learning_rate": 6.2377424275441275e-06,
      "loss": 2.3875,
      "step": 24100
    },
    {
      "epoch": 2.6267788808541352,
      "grad_norm": 1.5660817623138428,
      "learning_rate": 6.219583060942835e-06,
      "loss": 2.4756,
      "step": 24110
    },
    {
      "epoch": 2.6278683389848974,
      "grad_norm": 1.537647008895874,
      "learning_rate": 6.201423694341542e-06,
      "loss": 2.3576,
      "step": 24120
    },
    {
      "epoch": 2.6289577971156595,
      "grad_norm": 1.5601904392242432,
      "learning_rate": 6.183264327740249e-06,
      "loss": 2.4973,
      "step": 24130
    },
    {
      "epoch": 2.6300472552464216,
      "grad_norm": 1.6200302839279175,
      "learning_rate": 6.165104961138956e-06,
      "loss": 2.4274,
      "step": 24140
    },
    {
      "epoch": 2.631136713377184,
      "grad_norm": 1.4998120069503784,
      "learning_rate": 6.146945594537663e-06,
      "loss": 2.4282,
      "step": 24150
    },
    {
      "epoch": 2.6322261715079462,
      "grad_norm": 1.5458345413208008,
      "learning_rate": 6.12878622793637e-06,
      "loss": 2.4608,
      "step": 24160
    },
    {
      "epoch": 2.6333156296387084,
      "grad_norm": 1.5054696798324585,
      "learning_rate": 6.110626861335076e-06,
      "loss": 2.3607,
      "step": 24170
    },
    {
      "epoch": 2.634405087769471,
      "grad_norm": 1.6035746335983276,
      "learning_rate": 6.092467494733784e-06,
      "loss": 2.4062,
      "step": 24180
    },
    {
      "epoch": 2.635494545900233,
      "grad_norm": 1.5135737657546997,
      "learning_rate": 6.074308128132491e-06,
      "loss": 2.4124,
      "step": 24190
    },
    {
      "epoch": 2.636584004030995,
      "grad_norm": 1.708957314491272,
      "learning_rate": 6.056148761531199e-06,
      "loss": 2.3875,
      "step": 24200
    },
    {
      "epoch": 2.6376734621617572,
      "grad_norm": 1.5794126987457275,
      "learning_rate": 6.037989394929905e-06,
      "loss": 2.5055,
      "step": 24210
    },
    {
      "epoch": 2.6387629202925194,
      "grad_norm": 1.5263913869857788,
      "learning_rate": 6.019830028328612e-06,
      "loss": 2.3926,
      "step": 24220
    },
    {
      "epoch": 2.6398523784232815,
      "grad_norm": 1.5442323684692383,
      "learning_rate": 6.001670661727319e-06,
      "loss": 2.4273,
      "step": 24230
    },
    {
      "epoch": 2.640941836554044,
      "grad_norm": 1.5299626588821411,
      "learning_rate": 5.983511295126026e-06,
      "loss": 2.4135,
      "step": 24240
    },
    {
      "epoch": 2.642031294684806,
      "grad_norm": 1.6098664999008179,
      "learning_rate": 5.965351928524733e-06,
      "loss": 2.4698,
      "step": 24250
    },
    {
      "epoch": 2.6431207528155682,
      "grad_norm": 1.590052604675293,
      "learning_rate": 5.94719256192344e-06,
      "loss": 2.4561,
      "step": 24260
    },
    {
      "epoch": 2.644210210946331,
      "grad_norm": 1.4424848556518555,
      "learning_rate": 5.9290331953221475e-06,
      "loss": 2.3594,
      "step": 24270
    },
    {
      "epoch": 2.645299669077093,
      "grad_norm": 1.4671884775161743,
      "learning_rate": 5.910873828720855e-06,
      "loss": 2.4816,
      "step": 24280
    },
    {
      "epoch": 2.646389127207855,
      "grad_norm": 1.5277612209320068,
      "learning_rate": 5.892714462119561e-06,
      "loss": 2.4988,
      "step": 24290
    },
    {
      "epoch": 2.647478585338617,
      "grad_norm": 1.611807942390442,
      "learning_rate": 5.874555095518269e-06,
      "loss": 2.4088,
      "step": 24300
    },
    {
      "epoch": 2.6485680434693792,
      "grad_norm": 1.6864653825759888,
      "learning_rate": 5.856395728916975e-06,
      "loss": 2.4,
      "step": 24310
    },
    {
      "epoch": 2.649657501600142,
      "grad_norm": 1.6055645942687988,
      "learning_rate": 5.838236362315683e-06,
      "loss": 2.369,
      "step": 24320
    },
    {
      "epoch": 2.650746959730904,
      "grad_norm": 1.5712348222732544,
      "learning_rate": 5.82007699571439e-06,
      "loss": 2.4452,
      "step": 24330
    },
    {
      "epoch": 2.651836417861666,
      "grad_norm": 1.498620867729187,
      "learning_rate": 5.8019176291130965e-06,
      "loss": 2.357,
      "step": 24340
    },
    {
      "epoch": 2.652925875992428,
      "grad_norm": 1.5736273527145386,
      "learning_rate": 5.783758262511804e-06,
      "loss": 2.3784,
      "step": 24350
    },
    {
      "epoch": 2.6540153341231907,
      "grad_norm": 1.5415524244308472,
      "learning_rate": 5.765598895910511e-06,
      "loss": 2.4243,
      "step": 24360
    },
    {
      "epoch": 2.655104792253953,
      "grad_norm": 1.5516713857650757,
      "learning_rate": 5.747439529309219e-06,
      "loss": 2.3954,
      "step": 24370
    },
    {
      "epoch": 2.656194250384715,
      "grad_norm": 1.5455604791641235,
      "learning_rate": 5.729280162707925e-06,
      "loss": 2.386,
      "step": 24380
    },
    {
      "epoch": 2.657283708515477,
      "grad_norm": 1.5212256908416748,
      "learning_rate": 5.711120796106632e-06,
      "loss": 2.4751,
      "step": 24390
    },
    {
      "epoch": 2.658373166646239,
      "grad_norm": 1.6204187870025635,
      "learning_rate": 5.692961429505339e-06,
      "loss": 2.4359,
      "step": 24400
    },
    {
      "epoch": 2.6594626247770017,
      "grad_norm": 1.5984309911727905,
      "learning_rate": 5.674802062904046e-06,
      "loss": 2.428,
      "step": 24410
    },
    {
      "epoch": 2.660552082907764,
      "grad_norm": 1.519423246383667,
      "learning_rate": 5.656642696302753e-06,
      "loss": 2.4079,
      "step": 24420
    },
    {
      "epoch": 2.661641541038526,
      "grad_norm": 1.5334819555282593,
      "learning_rate": 5.6384833297014605e-06,
      "loss": 2.4841,
      "step": 24430
    },
    {
      "epoch": 2.662730999169288,
      "grad_norm": 1.4564801454544067,
      "learning_rate": 5.6203239631001676e-06,
      "loss": 2.4188,
      "step": 24440
    },
    {
      "epoch": 2.6638204573000506,
      "grad_norm": 1.592260479927063,
      "learning_rate": 5.602164596498875e-06,
      "loss": 2.4064,
      "step": 24450
    },
    {
      "epoch": 2.6649099154308127,
      "grad_norm": 1.5695338249206543,
      "learning_rate": 5.584005229897581e-06,
      "loss": 2.324,
      "step": 24460
    },
    {
      "epoch": 2.665999373561575,
      "grad_norm": 1.578718900680542,
      "learning_rate": 5.565845863296289e-06,
      "loss": 2.3418,
      "step": 24470
    },
    {
      "epoch": 2.667088831692337,
      "grad_norm": 1.630568265914917,
      "learning_rate": 5.547686496694995e-06,
      "loss": 2.4884,
      "step": 24480
    },
    {
      "epoch": 2.668178289823099,
      "grad_norm": 1.4168506860733032,
      "learning_rate": 5.529527130093703e-06,
      "loss": 2.326,
      "step": 24490
    },
    {
      "epoch": 2.6692677479538616,
      "grad_norm": 1.5894562005996704,
      "learning_rate": 5.511367763492409e-06,
      "loss": 2.4092,
      "step": 24500
    },
    {
      "epoch": 2.6703572060846237,
      "grad_norm": 1.513567328453064,
      "learning_rate": 5.4932083968911165e-06,
      "loss": 2.4183,
      "step": 24510
    },
    {
      "epoch": 2.671446664215386,
      "grad_norm": 1.5554558038711548,
      "learning_rate": 5.475049030289824e-06,
      "loss": 2.4024,
      "step": 24520
    },
    {
      "epoch": 2.6725361223461483,
      "grad_norm": 1.594049334526062,
      "learning_rate": 5.456889663688531e-06,
      "loss": 2.3521,
      "step": 24530
    },
    {
      "epoch": 2.6736255804769105,
      "grad_norm": 1.5972501039505005,
      "learning_rate": 5.438730297087238e-06,
      "loss": 2.3349,
      "step": 24540
    },
    {
      "epoch": 2.6747150386076726,
      "grad_norm": 1.583831787109375,
      "learning_rate": 5.420570930485945e-06,
      "loss": 2.3678,
      "step": 24550
    },
    {
      "epoch": 2.6758044967384347,
      "grad_norm": 1.618718147277832,
      "learning_rate": 5.402411563884652e-06,
      "loss": 2.3625,
      "step": 24560
    },
    {
      "epoch": 2.676893954869197,
      "grad_norm": 1.5335229635238647,
      "learning_rate": 5.384252197283359e-06,
      "loss": 2.4247,
      "step": 24570
    },
    {
      "epoch": 2.677983412999959,
      "grad_norm": 1.5922166109085083,
      "learning_rate": 5.3660928306820654e-06,
      "loss": 2.4008,
      "step": 24580
    },
    {
      "epoch": 2.6790728711307215,
      "grad_norm": 1.6253917217254639,
      "learning_rate": 5.347933464080773e-06,
      "loss": 2.4265,
      "step": 24590
    },
    {
      "epoch": 2.6801623292614836,
      "grad_norm": 1.5188319683074951,
      "learning_rate": 5.32977409747948e-06,
      "loss": 2.4733,
      "step": 24600
    },
    {
      "epoch": 2.6812517873922457,
      "grad_norm": 1.4390652179718018,
      "learning_rate": 5.311614730878188e-06,
      "loss": 2.3066,
      "step": 24610
    },
    {
      "epoch": 2.6823412455230082,
      "grad_norm": 1.631441354751587,
      "learning_rate": 5.293455364276894e-06,
      "loss": 2.3539,
      "step": 24620
    },
    {
      "epoch": 2.6834307036537703,
      "grad_norm": 1.590954065322876,
      "learning_rate": 5.275295997675601e-06,
      "loss": 2.4785,
      "step": 24630
    },
    {
      "epoch": 2.6845201617845325,
      "grad_norm": 1.5106596946716309,
      "learning_rate": 5.257136631074308e-06,
      "loss": 2.3778,
      "step": 24640
    },
    {
      "epoch": 2.6856096199152946,
      "grad_norm": 1.5532596111297607,
      "learning_rate": 5.238977264473015e-06,
      "loss": 2.417,
      "step": 24650
    },
    {
      "epoch": 2.6866990780460567,
      "grad_norm": 1.6265857219696045,
      "learning_rate": 5.220817897871722e-06,
      "loss": 2.3576,
      "step": 24660
    },
    {
      "epoch": 2.687788536176819,
      "grad_norm": 1.5750482082366943,
      "learning_rate": 5.2026585312704294e-06,
      "loss": 2.3684,
      "step": 24670
    },
    {
      "epoch": 2.6888779943075813,
      "grad_norm": 1.6273009777069092,
      "learning_rate": 5.1844991646691366e-06,
      "loss": 2.4764,
      "step": 24680
    },
    {
      "epoch": 2.6899674524383435,
      "grad_norm": 1.5545066595077515,
      "learning_rate": 5.166339798067844e-06,
      "loss": 2.4441,
      "step": 24690
    },
    {
      "epoch": 2.6910569105691056,
      "grad_norm": 1.4805794954299927,
      "learning_rate": 5.14818043146655e-06,
      "loss": 2.3942,
      "step": 24700
    },
    {
      "epoch": 2.692146368699868,
      "grad_norm": 1.5790296792984009,
      "learning_rate": 5.130021064865258e-06,
      "loss": 2.413,
      "step": 24710
    },
    {
      "epoch": 2.6932358268306302,
      "grad_norm": 1.5319340229034424,
      "learning_rate": 5.111861698263965e-06,
      "loss": 2.4379,
      "step": 24720
    },
    {
      "epoch": 2.6943252849613923,
      "grad_norm": 1.5880658626556396,
      "learning_rate": 5.093702331662672e-06,
      "loss": 2.4241,
      "step": 24730
    },
    {
      "epoch": 2.6954147430921545,
      "grad_norm": 1.526500940322876,
      "learning_rate": 5.075542965061379e-06,
      "loss": 2.2926,
      "step": 24740
    },
    {
      "epoch": 2.6965042012229166,
      "grad_norm": 1.5606515407562256,
      "learning_rate": 5.0573835984600855e-06,
      "loss": 2.3971,
      "step": 24750
    },
    {
      "epoch": 2.697593659353679,
      "grad_norm": 1.5113333463668823,
      "learning_rate": 5.0392242318587935e-06,
      "loss": 2.4612,
      "step": 24760
    },
    {
      "epoch": 2.6986831174844412,
      "grad_norm": 1.5265017747879028,
      "learning_rate": 5.0210648652575e-06,
      "loss": 2.3524,
      "step": 24770
    },
    {
      "epoch": 2.6997725756152033,
      "grad_norm": 1.54143488407135,
      "learning_rate": 5.002905498656208e-06,
      "loss": 2.4414,
      "step": 24780
    },
    {
      "epoch": 2.7008620337459655,
      "grad_norm": 1.5339981317520142,
      "learning_rate": 4.984746132054914e-06,
      "loss": 2.3571,
      "step": 24790
    },
    {
      "epoch": 2.701951491876728,
      "grad_norm": 1.5133551359176636,
      "learning_rate": 4.966586765453621e-06,
      "loss": 2.3707,
      "step": 24800
    },
    {
      "epoch": 2.70304095000749,
      "grad_norm": 1.613998532295227,
      "learning_rate": 4.948427398852328e-06,
      "loss": 2.3896,
      "step": 24810
    },
    {
      "epoch": 2.7041304081382522,
      "grad_norm": 1.6041003465652466,
      "learning_rate": 4.930268032251035e-06,
      "loss": 2.4273,
      "step": 24820
    },
    {
      "epoch": 2.7052198662690143,
      "grad_norm": 1.5440152883529663,
      "learning_rate": 4.912108665649742e-06,
      "loss": 2.3846,
      "step": 24830
    },
    {
      "epoch": 2.7063093243997765,
      "grad_norm": 1.4731650352478027,
      "learning_rate": 4.8939492990484495e-06,
      "loss": 2.4641,
      "step": 24840
    },
    {
      "epoch": 2.707398782530539,
      "grad_norm": 1.4962037801742554,
      "learning_rate": 4.875789932447157e-06,
      "loss": 2.4734,
      "step": 24850
    },
    {
      "epoch": 2.708488240661301,
      "grad_norm": 1.5564110279083252,
      "learning_rate": 4.857630565845864e-06,
      "loss": 2.4209,
      "step": 24860
    },
    {
      "epoch": 2.7095776987920632,
      "grad_norm": 1.5781110525131226,
      "learning_rate": 4.839471199244571e-06,
      "loss": 2.5464,
      "step": 24870
    },
    {
      "epoch": 2.710667156922826,
      "grad_norm": 1.538772463798523,
      "learning_rate": 4.821311832643278e-06,
      "loss": 2.4227,
      "step": 24880
    },
    {
      "epoch": 2.711756615053588,
      "grad_norm": 1.6102571487426758,
      "learning_rate": 4.803152466041984e-06,
      "loss": 2.3593,
      "step": 24890
    },
    {
      "epoch": 2.71284607318435,
      "grad_norm": 1.508875846862793,
      "learning_rate": 4.784993099440692e-06,
      "loss": 2.4416,
      "step": 24900
    },
    {
      "epoch": 2.713935531315112,
      "grad_norm": 1.5025038719177246,
      "learning_rate": 4.7668337328393984e-06,
      "loss": 2.4293,
      "step": 24910
    },
    {
      "epoch": 2.7150249894458742,
      "grad_norm": 1.5331593751907349,
      "learning_rate": 4.748674366238106e-06,
      "loss": 2.5003,
      "step": 24920
    },
    {
      "epoch": 2.7161144475766363,
      "grad_norm": 1.4466392993927002,
      "learning_rate": 4.730514999636813e-06,
      "loss": 2.3627,
      "step": 24930
    },
    {
      "epoch": 2.717203905707399,
      "grad_norm": 1.5364834070205688,
      "learning_rate": 4.71235563303552e-06,
      "loss": 2.3473,
      "step": 24940
    },
    {
      "epoch": 2.718293363838161,
      "grad_norm": 1.5306557416915894,
      "learning_rate": 4.694196266434227e-06,
      "loss": 2.4266,
      "step": 24950
    },
    {
      "epoch": 2.719382821968923,
      "grad_norm": 1.5976356267929077,
      "learning_rate": 4.676036899832934e-06,
      "loss": 2.5213,
      "step": 24960
    },
    {
      "epoch": 2.7204722800996857,
      "grad_norm": 1.5135332345962524,
      "learning_rate": 4.657877533231641e-06,
      "loss": 2.4016,
      "step": 24970
    },
    {
      "epoch": 2.721561738230448,
      "grad_norm": 1.5271490812301636,
      "learning_rate": 4.639718166630348e-06,
      "loss": 2.4196,
      "step": 24980
    },
    {
      "epoch": 2.72265119636121,
      "grad_norm": 1.6257387399673462,
      "learning_rate": 4.621558800029055e-06,
      "loss": 2.3404,
      "step": 24990
    },
    {
      "epoch": 2.723740654491972,
      "grad_norm": 1.5425329208374023,
      "learning_rate": 4.6033994334277624e-06,
      "loss": 2.3369,
      "step": 25000
    },
    {
      "epoch": 2.724830112622734,
      "grad_norm": 1.6353071928024292,
      "learning_rate": 4.585240066826469e-06,
      "loss": 2.3884,
      "step": 25010
    },
    {
      "epoch": 2.7259195707534962,
      "grad_norm": 1.6792325973510742,
      "learning_rate": 4.567080700225177e-06,
      "loss": 2.3931,
      "step": 25020
    },
    {
      "epoch": 2.727009028884259,
      "grad_norm": 1.6183984279632568,
      "learning_rate": 4.548921333623883e-06,
      "loss": 2.3989,
      "step": 25030
    },
    {
      "epoch": 2.728098487015021,
      "grad_norm": 1.5658539533615112,
      "learning_rate": 4.530761967022591e-06,
      "loss": 2.3808,
      "step": 25040
    },
    {
      "epoch": 2.729187945145783,
      "grad_norm": 1.5560442209243774,
      "learning_rate": 4.512602600421297e-06,
      "loss": 2.3283,
      "step": 25050
    },
    {
      "epoch": 2.7302774032765456,
      "grad_norm": 1.638598918914795,
      "learning_rate": 4.494443233820004e-06,
      "loss": 2.5165,
      "step": 25060
    },
    {
      "epoch": 2.7313668614073077,
      "grad_norm": 1.571568250656128,
      "learning_rate": 4.476283867218711e-06,
      "loss": 2.4113,
      "step": 25070
    },
    {
      "epoch": 2.7324563195380698,
      "grad_norm": 1.5600059032440186,
      "learning_rate": 4.4581245006174185e-06,
      "loss": 2.4203,
      "step": 25080
    },
    {
      "epoch": 2.733545777668832,
      "grad_norm": 1.51179039478302,
      "learning_rate": 4.4399651340161264e-06,
      "loss": 2.3652,
      "step": 25090
    },
    {
      "epoch": 2.734635235799594,
      "grad_norm": 1.5186586380004883,
      "learning_rate": 4.421805767414833e-06,
      "loss": 2.3182,
      "step": 25100
    },
    {
      "epoch": 2.7357246939303566,
      "grad_norm": 1.629774808883667,
      "learning_rate": 4.40364640081354e-06,
      "loss": 2.4722,
      "step": 25110
    },
    {
      "epoch": 2.7368141520611187,
      "grad_norm": 1.5502970218658447,
      "learning_rate": 4.385487034212247e-06,
      "loss": 2.3797,
      "step": 25120
    },
    {
      "epoch": 2.7379036101918808,
      "grad_norm": 1.6150323152542114,
      "learning_rate": 4.367327667610954e-06,
      "loss": 2.4441,
      "step": 25130
    },
    {
      "epoch": 2.738993068322643,
      "grad_norm": 1.5997711420059204,
      "learning_rate": 4.349168301009661e-06,
      "loss": 2.4512,
      "step": 25140
    },
    {
      "epoch": 2.7400825264534054,
      "grad_norm": 1.5349235534667969,
      "learning_rate": 4.331008934408368e-06,
      "loss": 2.441,
      "step": 25150
    },
    {
      "epoch": 2.7411719845841676,
      "grad_norm": 1.5175714492797852,
      "learning_rate": 4.312849567807075e-06,
      "loss": 2.3872,
      "step": 25160
    },
    {
      "epoch": 2.7422614427149297,
      "grad_norm": 1.6171603202819824,
      "learning_rate": 4.2946902012057825e-06,
      "loss": 2.4821,
      "step": 25170
    },
    {
      "epoch": 2.7433509008456918,
      "grad_norm": 1.5924477577209473,
      "learning_rate": 4.276530834604489e-06,
      "loss": 2.3317,
      "step": 25180
    },
    {
      "epoch": 2.744440358976454,
      "grad_norm": 1.5642329454421997,
      "learning_rate": 4.258371468003197e-06,
      "loss": 2.421,
      "step": 25190
    },
    {
      "epoch": 2.7455298171072164,
      "grad_norm": 1.5149742364883423,
      "learning_rate": 4.240212101401903e-06,
      "loss": 2.4859,
      "step": 25200
    },
    {
      "epoch": 2.7466192752379786,
      "grad_norm": 1.5664877891540527,
      "learning_rate": 4.222052734800611e-06,
      "loss": 2.4456,
      "step": 25210
    },
    {
      "epoch": 2.7477087333687407,
      "grad_norm": 1.5358304977416992,
      "learning_rate": 4.203893368199317e-06,
      "loss": 2.4817,
      "step": 25220
    },
    {
      "epoch": 2.7487981914995028,
      "grad_norm": 1.5041452646255493,
      "learning_rate": 4.185734001598024e-06,
      "loss": 2.3914,
      "step": 25230
    },
    {
      "epoch": 2.7498876496302653,
      "grad_norm": 1.4735640287399292,
      "learning_rate": 4.1675746349967314e-06,
      "loss": 2.4296,
      "step": 25240
    },
    {
      "epoch": 2.7509771077610274,
      "grad_norm": 1.623412847518921,
      "learning_rate": 4.1494152683954385e-06,
      "loss": 2.3444,
      "step": 25250
    },
    {
      "epoch": 2.7520665658917896,
      "grad_norm": 1.568911075592041,
      "learning_rate": 4.131255901794146e-06,
      "loss": 2.4188,
      "step": 25260
    },
    {
      "epoch": 2.7531560240225517,
      "grad_norm": 1.5692076683044434,
      "learning_rate": 4.113096535192853e-06,
      "loss": 2.4473,
      "step": 25270
    },
    {
      "epoch": 2.7542454821533138,
      "grad_norm": 1.6656173467636108,
      "learning_rate": 4.09493716859156e-06,
      "loss": 2.3931,
      "step": 25280
    },
    {
      "epoch": 2.7553349402840763,
      "grad_norm": 1.6337831020355225,
      "learning_rate": 4.076777801990267e-06,
      "loss": 2.4611,
      "step": 25290
    },
    {
      "epoch": 2.7564243984148384,
      "grad_norm": 1.569606900215149,
      "learning_rate": 4.058618435388973e-06,
      "loss": 2.4222,
      "step": 25300
    },
    {
      "epoch": 2.7575138565456006,
      "grad_norm": 1.653554916381836,
      "learning_rate": 4.040459068787681e-06,
      "loss": 2.4363,
      "step": 25310
    },
    {
      "epoch": 2.758603314676363,
      "grad_norm": 1.618896484375,
      "learning_rate": 4.0222997021863875e-06,
      "loss": 2.4674,
      "step": 25320
    },
    {
      "epoch": 2.759692772807125,
      "grad_norm": 1.6085102558135986,
      "learning_rate": 4.0041403355850954e-06,
      "loss": 2.4244,
      "step": 25330
    },
    {
      "epoch": 2.7607822309378873,
      "grad_norm": 1.4986673593521118,
      "learning_rate": 3.985980968983802e-06,
      "loss": 2.4254,
      "step": 25340
    },
    {
      "epoch": 2.7618716890686494,
      "grad_norm": 1.5261684656143188,
      "learning_rate": 3.967821602382509e-06,
      "loss": 2.386,
      "step": 25350
    },
    {
      "epoch": 2.7629611471994115,
      "grad_norm": 1.611460566520691,
      "learning_rate": 3.949662235781216e-06,
      "loss": 2.339,
      "step": 25360
    },
    {
      "epoch": 2.7640506053301737,
      "grad_norm": 1.5601286888122559,
      "learning_rate": 3.931502869179923e-06,
      "loss": 2.4373,
      "step": 25370
    },
    {
      "epoch": 2.765140063460936,
      "grad_norm": 1.585548758506775,
      "learning_rate": 3.91334350257863e-06,
      "loss": 2.3779,
      "step": 25380
    },
    {
      "epoch": 2.7662295215916983,
      "grad_norm": 1.495926856994629,
      "learning_rate": 3.895184135977337e-06,
      "loss": 2.3848,
      "step": 25390
    },
    {
      "epoch": 2.7673189797224604,
      "grad_norm": 1.5733071565628052,
      "learning_rate": 3.877024769376044e-06,
      "loss": 2.4344,
      "step": 25400
    },
    {
      "epoch": 2.768408437853223,
      "grad_norm": 1.6158586740493774,
      "learning_rate": 3.8588654027747515e-06,
      "loss": 2.3543,
      "step": 25410
    },
    {
      "epoch": 2.769497895983985,
      "grad_norm": 1.6533957719802856,
      "learning_rate": 3.840706036173458e-06,
      "loss": 2.401,
      "step": 25420
    },
    {
      "epoch": 2.770587354114747,
      "grad_norm": 1.5812857151031494,
      "learning_rate": 3.822546669572166e-06,
      "loss": 2.4714,
      "step": 25430
    },
    {
      "epoch": 2.7716768122455093,
      "grad_norm": 1.6226361989974976,
      "learning_rate": 3.8043873029708724e-06,
      "loss": 2.4799,
      "step": 25440
    },
    {
      "epoch": 2.7727662703762714,
      "grad_norm": 1.6298158168792725,
      "learning_rate": 3.7862279363695795e-06,
      "loss": 2.4532,
      "step": 25450
    },
    {
      "epoch": 2.7738557285070335,
      "grad_norm": 1.5823861360549927,
      "learning_rate": 3.768068569768286e-06,
      "loss": 2.4568,
      "step": 25460
    },
    {
      "epoch": 2.774945186637796,
      "grad_norm": 1.651930332183838,
      "learning_rate": 3.7499092031669937e-06,
      "loss": 2.4096,
      "step": 25470
    },
    {
      "epoch": 2.776034644768558,
      "grad_norm": 1.5253393650054932,
      "learning_rate": 3.7317498365657013e-06,
      "loss": 2.3611,
      "step": 25480
    },
    {
      "epoch": 2.7771241028993203,
      "grad_norm": 1.5461293458938599,
      "learning_rate": 3.713590469964408e-06,
      "loss": 2.412,
      "step": 25490
    },
    {
      "epoch": 2.778213561030083,
      "grad_norm": 1.540010929107666,
      "learning_rate": 3.695431103363115e-06,
      "loss": 2.4056,
      "step": 25500
    },
    {
      "epoch": 2.779303019160845,
      "grad_norm": 1.5991132259368896,
      "learning_rate": 3.6772717367618217e-06,
      "loss": 2.3436,
      "step": 25510
    },
    {
      "epoch": 2.780392477291607,
      "grad_norm": 1.6093603372573853,
      "learning_rate": 3.6591123701605293e-06,
      "loss": 2.5097,
      "step": 25520
    },
    {
      "epoch": 2.781481935422369,
      "grad_norm": 1.5563545227050781,
      "learning_rate": 3.640953003559236e-06,
      "loss": 2.3571,
      "step": 25530
    },
    {
      "epoch": 2.7825713935531313,
      "grad_norm": 1.597238302230835,
      "learning_rate": 3.6227936369579435e-06,
      "loss": 2.3613,
      "step": 25540
    },
    {
      "epoch": 2.783660851683894,
      "grad_norm": 1.6357594728469849,
      "learning_rate": 3.60463427035665e-06,
      "loss": 2.3867,
      "step": 25550
    },
    {
      "epoch": 2.784750309814656,
      "grad_norm": 1.5132715702056885,
      "learning_rate": 3.5864749037553573e-06,
      "loss": 2.3227,
      "step": 25560
    },
    {
      "epoch": 2.785839767945418,
      "grad_norm": 1.5511599779129028,
      "learning_rate": 3.568315537154064e-06,
      "loss": 2.3063,
      "step": 25570
    },
    {
      "epoch": 2.78692922607618,
      "grad_norm": 1.5902403593063354,
      "learning_rate": 3.5501561705527715e-06,
      "loss": 2.3891,
      "step": 25580
    },
    {
      "epoch": 2.7880186842069428,
      "grad_norm": 1.5438038110733032,
      "learning_rate": 3.5319968039514782e-06,
      "loss": 2.3417,
      "step": 25590
    },
    {
      "epoch": 2.789108142337705,
      "grad_norm": 1.561235785484314,
      "learning_rate": 3.5138374373501858e-06,
      "loss": 2.3439,
      "step": 25600
    },
    {
      "epoch": 2.790197600468467,
      "grad_norm": 1.5547691583633423,
      "learning_rate": 3.4956780707488924e-06,
      "loss": 2.4203,
      "step": 25610
    },
    {
      "epoch": 2.791287058599229,
      "grad_norm": 1.5693352222442627,
      "learning_rate": 3.4775187041475996e-06,
      "loss": 2.3963,
      "step": 25620
    },
    {
      "epoch": 2.792376516729991,
      "grad_norm": 1.5982030630111694,
      "learning_rate": 3.4593593375463062e-06,
      "loss": 2.3711,
      "step": 25630
    },
    {
      "epoch": 2.7934659748607538,
      "grad_norm": 1.5663505792617798,
      "learning_rate": 3.4411999709450138e-06,
      "loss": 2.4222,
      "step": 25640
    },
    {
      "epoch": 2.794555432991516,
      "grad_norm": 1.5046377182006836,
      "learning_rate": 3.4230406043437205e-06,
      "loss": 2.4023,
      "step": 25650
    },
    {
      "epoch": 2.795644891122278,
      "grad_norm": 1.6219713687896729,
      "learning_rate": 3.404881237742428e-06,
      "loss": 2.3453,
      "step": 25660
    },
    {
      "epoch": 2.7967343492530405,
      "grad_norm": 1.5163766145706177,
      "learning_rate": 3.3867218711411347e-06,
      "loss": 2.4716,
      "step": 25670
    },
    {
      "epoch": 2.7978238073838027,
      "grad_norm": 1.5997297763824463,
      "learning_rate": 3.368562504539842e-06,
      "loss": 2.3411,
      "step": 25680
    },
    {
      "epoch": 2.7989132655145648,
      "grad_norm": 1.5675328969955444,
      "learning_rate": 3.3504031379385485e-06,
      "loss": 2.3686,
      "step": 25690
    },
    {
      "epoch": 2.800002723645327,
      "grad_norm": 1.5901288986206055,
      "learning_rate": 3.332243771337256e-06,
      "loss": 2.4298,
      "step": 25700
    },
    {
      "epoch": 2.801092181776089,
      "grad_norm": 1.5600650310516357,
      "learning_rate": 3.3140844047359627e-06,
      "loss": 2.4151,
      "step": 25710
    },
    {
      "epoch": 2.802181639906851,
      "grad_norm": 1.486994743347168,
      "learning_rate": 3.2959250381346702e-06,
      "loss": 2.345,
      "step": 25720
    },
    {
      "epoch": 2.8032710980376137,
      "grad_norm": 1.5656405687332153,
      "learning_rate": 3.277765671533377e-06,
      "loss": 2.4277,
      "step": 25730
    },
    {
      "epoch": 2.8043605561683758,
      "grad_norm": 1.4874145984649658,
      "learning_rate": 3.259606304932084e-06,
      "loss": 2.3706,
      "step": 25740
    },
    {
      "epoch": 2.805450014299138,
      "grad_norm": 1.549359917640686,
      "learning_rate": 3.2414469383307907e-06,
      "loss": 2.4747,
      "step": 25750
    },
    {
      "epoch": 2.8065394724299004,
      "grad_norm": 1.5778836011886597,
      "learning_rate": 3.2232875717294983e-06,
      "loss": 2.4946,
      "step": 25760
    },
    {
      "epoch": 2.8076289305606625,
      "grad_norm": 1.5167311429977417,
      "learning_rate": 3.205128205128205e-06,
      "loss": 2.4579,
      "step": 25770
    },
    {
      "epoch": 2.8087183886914247,
      "grad_norm": 1.5398752689361572,
      "learning_rate": 3.1869688385269125e-06,
      "loss": 2.3088,
      "step": 25780
    },
    {
      "epoch": 2.8098078468221868,
      "grad_norm": 1.5166808366775513,
      "learning_rate": 3.168809471925619e-06,
      "loss": 2.377,
      "step": 25790
    },
    {
      "epoch": 2.810897304952949,
      "grad_norm": 1.5616730451583862,
      "learning_rate": 3.1506501053243263e-06,
      "loss": 2.4243,
      "step": 25800
    },
    {
      "epoch": 2.811986763083711,
      "grad_norm": 1.5845752954483032,
      "learning_rate": 3.132490738723033e-06,
      "loss": 2.3718,
      "step": 25810
    },
    {
      "epoch": 2.8130762212144735,
      "grad_norm": 1.5597314834594727,
      "learning_rate": 3.1143313721217405e-06,
      "loss": 2.3753,
      "step": 25820
    },
    {
      "epoch": 2.8141656793452356,
      "grad_norm": 1.5161975622177124,
      "learning_rate": 3.0961720055204476e-06,
      "loss": 2.4603,
      "step": 25830
    },
    {
      "epoch": 2.8152551374759978,
      "grad_norm": 1.5134973526000977,
      "learning_rate": 3.0780126389191547e-06,
      "loss": 2.445,
      "step": 25840
    },
    {
      "epoch": 2.8163445956067603,
      "grad_norm": 1.570435881614685,
      "learning_rate": 3.059853272317862e-06,
      "loss": 2.4169,
      "step": 25850
    },
    {
      "epoch": 2.8174340537375224,
      "grad_norm": 1.6212658882141113,
      "learning_rate": 3.0416939057165685e-06,
      "loss": 2.4078,
      "step": 25860
    },
    {
      "epoch": 2.8185235118682845,
      "grad_norm": 1.5408351421356201,
      "learning_rate": 3.0235345391152756e-06,
      "loss": 2.4863,
      "step": 25870
    },
    {
      "epoch": 2.8196129699990466,
      "grad_norm": 1.5711379051208496,
      "learning_rate": 3.0053751725139828e-06,
      "loss": 2.3823,
      "step": 25880
    },
    {
      "epoch": 2.8207024281298088,
      "grad_norm": 1.5824592113494873,
      "learning_rate": 2.98721580591269e-06,
      "loss": 2.4408,
      "step": 25890
    },
    {
      "epoch": 2.8217918862605713,
      "grad_norm": 1.515429139137268,
      "learning_rate": 2.969056439311397e-06,
      "loss": 2.3913,
      "step": 25900
    },
    {
      "epoch": 2.8228813443913334,
      "grad_norm": 1.570541501045227,
      "learning_rate": 2.950897072710104e-06,
      "loss": 2.3548,
      "step": 25910
    },
    {
      "epoch": 2.8239708025220955,
      "grad_norm": 1.5797176361083984,
      "learning_rate": 2.9327377061088108e-06,
      "loss": 2.4604,
      "step": 25920
    },
    {
      "epoch": 2.8250602606528576,
      "grad_norm": 1.6130036115646362,
      "learning_rate": 2.914578339507518e-06,
      "loss": 2.3979,
      "step": 25930
    },
    {
      "epoch": 2.82614971878362,
      "grad_norm": 1.5693947076797485,
      "learning_rate": 2.896418972906225e-06,
      "loss": 2.4497,
      "step": 25940
    },
    {
      "epoch": 2.8272391769143823,
      "grad_norm": 1.4747679233551025,
      "learning_rate": 2.878259606304932e-06,
      "loss": 2.3773,
      "step": 25950
    },
    {
      "epoch": 2.8283286350451444,
      "grad_norm": 1.5490323305130005,
      "learning_rate": 2.8601002397036392e-06,
      "loss": 2.3976,
      "step": 25960
    },
    {
      "epoch": 2.8294180931759065,
      "grad_norm": 1.7105625867843628,
      "learning_rate": 2.8419408731023463e-06,
      "loss": 2.5436,
      "step": 25970
    },
    {
      "epoch": 2.8305075513066686,
      "grad_norm": 1.5883195400238037,
      "learning_rate": 2.8237815065010535e-06,
      "loss": 2.4146,
      "step": 25980
    },
    {
      "epoch": 2.831597009437431,
      "grad_norm": 1.6407628059387207,
      "learning_rate": 2.80562213989976e-06,
      "loss": 2.4549,
      "step": 25990
    },
    {
      "epoch": 2.8326864675681933,
      "grad_norm": 1.5492764711380005,
      "learning_rate": 2.7874627732984673e-06,
      "loss": 2.3818,
      "step": 26000
    },
    {
      "epoch": 2.8337759256989554,
      "grad_norm": 1.5236307382583618,
      "learning_rate": 2.7693034066971744e-06,
      "loss": 2.4393,
      "step": 26010
    },
    {
      "epoch": 2.8348653838297175,
      "grad_norm": 1.704175591468811,
      "learning_rate": 2.7511440400958815e-06,
      "loss": 2.3793,
      "step": 26020
    },
    {
      "epoch": 2.83595484196048,
      "grad_norm": 1.559712290763855,
      "learning_rate": 2.732984673494589e-06,
      "loss": 2.3887,
      "step": 26030
    },
    {
      "epoch": 2.837044300091242,
      "grad_norm": 1.5996129512786865,
      "learning_rate": 2.7148253068932957e-06,
      "loss": 2.4044,
      "step": 26040
    },
    {
      "epoch": 2.8381337582220043,
      "grad_norm": 1.6290366649627686,
      "learning_rate": 2.696665940292003e-06,
      "loss": 2.3722,
      "step": 26050
    },
    {
      "epoch": 2.8392232163527664,
      "grad_norm": 1.526914358139038,
      "learning_rate": 2.67850657369071e-06,
      "loss": 2.3365,
      "step": 26060
    },
    {
      "epoch": 2.8403126744835285,
      "grad_norm": 1.5643337965011597,
      "learning_rate": 2.660347207089417e-06,
      "loss": 2.429,
      "step": 26070
    },
    {
      "epoch": 2.841402132614291,
      "grad_norm": 1.6327461004257202,
      "learning_rate": 2.642187840488124e-06,
      "loss": 2.4837,
      "step": 26080
    },
    {
      "epoch": 2.842491590745053,
      "grad_norm": 1.4950363636016846,
      "learning_rate": 2.6240284738868313e-06,
      "loss": 2.4234,
      "step": 26090
    },
    {
      "epoch": 2.8435810488758153,
      "grad_norm": 1.540561556816101,
      "learning_rate": 2.605869107285538e-06,
      "loss": 2.3794,
      "step": 26100
    },
    {
      "epoch": 2.844670507006578,
      "grad_norm": 1.5856678485870361,
      "learning_rate": 2.587709740684245e-06,
      "loss": 2.3993,
      "step": 26110
    },
    {
      "epoch": 2.84575996513734,
      "grad_norm": 1.5923504829406738,
      "learning_rate": 2.569550374082952e-06,
      "loss": 2.4964,
      "step": 26120
    },
    {
      "epoch": 2.846849423268102,
      "grad_norm": 1.5865724086761475,
      "learning_rate": 2.5513910074816593e-06,
      "loss": 2.4913,
      "step": 26130
    },
    {
      "epoch": 2.847938881398864,
      "grad_norm": 1.566381573677063,
      "learning_rate": 2.5332316408803664e-06,
      "loss": 2.454,
      "step": 26140
    },
    {
      "epoch": 2.8490283395296263,
      "grad_norm": 1.5558222532272339,
      "learning_rate": 2.5150722742790735e-06,
      "loss": 2.3483,
      "step": 26150
    },
    {
      "epoch": 2.8501177976603884,
      "grad_norm": 1.5060404539108276,
      "learning_rate": 2.49691290767778e-06,
      "loss": 2.3673,
      "step": 26160
    },
    {
      "epoch": 2.851207255791151,
      "grad_norm": 1.5642491579055786,
      "learning_rate": 2.4787535410764873e-06,
      "loss": 2.3744,
      "step": 26170
    },
    {
      "epoch": 2.852296713921913,
      "grad_norm": 1.5414869785308838,
      "learning_rate": 2.4605941744751944e-06,
      "loss": 2.3571,
      "step": 26180
    },
    {
      "epoch": 2.853386172052675,
      "grad_norm": 1.4963823556900024,
      "learning_rate": 2.4424348078739015e-06,
      "loss": 2.3997,
      "step": 26190
    },
    {
      "epoch": 2.8544756301834378,
      "grad_norm": 1.6046239137649536,
      "learning_rate": 2.4242754412726086e-06,
      "loss": 2.4369,
      "step": 26200
    },
    {
      "epoch": 2.8555650883142,
      "grad_norm": 1.5515440702438354,
      "learning_rate": 2.4061160746713158e-06,
      "loss": 2.3367,
      "step": 26210
    },
    {
      "epoch": 2.856654546444962,
      "grad_norm": 1.5753793716430664,
      "learning_rate": 2.3879567080700224e-06,
      "loss": 2.4219,
      "step": 26220
    },
    {
      "epoch": 2.857744004575724,
      "grad_norm": 1.7025233507156372,
      "learning_rate": 2.3697973414687296e-06,
      "loss": 2.5044,
      "step": 26230
    },
    {
      "epoch": 2.858833462706486,
      "grad_norm": 1.5712171792984009,
      "learning_rate": 2.3516379748674367e-06,
      "loss": 2.399,
      "step": 26240
    },
    {
      "epoch": 2.8599229208372483,
      "grad_norm": 1.570685625076294,
      "learning_rate": 2.3334786082661438e-06,
      "loss": 2.4799,
      "step": 26250
    },
    {
      "epoch": 2.861012378968011,
      "grad_norm": 1.6787587404251099,
      "learning_rate": 2.315319241664851e-06,
      "loss": 2.4495,
      "step": 26260
    },
    {
      "epoch": 2.862101837098773,
      "grad_norm": 1.5460505485534668,
      "learning_rate": 2.297159875063558e-06,
      "loss": 2.3998,
      "step": 26270
    },
    {
      "epoch": 2.863191295229535,
      "grad_norm": 1.5368142127990723,
      "learning_rate": 2.2790005084622647e-06,
      "loss": 2.3658,
      "step": 26280
    },
    {
      "epoch": 2.8642807533602976,
      "grad_norm": 1.5252856016159058,
      "learning_rate": 2.260841141860972e-06,
      "loss": 2.3763,
      "step": 26290
    },
    {
      "epoch": 2.8653702114910597,
      "grad_norm": 1.5635031461715698,
      "learning_rate": 2.242681775259679e-06,
      "loss": 2.4713,
      "step": 26300
    },
    {
      "epoch": 2.866459669621822,
      "grad_norm": 1.6708024740219116,
      "learning_rate": 2.224522408658386e-06,
      "loss": 2.4111,
      "step": 26310
    },
    {
      "epoch": 2.867549127752584,
      "grad_norm": 1.6960749626159668,
      "learning_rate": 2.206363042057093e-06,
      "loss": 2.4133,
      "step": 26320
    },
    {
      "epoch": 2.868638585883346,
      "grad_norm": 1.5870726108551025,
      "learning_rate": 2.1882036754558002e-06,
      "loss": 2.4283,
      "step": 26330
    },
    {
      "epoch": 2.8697280440141086,
      "grad_norm": 1.5963164567947388,
      "learning_rate": 2.170044308854507e-06,
      "loss": 2.3892,
      "step": 26340
    },
    {
      "epoch": 2.8708175021448707,
      "grad_norm": 1.5253466367721558,
      "learning_rate": 2.151884942253214e-06,
      "loss": 2.4856,
      "step": 26350
    },
    {
      "epoch": 2.871906960275633,
      "grad_norm": 1.5654995441436768,
      "learning_rate": 2.133725575651921e-06,
      "loss": 2.4188,
      "step": 26360
    },
    {
      "epoch": 2.872996418406395,
      "grad_norm": 1.4922213554382324,
      "learning_rate": 2.1155662090506283e-06,
      "loss": 2.4598,
      "step": 26370
    },
    {
      "epoch": 2.8740858765371575,
      "grad_norm": 1.5026081800460815,
      "learning_rate": 2.0974068424493354e-06,
      "loss": 2.3939,
      "step": 26380
    },
    {
      "epoch": 2.8751753346679196,
      "grad_norm": 1.4744489192962646,
      "learning_rate": 2.0792474758480425e-06,
      "loss": 2.4227,
      "step": 26390
    },
    {
      "epoch": 2.8762647927986817,
      "grad_norm": 1.5913695096969604,
      "learning_rate": 2.0610881092467496e-06,
      "loss": 2.4282,
      "step": 26400
    },
    {
      "epoch": 2.877354250929444,
      "grad_norm": 1.5917023420333862,
      "learning_rate": 2.0429287426454567e-06,
      "loss": 2.3814,
      "step": 26410
    },
    {
      "epoch": 2.878443709060206,
      "grad_norm": 1.5321813821792603,
      "learning_rate": 2.024769376044164e-06,
      "loss": 2.4182,
      "step": 26420
    },
    {
      "epoch": 2.8795331671909685,
      "grad_norm": 1.5844109058380127,
      "learning_rate": 2.006610009442871e-06,
      "loss": 2.4849,
      "step": 26430
    },
    {
      "epoch": 2.8806226253217306,
      "grad_norm": 1.6156485080718994,
      "learning_rate": 1.988450642841578e-06,
      "loss": 2.4665,
      "step": 26440
    },
    {
      "epoch": 2.8817120834524927,
      "grad_norm": 1.564466953277588,
      "learning_rate": 1.970291276240285e-06,
      "loss": 2.4127,
      "step": 26450
    },
    {
      "epoch": 2.8828015415832553,
      "grad_norm": 1.6395490169525146,
      "learning_rate": 1.952131909638992e-06,
      "loss": 2.4292,
      "step": 26460
    },
    {
      "epoch": 2.8838909997140174,
      "grad_norm": 1.5738656520843506,
      "learning_rate": 1.933972543037699e-06,
      "loss": 2.4309,
      "step": 26470
    },
    {
      "epoch": 2.8849804578447795,
      "grad_norm": 1.5615588426589966,
      "learning_rate": 1.915813176436406e-06,
      "loss": 2.5117,
      "step": 26480
    },
    {
      "epoch": 2.8860699159755416,
      "grad_norm": 1.5630137920379639,
      "learning_rate": 1.8976538098351132e-06,
      "loss": 2.4412,
      "step": 26490
    },
    {
      "epoch": 2.8871593741063037,
      "grad_norm": 1.5212337970733643,
      "learning_rate": 1.87949444323382e-06,
      "loss": 2.4768,
      "step": 26500
    },
    {
      "epoch": 2.888248832237066,
      "grad_norm": 1.5349094867706299,
      "learning_rate": 1.8613350766325272e-06,
      "loss": 2.3784,
      "step": 26510
    },
    {
      "epoch": 2.8893382903678284,
      "grad_norm": 1.6193459033966064,
      "learning_rate": 1.8431757100312343e-06,
      "loss": 2.4483,
      "step": 26520
    },
    {
      "epoch": 2.8904277484985905,
      "grad_norm": 1.6787222623825073,
      "learning_rate": 1.8250163434299412e-06,
      "loss": 2.3916,
      "step": 26530
    },
    {
      "epoch": 2.8915172066293526,
      "grad_norm": 1.5429039001464844,
      "learning_rate": 1.8068569768286483e-06,
      "loss": 2.4286,
      "step": 26540
    },
    {
      "epoch": 2.892606664760115,
      "grad_norm": 1.6324524879455566,
      "learning_rate": 1.7886976102273554e-06,
      "loss": 2.4572,
      "step": 26550
    },
    {
      "epoch": 2.8936961228908773,
      "grad_norm": 1.4966176748275757,
      "learning_rate": 1.7705382436260625e-06,
      "loss": 2.4445,
      "step": 26560
    },
    {
      "epoch": 2.8947855810216394,
      "grad_norm": 1.6374564170837402,
      "learning_rate": 1.7523788770247694e-06,
      "loss": 2.3904,
      "step": 26570
    },
    {
      "epoch": 2.8958750391524015,
      "grad_norm": 1.6978591680526733,
      "learning_rate": 1.7342195104234766e-06,
      "loss": 2.4245,
      "step": 26580
    },
    {
      "epoch": 2.8969644972831636,
      "grad_norm": 1.5543159246444702,
      "learning_rate": 1.7160601438221837e-06,
      "loss": 2.3783,
      "step": 26590
    },
    {
      "epoch": 2.8980539554139257,
      "grad_norm": 1.6067531108856201,
      "learning_rate": 1.6979007772208906e-06,
      "loss": 2.3741,
      "step": 26600
    },
    {
      "epoch": 2.8991434135446883,
      "grad_norm": 1.5817246437072754,
      "learning_rate": 1.6797414106195977e-06,
      "loss": 2.369,
      "step": 26610
    },
    {
      "epoch": 2.9002328716754504,
      "grad_norm": 1.6134566068649292,
      "learning_rate": 1.6615820440183048e-06,
      "loss": 2.3909,
      "step": 26620
    },
    {
      "epoch": 2.9013223298062125,
      "grad_norm": 1.585544228553772,
      "learning_rate": 1.6434226774170117e-06,
      "loss": 2.4535,
      "step": 26630
    },
    {
      "epoch": 2.902411787936975,
      "grad_norm": 1.5271228551864624,
      "learning_rate": 1.6252633108157188e-06,
      "loss": 2.3713,
      "step": 26640
    },
    {
      "epoch": 2.903501246067737,
      "grad_norm": 1.549068570137024,
      "learning_rate": 1.607103944214426e-06,
      "loss": 2.3787,
      "step": 26650
    },
    {
      "epoch": 2.9045907041984993,
      "grad_norm": 1.5069324970245361,
      "learning_rate": 1.5889445776131328e-06,
      "loss": 2.3534,
      "step": 26660
    },
    {
      "epoch": 2.9056801623292614,
      "grad_norm": 1.5417557954788208,
      "learning_rate": 1.57078521101184e-06,
      "loss": 2.452,
      "step": 26670
    },
    {
      "epoch": 2.9067696204600235,
      "grad_norm": 1.5374068021774292,
      "learning_rate": 1.552625844410547e-06,
      "loss": 2.429,
      "step": 26680
    },
    {
      "epoch": 2.907859078590786,
      "grad_norm": 1.49294114112854,
      "learning_rate": 1.5344664778092541e-06,
      "loss": 2.4186,
      "step": 26690
    },
    {
      "epoch": 2.908948536721548,
      "grad_norm": 1.6126223802566528,
      "learning_rate": 1.5163071112079613e-06,
      "loss": 2.414,
      "step": 26700
    },
    {
      "epoch": 2.9100379948523103,
      "grad_norm": 1.6466187238693237,
      "learning_rate": 1.4981477446066682e-06,
      "loss": 2.389,
      "step": 26710
    },
    {
      "epoch": 2.9111274529830724,
      "grad_norm": 1.5122402906417847,
      "learning_rate": 1.4799883780053753e-06,
      "loss": 2.3511,
      "step": 26720
    },
    {
      "epoch": 2.912216911113835,
      "grad_norm": 1.5529311895370483,
      "learning_rate": 1.4618290114040824e-06,
      "loss": 2.4288,
      "step": 26730
    },
    {
      "epoch": 2.913306369244597,
      "grad_norm": 1.610452651977539,
      "learning_rate": 1.4436696448027893e-06,
      "loss": 2.3128,
      "step": 26740
    },
    {
      "epoch": 2.914395827375359,
      "grad_norm": 1.540506362915039,
      "learning_rate": 1.4255102782014964e-06,
      "loss": 2.4257,
      "step": 26750
    },
    {
      "epoch": 2.9154852855061213,
      "grad_norm": 1.5817298889160156,
      "learning_rate": 1.4073509116002035e-06,
      "loss": 2.3858,
      "step": 26760
    },
    {
      "epoch": 2.9165747436368834,
      "grad_norm": 1.5868247747421265,
      "learning_rate": 1.3891915449989106e-06,
      "loss": 2.4385,
      "step": 26770
    },
    {
      "epoch": 2.917664201767646,
      "grad_norm": 1.5623337030410767,
      "learning_rate": 1.3710321783976175e-06,
      "loss": 2.3995,
      "step": 26780
    },
    {
      "epoch": 2.918753659898408,
      "grad_norm": 1.573090672492981,
      "learning_rate": 1.3528728117963246e-06,
      "loss": 2.3881,
      "step": 26790
    },
    {
      "epoch": 2.91984311802917,
      "grad_norm": 1.5448615550994873,
      "learning_rate": 1.3347134451950317e-06,
      "loss": 2.4273,
      "step": 26800
    },
    {
      "epoch": 2.9209325761599323,
      "grad_norm": 1.5365562438964844,
      "learning_rate": 1.3165540785937386e-06,
      "loss": 2.3787,
      "step": 26810
    },
    {
      "epoch": 2.922022034290695,
      "grad_norm": 1.5340306758880615,
      "learning_rate": 1.2983947119924458e-06,
      "loss": 2.4133,
      "step": 26820
    },
    {
      "epoch": 2.923111492421457,
      "grad_norm": 1.4796322584152222,
      "learning_rate": 1.2802353453911529e-06,
      "loss": 2.3609,
      "step": 26830
    },
    {
      "epoch": 2.924200950552219,
      "grad_norm": 1.573423981666565,
      "learning_rate": 1.2620759787898598e-06,
      "loss": 2.3961,
      "step": 26840
    },
    {
      "epoch": 2.925290408682981,
      "grad_norm": 1.55930495262146,
      "learning_rate": 1.2439166121885669e-06,
      "loss": 2.3524,
      "step": 26850
    },
    {
      "epoch": 2.9263798668137433,
      "grad_norm": 1.7378487586975098,
      "learning_rate": 1.225757245587274e-06,
      "loss": 2.5647,
      "step": 26860
    },
    {
      "epoch": 2.927469324944506,
      "grad_norm": 1.6343854665756226,
      "learning_rate": 1.2075978789859809e-06,
      "loss": 2.4645,
      "step": 26870
    },
    {
      "epoch": 2.928558783075268,
      "grad_norm": 1.510011911392212,
      "learning_rate": 1.1894385123846882e-06,
      "loss": 2.4823,
      "step": 26880
    },
    {
      "epoch": 2.92964824120603,
      "grad_norm": 1.620382308959961,
      "learning_rate": 1.1712791457833951e-06,
      "loss": 2.4302,
      "step": 26890
    },
    {
      "epoch": 2.9307376993367926,
      "grad_norm": 1.6252316236495972,
      "learning_rate": 1.1531197791821022e-06,
      "loss": 2.446,
      "step": 26900
    },
    {
      "epoch": 2.9318271574675547,
      "grad_norm": 1.4655698537826538,
      "learning_rate": 1.1349604125808093e-06,
      "loss": 2.3964,
      "step": 26910
    },
    {
      "epoch": 2.932916615598317,
      "grad_norm": 1.5813217163085938,
      "learning_rate": 1.1168010459795162e-06,
      "loss": 2.4796,
      "step": 26920
    },
    {
      "epoch": 2.934006073729079,
      "grad_norm": 1.56938636302948,
      "learning_rate": 1.0986416793782233e-06,
      "loss": 2.4039,
      "step": 26930
    },
    {
      "epoch": 2.935095531859841,
      "grad_norm": 1.5658386945724487,
      "learning_rate": 1.0804823127769305e-06,
      "loss": 2.4703,
      "step": 26940
    },
    {
      "epoch": 2.936184989990603,
      "grad_norm": 1.514798641204834,
      "learning_rate": 1.0623229461756376e-06,
      "loss": 2.4194,
      "step": 26950
    },
    {
      "epoch": 2.9372744481213657,
      "grad_norm": 1.6206351518630981,
      "learning_rate": 1.0441635795743445e-06,
      "loss": 2.4078,
      "step": 26960
    },
    {
      "epoch": 2.938363906252128,
      "grad_norm": 1.7555261850357056,
      "learning_rate": 1.0260042129730516e-06,
      "loss": 2.3776,
      "step": 26970
    },
    {
      "epoch": 2.93945336438289,
      "grad_norm": 1.515862226486206,
      "learning_rate": 1.0078448463717587e-06,
      "loss": 2.3848,
      "step": 26980
    },
    {
      "epoch": 2.9405428225136525,
      "grad_norm": 1.5689016580581665,
      "learning_rate": 9.896854797704656e-07,
      "loss": 2.4946,
      "step": 26990
    },
    {
      "epoch": 2.9416322806444146,
      "grad_norm": 1.6077464818954468,
      "learning_rate": 9.715261131691727e-07,
      "loss": 2.3619,
      "step": 27000
    },
    {
      "epoch": 2.9427217387751767,
      "grad_norm": 1.592085599899292,
      "learning_rate": 9.533667465678797e-07,
      "loss": 2.4063,
      "step": 27010
    },
    {
      "epoch": 2.943811196905939,
      "grad_norm": 1.4227460622787476,
      "learning_rate": 9.352073799665867e-07,
      "loss": 2.3314,
      "step": 27020
    },
    {
      "epoch": 2.944900655036701,
      "grad_norm": 1.5739320516586304,
      "learning_rate": 9.170480133652938e-07,
      "loss": 2.3774,
      "step": 27030
    },
    {
      "epoch": 2.945990113167463,
      "grad_norm": 1.5678952932357788,
      "learning_rate": 8.988886467640008e-07,
      "loss": 2.3897,
      "step": 27040
    },
    {
      "epoch": 2.9470795712982256,
      "grad_norm": 1.7154439687728882,
      "learning_rate": 8.807292801627079e-07,
      "loss": 2.4402,
      "step": 27050
    },
    {
      "epoch": 2.9481690294289877,
      "grad_norm": 1.5214037895202637,
      "learning_rate": 8.62569913561415e-07,
      "loss": 2.3566,
      "step": 27060
    },
    {
      "epoch": 2.94925848755975,
      "grad_norm": 1.542222261428833,
      "learning_rate": 8.444105469601222e-07,
      "loss": 2.3192,
      "step": 27070
    },
    {
      "epoch": 2.9503479456905124,
      "grad_norm": 1.452546238899231,
      "learning_rate": 8.262511803588292e-07,
      "loss": 2.336,
      "step": 27080
    },
    {
      "epoch": 2.9514374038212745,
      "grad_norm": 1.576379418373108,
      "learning_rate": 8.080918137575363e-07,
      "loss": 2.3451,
      "step": 27090
    },
    {
      "epoch": 2.9525268619520366,
      "grad_norm": 1.600455641746521,
      "learning_rate": 7.899324471562433e-07,
      "loss": 2.4717,
      "step": 27100
    },
    {
      "epoch": 2.9536163200827987,
      "grad_norm": 1.5806562900543213,
      "learning_rate": 7.717730805549502e-07,
      "loss": 2.3889,
      "step": 27110
    },
    {
      "epoch": 2.954705778213561,
      "grad_norm": 1.5991814136505127,
      "learning_rate": 7.536137139536574e-07,
      "loss": 2.4163,
      "step": 27120
    },
    {
      "epoch": 2.9557952363443234,
      "grad_norm": 1.5245375633239746,
      "learning_rate": 7.354543473523644e-07,
      "loss": 2.3888,
      "step": 27130
    },
    {
      "epoch": 2.9568846944750855,
      "grad_norm": 1.5655319690704346,
      "learning_rate": 7.172949807510714e-07,
      "loss": 2.3907,
      "step": 27140
    },
    {
      "epoch": 2.9579741526058476,
      "grad_norm": 1.5856297016143799,
      "learning_rate": 6.991356141497785e-07,
      "loss": 2.3309,
      "step": 27150
    },
    {
      "epoch": 2.9590636107366097,
      "grad_norm": 1.6380971670150757,
      "learning_rate": 6.827921842086149e-07,
      "loss": 2.5012,
      "step": 27160
    },
    {
      "epoch": 2.9601530688673723,
      "grad_norm": 1.5526002645492554,
      "learning_rate": 6.646328176073219e-07,
      "loss": 2.4591,
      "step": 27170
    },
    {
      "epoch": 2.9612425269981344,
      "grad_norm": 1.5840809345245361,
      "learning_rate": 6.464734510060289e-07,
      "loss": 2.4757,
      "step": 27180
    },
    {
      "epoch": 2.9623319851288965,
      "grad_norm": 1.5256435871124268,
      "learning_rate": 6.28314084404736e-07,
      "loss": 2.4999,
      "step": 27190
    },
    {
      "epoch": 2.9634214432596586,
      "grad_norm": 1.6371995210647583,
      "learning_rate": 6.10154717803443e-07,
      "loss": 2.4781,
      "step": 27200
    },
    {
      "epoch": 2.9645109013904207,
      "grad_norm": 1.4318183660507202,
      "learning_rate": 5.919953512021501e-07,
      "loss": 2.4222,
      "step": 27210
    },
    {
      "epoch": 2.9656003595211833,
      "grad_norm": 1.5415740013122559,
      "learning_rate": 5.738359846008572e-07,
      "loss": 2.4077,
      "step": 27220
    },
    {
      "epoch": 2.9666898176519454,
      "grad_norm": 1.5850958824157715,
      "learning_rate": 5.556766179995642e-07,
      "loss": 2.4193,
      "step": 27230
    },
    {
      "epoch": 2.9677792757827075,
      "grad_norm": 1.5194011926651,
      "learning_rate": 5.375172513982712e-07,
      "loss": 2.3347,
      "step": 27240
    },
    {
      "epoch": 2.96886873391347,
      "grad_norm": 1.5888867378234863,
      "learning_rate": 5.193578847969783e-07,
      "loss": 2.3667,
      "step": 27250
    },
    {
      "epoch": 2.969958192044232,
      "grad_norm": 1.6443383693695068,
      "learning_rate": 5.011985181956853e-07,
      "loss": 2.4084,
      "step": 27260
    },
    {
      "epoch": 2.9710476501749943,
      "grad_norm": 1.6237305402755737,
      "learning_rate": 4.830391515943924e-07,
      "loss": 2.3918,
      "step": 27270
    },
    {
      "epoch": 2.9721371083057564,
      "grad_norm": 1.5199886560440063,
      "learning_rate": 4.648797849930994e-07,
      "loss": 2.4335,
      "step": 27280
    },
    {
      "epoch": 2.9732265664365185,
      "grad_norm": 1.6147271394729614,
      "learning_rate": 4.4672041839180647e-07,
      "loss": 2.3768,
      "step": 27290
    },
    {
      "epoch": 2.9743160245672806,
      "grad_norm": 1.5564554929733276,
      "learning_rate": 4.2856105179051353e-07,
      "loss": 2.3975,
      "step": 27300
    },
    {
      "epoch": 2.975405482698043,
      "grad_norm": 1.6291383504867554,
      "learning_rate": 4.1040168518922064e-07,
      "loss": 2.4176,
      "step": 27310
    },
    {
      "epoch": 2.9764949408288053,
      "grad_norm": 1.5489674806594849,
      "learning_rate": 3.922423185879277e-07,
      "loss": 2.4005,
      "step": 27320
    },
    {
      "epoch": 2.9775843989595674,
      "grad_norm": 1.481913447380066,
      "learning_rate": 3.7408295198663476e-07,
      "loss": 2.3777,
      "step": 27330
    },
    {
      "epoch": 2.97867385709033,
      "grad_norm": 1.5392690896987915,
      "learning_rate": 3.5592358538534177e-07,
      "loss": 2.469,
      "step": 27340
    },
    {
      "epoch": 2.979763315221092,
      "grad_norm": 1.6018636226654053,
      "learning_rate": 3.377642187840488e-07,
      "loss": 2.4386,
      "step": 27350
    },
    {
      "epoch": 2.980852773351854,
      "grad_norm": 1.6342202425003052,
      "learning_rate": 3.196048521827559e-07,
      "loss": 2.3074,
      "step": 27360
    },
    {
      "epoch": 2.9819422314826163,
      "grad_norm": 1.4698443412780762,
      "learning_rate": 3.014454855814629e-07,
      "loss": 2.4199,
      "step": 27370
    },
    {
      "epoch": 2.9830316896133784,
      "grad_norm": 1.5485731363296509,
      "learning_rate": 2.8328611898017e-07,
      "loss": 2.3839,
      "step": 27380
    },
    {
      "epoch": 2.9841211477441405,
      "grad_norm": 1.6433807611465454,
      "learning_rate": 2.6512675237887706e-07,
      "loss": 2.4788,
      "step": 27390
    },
    {
      "epoch": 2.985210605874903,
      "grad_norm": 1.6346684694290161,
      "learning_rate": 2.4696738577758406e-07,
      "loss": 2.4347,
      "step": 27400
    },
    {
      "epoch": 2.986300064005665,
      "grad_norm": 1.5703984498977661,
      "learning_rate": 2.2880801917629112e-07,
      "loss": 2.4529,
      "step": 27410
    },
    {
      "epoch": 2.9873895221364273,
      "grad_norm": 1.543189525604248,
      "learning_rate": 2.106486525749982e-07,
      "loss": 2.3166,
      "step": 27420
    },
    {
      "epoch": 2.98847898026719,
      "grad_norm": 1.586500644683838,
      "learning_rate": 1.9248928597370527e-07,
      "loss": 2.417,
      "step": 27430
    },
    {
      "epoch": 2.989568438397952,
      "grad_norm": 1.5910568237304688,
      "learning_rate": 1.743299193724123e-07,
      "loss": 2.3933,
      "step": 27440
    },
    {
      "epoch": 2.990657896528714,
      "grad_norm": 1.6244711875915527,
      "learning_rate": 1.5617055277111933e-07,
      "loss": 2.4508,
      "step": 27450
    },
    {
      "epoch": 2.991747354659476,
      "grad_norm": 1.584057331085205,
      "learning_rate": 1.3801118616982642e-07,
      "loss": 2.3968,
      "step": 27460
    },
    {
      "epoch": 2.9928368127902383,
      "grad_norm": 1.5579731464385986,
      "learning_rate": 1.1985181956853345e-07,
      "loss": 2.3553,
      "step": 27470
    },
    {
      "epoch": 2.993926270921001,
      "grad_norm": 1.6320524215698242,
      "learning_rate": 1.0169245296724052e-07,
      "loss": 2.4208,
      "step": 27480
    },
    {
      "epoch": 2.995015729051763,
      "grad_norm": 1.5324331521987915,
      "learning_rate": 8.353308636594755e-08,
      "loss": 2.3614,
      "step": 27490
    },
    {
      "epoch": 2.996105187182525,
      "grad_norm": 1.5390194654464722,
      "learning_rate": 6.537371976465461e-08,
      "loss": 2.3721,
      "step": 27500
    },
    {
      "epoch": 2.997194645313287,
      "grad_norm": 1.4433872699737549,
      "learning_rate": 4.7214353163361665e-08,
      "loss": 2.3399,
      "step": 27510
    },
    {
      "epoch": 2.9982841034440497,
      "grad_norm": 1.5097748041152954,
      "learning_rate": 2.9054986562068713e-08,
      "loss": 2.5198,
      "step": 27520
    },
    {
      "epoch": 2.999373561574812,
      "grad_norm": 1.4987019300460815,
      "learning_rate": 1.0895619960775769e-08,
      "loss": 2.404,
      "step": 27530
    }
  ],
  "logging_steps": 10,
  "max_steps": 27534,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.915603833984123e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
