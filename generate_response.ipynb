# 3B라서 VRAM 12GB이상에서만 가능합니다.

import pandas as pd

df = pd.read_json('dataFromAIHUB.json')

import os
os.environ['HF_TOKEN']="hugging_face_token"

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "meta-llama/Llama-3.2-3B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
   model_id,
    #torch_dtype="auto",
    device_map="cuda:1",
)

def generate_response(system_message, user_message):
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message},
    ]

    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(model.device)

    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]

    outputs = model.generate(
        input_ids,
        max_new_tokens=512,
        eos_token_id=terminators,
        do_sample=True,
        # beomi 모델의 경우 temperature 를 1로 줌 (더 다양한 답변 생성)
        # temperature=1,
        temperature=0.6,
        top_p=0.9
        
    )

    response = outputs[0][input_ids.shape[-1]:]

    return tokenizer.decode(response, skip_special_tokens=True)

orginal_text = df.loc['content'][0]

# model output (summarization 결과)
llama3_summary_text = generate_response(system_message="You are a chatbot that performs summarization tasks. Generate a concise summary in Korean, ensuring it stays within 512 tokens and captures only the key points. If you fail to do this well, I won’t be able to earn my salary. Reconstruct the original content to create a summary with natural and newly written sentences. Ensure that the summary is concise, coherent, and captures the key points effectively.",
                             user_message=orginal_text)

print(llama3_summary_text)
